[{"number":"23628","comments":[{"date":"2017-03-17T11:24:14Z","author":"ywelsch","text":"We need the stack trace for this ArrayIndexOutOfBoundsException. It is missing here as the JVM [optimizes throwing repeatedly the same exception](http:\/\/www.oracle.com\/technetwork\/java\/javase\/relnotes-139183.html):\r\n> \"The compiler in the server VM now provides correct stack backtraces for all \"cold\" built-in exceptions. For performance purposes, when such an exception is thrown a few times, the method may be recompiled. After recompilation, the compiler may choose a faster tactic using preallocated exceptions that do not provide a stack trace. To disable completely the use of preallocated exceptions, use this new flag: -XX:-OmitStackTraceInFastThrow.\"\r\n\r\nCan you grep in your logs for the first occurrence of this exception?"},{"date":"2017-03-18T16:57:17Z","author":"ojundt","text":"```\r\n[2017-03-18T15:23:24,661][DEBUG][o.e.a.s.TransportSearchAction] [elasticsearch5-flamingo-0640e123456614532] All shards failed for phase: [query_fetch]\r\norg.elasticsearch.transport.RemoteTransportException: [elasticsearch5-gull-065b812345909563f][123.12.0.2:9300][indices:data\/read\/search[phase\/query+fetch]]\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 33439509\r\n\tat org.apache.lucene.util.FixedBitSet.get(FixedBitSet.java:186) ~[lucene-core-6.4.1.jar:6.4.1 72f75b2503fa0aa4f0aff76d439874feb923bb0e - jpountz - 2017-02-01 14:43:32]\r\n\tat org.elasticsearch.search.fetch.FetchPhase.findRootDocumentIfNested(FetchPhase.java:177) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n\tat org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:150) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n\tat org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:370) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n\tat org.elasticsearch.action.search.SearchTransportService$9.messageReceived(SearchTransportService.java:322) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n\tat org.elasticsearch.action.search.SearchTransportService$9.messageReceived(SearchTransportService.java:319) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n\tat com.floragunn.searchguard.ssl.transport.SearchGuardSSLRequestHandler.messageReceivedDecorate(SearchGuardSSLRequestHandler.java:184) ~[?:?]\r\n\tat com.floragunn.searchguard.transport.SearchGuardRequestHandler.messageReceivedDecorate(SearchGuardRequestHandler.java:171) ~[?:?]\r\n\tat com.floragunn.searchguard.ssl.transport.SearchGuardSSLRequestHandler.messageReceived(SearchGuardSSLRequestHandler.java:139) ~[?:?]\r\n\tat com.floragunn.searchguard.SearchGuardPlugin$2$1.messageReceived(SearchGuardPlugin.java:284) ~[?:?]\r\n\tat org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:69) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n\tat org.elasticsearch.transport.TcpTransport$RequestHandler.doRun(TcpTransport.java:1488) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n\tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:596) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n\tat org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_91]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_91]\r\n  at java.lang.Thread.run(Thread.java:745) [?:1.8.0_91]\r\n```"},{"date":"2017-03-18T17:05:36Z","author":"ywelsch","text":"@jpountz @jimczi thoughts?"},{"date":"2017-03-20T14:50:31Z","author":"jimczi","text":"The docId that is checked in the parent bitset should always be smaller than `maxDoc` though in your issue it seems that there is a discrepancy between the reader that is used for search and the one used to fetch the documents. Since this query executes the query and the fetch phase in a single pass this should never happen unless a third party plugin messes with the reader.\r\nI checked a bit what SearchGuard is doing and it seems that the searcher\/reader is wrapped in this plugin in order to add a security layer on top of ES search. This is an expert feature and I suspect that the wrapper breaks some assumption in Lucene\/ES. \r\nConsidering that the SearchGuard plugin adds a search layer on top of ES, I'd advise to first see if the problem is related to SearchGuard (which I suspect) and open an issue there or to try to reproduce the problem without SearchGuard.\r\nEither case I'll close this issue for now since there is no proof that ES is responsible for this. Please reopen if you find evidence that SearchGuard is not the cause of this."},{"date":"2017-03-27T04:28:26Z","author":"ojundt","text":"Sorry guys, I just restarted our cluster without the SearchGuard plugin and the problem is still occurring:\r\n\r\n```\r\n[2017-03-27T03:52:56,546][DEBUG][o.e.a.s.TransportSearchAction] [elasticsearch5-petrel-0ff7463c013123456] All shards failed for phase: [query_fetch]\r\norg.elasticsearch.transport.RemoteTransportException: [elasticsearch5-flamingo-0640eca9d37123456][123.12.0.2:9300][indices:data\/read\/search[phase\/query+fetch]]\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 33423460\r\n\tat org.apache.lucene.util.FixedBitSet.get(FixedBitSet.java:186) ~[lucene-core-6.4.1.jar:6.4.1 72f75b2503fa0aa4f0aff76d439874feb923bb0e - jpountz - 2017-02-01 14:43:32]\r\n\tat org.elasticsearch.search.fetch.FetchPhase.findRootDocumentIfNested(FetchPhase.java:177) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n\tat org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:150) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n\tat org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:370) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n\tat org.elasticsearch.action.search.SearchTransportService$9.messageReceived(SearchTransportService.java:322) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n\tat org.elasticsearch.action.search.SearchTransportService$9.messageReceived(SearchTransportService.java:319) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n\tat org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:69) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n\tat org.elasticsearch.transport.TcpTransport$RequestHandler.doRun(TcpTransport.java:1488) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n\tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:596) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n\tat org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_91]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_91]\r\n\tat java.lang.Thread.run(Thread.java:745) [?:1.8.0_91]\r\n```"},{"date":"2017-03-27T10:52:29Z","author":"jimczi","text":"Fair enough @ojundt , reopen it is.\r\n\r\nAre you able to access the problematic queries ? It would help if you can share a reproducible case with the complete query that throws this exception.\r\nOtherwise can you share the mapping of your index and a complete example of your query:\r\n\r\n> \"query\" : {\r\n    <%VERY LARGE QUERY INCLUDING CONDITIONS ON NESTED TYPES%>\r\n  }, \r\n\r\n\r\n"},{"date":"2017-03-28T15:50:33Z","author":"ojundt","text":"@jimczi, thanks for reopening. I'm able to access the failing queries but I'm afraid I'm not allowed to share them here. Mappings maybe. Is there something specific you are looking for or that I can test for you?\r\n\r\nAs far as I can see now the same query succeeds after a few minutes so it's hard to reproduce. I will have a closer look at the end of the week."},{"date":"2017-03-29T19:01:34Z","author":"jimczi","text":"> Is there something specific you are looking for or that I can test for you?\r\n\r\nIt can be anything so we'll need to narrow the scope. Can you share the query tree of the failing query, just the query types without the content:\r\n\r\n`````\r\n\"query\": {\r\n  \"bool\": {\r\n     \"must\": {\r\n        \"term\": {\r\n           \/\/ private infos\r\n       }\r\n     }\r\n}\r\n`````\r\n\r\nI suspect that one of your inner query returns a `docID` greater than `maxDoc`. This should never happen so the query tree should filter the list of candidates to lookup."},{"date":"2017-04-05T21:08:21Z","author":"ojundt","text":"Alright, after some confusion and further drilling down I was able to consistently reproduce the error with the following setup:\r\n\r\nQuery (in query.json):\r\n```\r\n{\r\n  \"query\": {\r\n    \"function_score\": {\r\n      \"functions\": [\r\n        {\r\n          \"script_score\": {\r\n            \"script\": {\r\n              \"lang\": \"painless\",\r\n              \"inline\": \"-1 \/ doc['value'].value\"\r\n            }\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nSetup:\r\n```\r\ncurl -XPUT 'localhost:9205\/twitter\/tweet\/1?pretty' -H 'Content-Type: application\/json' -d'{ \"value\" : 0.0 }'\r\ncurl -XPOST 'localhost:9205\/twitter\/_search?pretty' -H 'Content-Type: application\/json' -d @query.json\r\n```\r\n\r\nOutput:\r\n```\r\n{\r\n  \"took\" : 6,\r\n  \"timed_out\" : false,\r\n  \"_shards\" : {\r\n    \"total\" : 5,\r\n    \"successful\" : 4,\r\n    \"failed\" : 1,\r\n    \"failures\" : [\r\n      {\r\n        \"shard\" : 3,\r\n        \"index\" : \"twitter\",\r\n        \"node\" : \"9D1hkKznR867YomO_6WtgA\",\r\n        \"reason\" : {\r\n          \"type\" : \"index_out_of_bounds_exception\",\r\n          \"reason\" : \"docID must be >= 0 and < maxDoc=1 (got docID=2147483647)\"\r\n        }\r\n      }\r\n    ]\r\n  },\r\n  \"hits\" : {\r\n    \"total\" : 1,\r\n    \"max_score\" : null,\r\n    \"hits\" : [ ]\r\n  }\r\n}\r\n```\r\n\r\nNow you may argue that division by zero is stupid anyway. Agree, but I'd expect a much different error message. Also, if you change `-1 \/ doc['value'].value` by `1 \/ doc['value'].value` it suddenly works:\r\n\r\n```\r\n{\r\n  \"took\" : 10,\r\n  \"timed_out\" : false,\r\n  \"_shards\" : {\r\n    \"total\" : 5,\r\n    \"successful\" : 5,\r\n    \"failed\" : 0\r\n  },\r\n  \"hits\" : {\r\n    \"total\" : 1,\r\n    \"max_score\" : 3.4028235E38,\r\n    \"hits\" : [\r\n      {\r\n        \"_index\" : \"twitter\",\r\n        \"_type\" : \"invoice\",\r\n        \"_id\" : \"1\",\r\n        \"_score\" : 3.4028235E38,\r\n        \"_source\" : {\r\n          \"value\" : 0.0\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\nHope this gives you guys a clear test case :)"}],"reopen_on":"2017-03-27T10:52:29Z","opened_by":"ojundt","closed_on":null,"description":"**Elasticsearch version**: 5.2.2\r\n\r\n**Plugins installed**:\r\n- EC2 Discovery\r\n- SearchGuard 5.2.2-11\r\n\r\n**JVM version**: openjdk 64bit 1.8.0_91\r\n\r\n**OS version**: Ubuntu 16.04\r\n\r\n**Description of the problem including expected versus actual behavior**:\r\nIn ~1% of large queries which span multiple types, including nested ones, we experience a 500 caused by `org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed` caused by `java.lang.ArrayIndexOutOfBoundsException`.\r\n\r\nWe are running a small ES cluster of 5 nodes (no dedicated master nodes) on AWS spread over two availability zones (2 instances in one, 3 in the other). Each instance is running ES in a docker container with external volumes.\r\n\r\nNodes are configured to use the EC2 AZ aware shard allocation. (`cluster.routing.allocation.awareness.attributes: aws_availability_zone`)\r\n\r\nFirst we though it might be a corrupted index. However, building a new index (not using the reindex API but basic bulk indexing) did not help. At the same time we switched to using custom routing with the new index. The error has been occurring more often since.\r\n\r\nSo far the error frequency clearly seems to be traffic dependent. It occurs more often during peak hours. It also tends to happen in batches. Sometimes there are no errors for hours and then suddenly a couple within a short timespan:\r\n<img width=\"874\" alt=\"screen shot 2017-03-17 at 10 49 21\" src=\"https:\/\/cloud.githubusercontent.com\/assets\/3025911\/24038425\/9587fb10-0b01-11e7-912f-ce7847fb5c14.png\">\r\n\r\nAny ideas of what's going on within these minutes?\r\n\r\n**Example trace**:\r\n\r\n```\r\n[2017-03-17T08:21:10,125][DEBUG][o.e.a.s.TransportSearchAction] [elasticsearch5-petrel-0ff7463123456e6e7] [<%INDEXNAME%>][4], node[Tci4YB12345dEhZebFINRQ], [R], s[STARTED], a[id=30tD94F3S_mq7uwk12341w]: Failed to execute [SearchRequest{searchType=QUERY_AND_FETCH, indices=[<%ALIAS INDEXNAME%>], indicesOptions=IndicesOptions[id=38, ignore_unavailable=false, allow_no_indices=true, expand_wildcards_open=true, expand_wildcards_closed=false, allow_alisases_to_multiple_indices=true, forbid_closed_indices=true], types=[<%6 DIFFERENT TYPES%>], routing='<OUR CUSTOM ROUTING ID>', preference='null', requestCache=null, scroll=null, source={\r\n  \"from\" : 0,\r\n  \"size\" : 50,\r\n  \"query\" : {\r\n    <%VERY LARGE QUERY INCLUDING CONDITIONS ON NESTED TYPES%>\r\n  },\r\n  \"stored_fields\" : [\r\n    \"_id\",\r\n    \"_type\"\r\n  ]\r\n}}]\r\norg.elasticsearch.transport.RemoteTransportException: [elasticsearch5-penguin-046876f8756512345][123.12.0.2:9300][indices:data\/read\/search[phase\/query+fetch]]\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException\r\n[2017-03-17T08:21:10,127][DEBUG][o.e.a.s.TransportSearchAction] [elasticsearch5-petrel-0ff7463123456e6e7] All shards failed for phase: [query_fetch]\r\norg.elasticsearch.transport.RemoteTransportException: [elasticsearch5-penguin-046876f8756512345][123.12.0.2:9300][indices:data\/read\/search[phase\/query+fetch]]\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException\r\norg.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed\r\n\tat org.elasticsearch.action.search.AbstractSearchAsyncAction.onFirstPhaseResult(AbstractSearchAsyncAction.java:208) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n\tat org.elasticsearch.action.search.AbstractSearchAsyncAction.access$100(AbstractSearchAsyncAction.java:52) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n\tat org.elasticsearch.action.search.AbstractSearchAsyncAction$1.onFailure(AbstractSearchAsyncAction.java:143) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n\tat org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:51) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n\tat com.floragunn.searchguard.transport.SearchGuardInterceptor$RestoringTransportResponseHandler.handleException(SearchGuardInterceptor.java:153) ~[?:?]\r\n\tat org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1024) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n\tat org.elasticsearch.transport.TcpTransport.lambda$handleException$17(TcpTransport.java:1411) ~[elasticsearch-5.2.2.jar:5.2.2]\r\n\tat org.elasticsearch.common.util.concurrent.EsExecutors$1.execute(EsExecutors.java:109) [elasticsearch-5.2.2.jar:5.2.2]\r\n\tat org.elasticsearch.transport.TcpTransport.handleException(TcpTransport.java:1409) [elasticsearch-5.2.2.jar:5.2.2]\r\n\tat org.elasticsearch.transport.TcpTransport.handlerResponseError(TcpTransport.java:1401) [elasticsearch-5.2.2.jar:5.2.2]\r\n\tat org.elasticsearch.transport.TcpTransport.messageReceived(TcpTransport.java:1345) [elasticsearch-5.2.2.jar:5.2.2]\r\n\tat org.elasticsearch.transport.netty4.Netty4MessageChannelHandler.channelRead(Netty4MessageChannelHandler.java:74) [transport-netty4-client-5.2.2.jar:5.2.2]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:363) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:349) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:341) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n\tat io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:293) [netty-codec-4.1.7.Final.jar:4.1.7.Final]\r\n\tat io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:280) [netty-codec-4.1.7.Final.jar:4.1.7.Final]\r\n\tat io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:396) [netty-codec-4.1.7.Final.jar:4.1.7.Final]\r\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:248) [netty-codec-4.1.7.Final.jar:4.1.7.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:363) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:349) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:341) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n\tat io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1139) [netty-handler-4.1.7.Final.jar:4.1.7.Final]\r\n\tat io.netty.handler.ssl.SslHandler.decode(SslHandler.java:950) [netty-handler-4.1.7.Final.jar:4.1.7.Final]\r\n\tat io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:411) [netty-codec-4.1.7.Final.jar:4.1.7.Final]\r\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:248) [netty-codec-4.1.7.Final.jar:4.1.7.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:363) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:349) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:341) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1334) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:363) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:349) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:926) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:129) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:642) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:527) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:481) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:441) [netty-transport-4.1.7.Final.jar:4.1.7.Final]\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) [netty-common-4.1.7.Final.jar:4.1.7.Final]\r\n\tat java.lang.Thread.run(Thread.java:745) [?:1.8.0_91]\r\nCaused by: org.elasticsearch.transport.RemoteTransportException: [elasticsearch5-penguin-046876f8756512345][123.12.0.2:9300][indices:data\/read\/search[phase\/query+fetch]]\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException\r\n```\r\n","id":"214972005","title":"Rare ArrayIndexOutOfBoundsException on all shards with large queries","reopen_by":"jimczi","opened_on":"2017-03-17T11:04:27Z","closed_by":"jimczi"},{"number":"23115","comments":[{"date":"2017-02-10T15:16:29Z","author":"clintongormley","text":"You're not escaping your URL correctly. Spaces should be escaped as `%20` or `+`"},{"date":"2017-02-10T15:26:24Z","author":"enguzekli","text":"I don't think that it is because of escaping. Did you check log lines? It might be an internal issue. The strange thing is that I see this exception in log and operation returns me as success. When I search, I can see the results. I think you should double check this."},{"date":"2017-02-10T15:28:51Z","author":"enguzekli","text":"A side note: I got same exception even I escape."},{"date":"2017-02-10T15:31:09Z","author":"clintongormley","text":"What exactly are you sending to Elasticsearch then?  I can't replicate this with (eg):\r\n\r\n    curl -XPUT \"http:\/\/localhost:9200\/testindex\/testtype\/id with_empty_space\" -d'{}'\r\n"},{"date":"2017-02-10T16:41:58Z","author":"clintongormley","text":"@jasontedor thinks we may not be escaping the `Location:` header correctly"},{"date":"2017-02-12T15:52:45Z","author":"jasontedor","text":"Thanks for reporting @enguzekli. I opened #23133."},{"date":"2017-02-14T12:29:46Z","author":"enguzekli","text":"You are welcome. Thanks for fixing it."}],"reopen_on":"2017-02-10T15:33:05Z","opened_by":"enguzekli","closed_on":"2017-02-13T14:34:53Z","description":"<!--\r\nGitHub is reserved for bug reports and feature requests. The best place\r\nto ask a general question is at the Elastic Discourse forums at\r\nhttps:\/\/discuss.elastic.co. If you are in fact posting a bug report or\r\na feature request, please include one and only one of the below blocks\r\nin your new issue. Note that whether you're filing a bug report or a\r\nfeature request, ensure that your submission is for an\r\n[OS that we support](https:\/\/www.elastic.co\/support\/matrix#show_os).\r\nBug reports on an OS that we do not support or feature requests\r\nspecific to an OS that we do not support will be closed.\r\n-->\r\n\r\n<!--\r\nIf you are filing a bug report, please remove the below feature\r\nrequest block and provide responses for all of the below items.\r\n-->\r\n\r\n**Elasticsearch version**: 5.2.0\r\n\r\n**Plugins installed**: -\r\n\r\n**JVM version**: Oracle JDK 1.8.0_121\r\n\r\n**OS version**: Centos 6\r\n\r\n**Description of the problem including expected versus actual behavior**:\r\n\r\n**Steps to reproduce**:\r\n 1. Add an entry with _id containing empty spaces. Sample id: **id with_empty_space** \r\n\r\n\r\n**Provide logs (if relevant)**:\r\n\r\n```\r\n[2017-02-10T17:53:08,876][WARN ][o.e.r.a.d.RestIndexAction] [TeY3Ao3] Location string is not a valid URI.\r\njava.net.URISyntaxException: Illegal character in path at index 57: \/testindex\/testtype\/id with_empty_space\r\n\tat java.net.URI$Parser.fail(URI.java:2848) ~[?:1.8.0_121]\r\n\tat java.net.URI$Parser.checkChars(URI.java:3021) ~[?:1.8.0_121]\r\n\tat java.net.URI$Parser.parseHierarchical(URI.java:3105) ~[?:1.8.0_121]\r\n\tat java.net.URI$Parser.parse(URI.java:3063) ~[?:1.8.0_121]\r\n\tat java.net.URI.<init>(URI.java:588) ~[?:1.8.0_121]\r\n\tat org.elasticsearch.action.DocWriteResponse.getLocation(DocWriteResponse.java:201) ~[elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.rest.action.document.RestIndexAction.lambda$null$0(RestIndexAction.java:96) ~[elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.rest.action.RestStatusToXContentListener.buildResponse(RestStatusToXContentListener.java:65) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.rest.action.RestStatusToXContentListener.buildResponse(RestStatusToXContentListener.java:56) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.rest.action.RestStatusToXContentListener.buildResponse(RestStatusToXContentListener.java:33) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.rest.action.RestResponseListener.processResponse(RestResponseListener.java:37) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.rest.action.RestActionListener.onResponse(RestActionListener.java:47) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.action.support.TransportAction$1.onResponse(TransportAction.java:91) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.action.support.TransportAction$1.onResponse(TransportAction.java:87) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase.finishOnSuccess(TransportReplicationAction.java:831) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$1.handleResponse(TransportReplicationAction.java:741) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase$1.handleResponse(TransportReplicationAction.java:727) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1017) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.transport.TransportService$DirectResponseChannel.processResponse(TransportService.java:1090) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1080) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1070) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.transport.DelegatingTransportChannel.sendResponse(DelegatingTransportChannel.java:58) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.transport.RequestHandlerRegistry$TransportChannelWrapper.sendResponse(RequestHandlerRegistry.java:111) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.action.support.replication.TransportReplicationAction$AsyncPrimaryAction$3.onResponse(TransportReplicationAction.java:353) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.action.support.replication.TransportReplicationAction$AsyncPrimaryAction$3.onResponse(TransportReplicationAction.java:347) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryResult.respond(TransportReplicationAction.java:401) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.action.support.replication.TransportWriteAction$WritePrimaryResult.respondIfPossible(TransportWriteAction.java:137) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.action.support.replication.TransportWriteAction$WritePrimaryResult.respond(TransportWriteAction.java:128) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.action.support.replication.TransportReplicationAction$AsyncPrimaryAction$2.onResponse(TransportReplicationAction.java:320) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.action.support.replication.TransportReplicationAction$AsyncPrimaryAction$2.onResponse(TransportReplicationAction.java:317) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.action.support.replication.ReplicationOperation.finish(ReplicationOperation.java:303) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.action.support.replication.ReplicationOperation.decPendingAndFinishIfNeeded(ReplicationOperation.java:284) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.action.support.replication.ReplicationOperation.execute(ReplicationOperation.java:132) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.action.support.replication.TransportReplicationAction$AsyncPrimaryAction.onResponse(TransportReplicationAction.java:327) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.action.support.replication.TransportReplicationAction$AsyncPrimaryAction.onResponse(TransportReplicationAction.java:262) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.action.support.replication.TransportReplicationAction$1.onResponse(TransportReplicationAction.java:864) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.action.support.replication.TransportReplicationAction$1.onResponse(TransportReplicationAction.java:861) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.index.shard.IndexShardOperationsLock.acquire(IndexShardOperationsLock.java:142) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.index.shard.IndexShard.acquirePrimaryOperationLock(IndexShard.java:1652) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.action.support.replication.TransportReplicationAction.acquirePrimaryShardReference(TransportReplicationAction.java:873) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.action.support.replication.TransportReplicationAction.access$400(TransportReplicationAction.java:92) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.action.support.replication.TransportReplicationAction$AsyncPrimaryAction.doRun(TransportReplicationAction.java:279) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:258) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:250) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:69) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.transport.TransportService$7.doRun(TransportService.java:610) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:596) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-5.2.0.jar:5.2.0]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_121]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_121]\r\n\tat java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]\r\n\r\n```","id":"206820678","title":"Strange Exception in ElasticSearch Logs","reopen_by":"clintongormley","opened_on":"2017-02-10T15:11:23Z","closed_by":"jasontedor"},{"number":"22881","comments":[{"date":"2017-02-01T14:51:09Z","author":"giorgosp","text":"Hello @clintongormley @jpountz, I am looking for my first elasticsearch contribution!\r\n\r\nHowever, I cannot reproduce this bug on my fresh elasticsearch build. \r\nThis is the response for `GET \/`:\r\n```\r\n{\r\n  \"name\": \"node-0\",\r\n  \"cluster_name\": \"distribution_run\",\r\n  \"cluster_uuid\": \"fQLnWXIERNuBj5vWoNtBlw\",\r\n  \"version\": {\r\n    \"number\": \"6.0.0-alpha1\",\r\n    \"build_hash\": \"a4ac29c\",\r\n    \"build_date\": \"2017-01-23T13:54:32.228Z\",\r\n    \"build_snapshot\": true,\r\n    \"lucene_version\": \"6.4.0\"\r\n  },\r\n  \"tagline\": \"You Know, for Search\"\r\n}\r\n```\r\n \r\nand this is what I get for the issue's request:\r\n```\r\nGET \/_search\r\n{\r\n  \"aggs\": {\r\n    \"foobar\": {\r\n      \"range\": {\r\n        \"field\": \"hey\"\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n```\r\n{\r\n  \"took\": 27,\r\n  \"timed_out\": false,\r\n  \"_shards\": {\r\n    \"total\": 5,\r\n    \"successful\": 5,\r\n    \"failed\": 0\r\n  },\r\n  \"hits\": {\r\n    \"total\": 0,\r\n    \"max_score\": null,\r\n    \"hits\": []\r\n  },\r\n  \"aggregations\": {\r\n    \"foobar\": {\r\n      \"buckets\": []\r\n    }\r\n  }\r\n}\r\n```\r\nMy `java -version` on `OS X 10.11.6`\r\n```\r\njava version \"1.8.0_60\"\r\nJava(TM) SE Runtime Environment (build 1.8.0_60-b27)\r\nJava HotSpot(TM) 64-Bit Server VM (build 25.60-b23, mixed mode)\r\n```\r\n\r\nI understand that my build is 1 version later, should I build the 5.1.1 version and check the issue?\r\nThanks!"},{"date":"2017-02-01T16:51:56Z","author":"jpountz","text":"Did you test on master or the 5.x branch? If it does not reproduce on 5.x, then then 5.1 branch is indeed what I would look at next."},{"date":"2017-02-01T23:18:39Z","author":"giorgosp","text":"Yes I was testing on master. I just checked 5.x, 5.1, 5.2 and it does reproduce on 5.1 only.\r\nSo I guess no further action is needed since it is ok on master and 5.x?"},{"date":"2017-02-02T00:27:53Z","author":"jpountz","text":"Good to know that issue is already fixed in 5.2! Thank you for investigating @giorgosp, we appreciate! I will close this issue now."},{"date":"2017-02-02T09:54:32Z","author":"jimczi","text":"@giorgosp I don't think it's resolved in 5.x nor master. The only difference with 5.1 is that you need to have at least one document with the range field to fail the query. The following recreation for instance fails in 5.1, 5.2, 5.x and master:\r\n\r\n`````\r\nPUT t\/t\/1\r\n{\r\n   \"number\": 3\r\n}\r\n\r\nGET \/_search\r\n{\r\n  \"aggs\": {\r\n    \"foobar\": {\r\n      \"range\": {\r\n        \"field\": \"number\"\r\n      }\r\n    }\r\n  }\r\n}\r\n`````"},{"date":"2017-02-02T22:58:25Z","author":"giorgosp","text":"Ok, I am checking it"},{"date":"2017-02-03T01:41:07Z","author":"giorgosp","text":"Ok, it fails  if the range aggregation query is invalid and the \"field\" value is an existing field inside the document (changing \"field\": \"number\" to \"field\": \"number1123\" doesn't reproduce the issue).\r\n\r\nI will keep working on it in my spare time but since I am also trying to familiarize myself with the codebase, if someone else wants to fix it faster than me then it is ok."}],"reopen_on":"2017-02-02T09:54:32Z","opened_by":"damienalexandre","closed_on":null,"description":"**Elasticsearch version**:  5.1.1\r\n\r\n**Plugins installed**: [analysis-icu]\r\n\r\n**JVM version**: Oracle Corporation\/OpenJDK 64-Bit Server VM\/1.8.0_111-internal\/25.111-b14\r\n\r\n**OS version**: Linux\/4.8.0-34-generic\/amd64\r\n\r\n**Description of the problem including expected versus actual behavior**: \r\n\r\nA NullPointerException is displayed to the user when the Range aggregation is malformed:\r\n\r\n```json\r\n{\r\n  \"error\": {\r\n    \"root_cause\": [\r\n      {\r\n        \"type\": \"null_pointer_exception\",\r\n        \"reason\": null\r\n      }\r\n    ],\r\n    \"type\": \"null_pointer_exception\",\r\n    \"reason\": null\r\n  },\r\n  \"status\": 500\r\n}\r\n```\r\n\r\n\r\n**Steps to reproduce**:\r\n\r\n 1. Run a badly formatted query:\r\n\r\n```json\r\nGET \/_search\r\n{\r\n  \"aggs\": {\r\n    \"foobar\": {\r\n      \"range\": {\r\n        \"field\": \"hey\"\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n**Provide logs (if relevant)**:\r\n\r\nTrace:\r\n\r\n```\r\n[2017-01-31T10:49:11,250][WARN ][r.suppressed             ] path: \/_search, params: {}\r\njava.lang.NullPointerException: null\r\n\tat org.elasticsearch.search.aggregations.bucket.range.RangeParser.createFactory(RangeParser.java:58) ~[elasticsearch-5.1.1.jar:5.1.1]\r\n\tat org.elasticsearch.search.aggregations.bucket.range.RangeParser.createFactory(RangeParser.java:39) ~[elasticsearch-5.1.1.jar:5.1.1]\r\n\tat org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser.parse(AbstractValuesSourceParser.java:150) ~[elasticsearch-5.1.1.jar:5.1.1]\r\n\tat org.elasticsearch.search.aggregations.support.AbstractValuesSourceParser$NumericValuesSourceParser.parse(AbstractValuesSourceParser.java:48) ~[elasticsearch-5.1.1.jar:5.1.1]\r\n\tat org.elasticsearch.search.aggregations.AggregatorParsers.parseAggregators(AggregatorParsers.java:156) ~[elasticsearch-5.1.1.jar:5.1.1]\r\n\tat org.elasticsearch.search.aggregations.AggregatorParsers.parseAggregators(AggregatorParsers.java:80) ~[elasticsearch-5.1.1.jar:5.1.1]\r\n\tat org.elasticsearch.search.builder.SearchSourceBuilder.parseXContent(SearchSourceBuilder.java:1018) ~[elasticsearch-5.1.1.jar:5.1.1]\r\n\tat org.elasticsearch.rest.action.search.RestSearchAction.parseSearchRequest(RestSearchAction.java:105) ~[elasticsearch-5.1.1.jar:5.1.1]\r\n\tat org.elasticsearch.rest.action.search.RestSearchAction.prepareRequest(RestSearchAction.java:81) ~[elasticsearch-5.1.1.jar:5.1.1]\r\n\tat org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:66) ~[elasticsearch-5.1.1.jar:5.1.1]\r\n\tat org.elasticsearch.rest.RestController.executeHandler(RestController.java:243) ~[elasticsearch-5.1.1.jar:5.1.1]\r\n\tat org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:200) [elasticsearch-5.1.1.jar:5.1.1]\r\n\tat org.elasticsearch.http.HttpServer.dispatchRequest(HttpServer.java:113) [elasticsearch-5.1.1.jar:5.1.1]\r\n\tat org.elasticsearch.http.netty4.Netty4HttpServerTransport.dispatchRequest(Netty4HttpServerTransport.java:507) [transport-netty4-5.1.1.jar:5.1.1]\r\n\tat org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:69) [transport-netty4-5.1.1.jar:5.1.1]\r\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:373) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:359) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:351) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat org.elasticsearch.http.netty4.pipelining.HttpPipeliningHandler.channelRead(HttpPipeliningHandler.java:66) [transport-netty4-5.1.1.jar:5.1.1]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:373) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:359) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:351) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) [netty-codec-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:373) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:359) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:351) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) [netty-codec-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:373) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:359) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:351) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) [netty-codec-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:373) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:359) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:351) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:293) [netty-codec-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:267) [netty-codec-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:373) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:359) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:351) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:373) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:359) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:351) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1334) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:373) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:359) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:926) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:129) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:651) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:536) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:490) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:450) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:873) [netty-common-4.1.6.Final.jar:4.1.6.Final]\r\n\tat java.lang.Thread.run(Thread.java:745) [?:1.8.0_111-internal]\r\n```\r\n\r\n","id":"204260836","title":"Range aggregation with no ranges throw a NullPointerException","reopen_by":"jimczi","opened_on":"2017-01-31T10:50:30Z","closed_by":"jpountz"},{"number":"22745","comments":[{"date":"2017-01-23T15:27:53Z","author":"clintongormley","text":"Fuzziness is behaving correctly here.  The idea is that words will be spelled correctly most of the time, with a few misspellings, so with fuzziness we give a slight edge to terms that appear more frequently, as this is more likely to be the correct spelling.\r\n\r\nIf your correct spelling is the only occurrence in the document collection, then it's going to rank more poorly"},{"date":"2017-01-23T16:20:15Z","author":"jeantil","text":"Let me rephrase so I am sure I understand : \r\nYou are saying the reason why `abb` gets score for `fuzzy aab` _and_ `exact abb` while `aae` gets score only for `exact aae` and not for fuzzy on `aab` is to give the  edge to the term appearing most frequently event if it is a fuzzy match ? \r\n\r\nThis is effectively overriding an exact match with a fuzzy match which runs counter to what both #9105 ( match\/multi_match +fuzzy replace FLT ) and #5883 (don't rank fuzzy above exact match) were saying... \r\n\r\nIs there a way to configure this behaviour ? \r\nWith FLT there was an option to disable TF\/IDF for specific use cases. I would like an option to always rank exact matches over fuzzy matches. \r\n\r\nMy exact use case is searching for cars and this bug breaks the search for porsche cars ranking the `911 type 991` over the `911 type 997` for people who searched for `911  997`. We have been stuck on 1.7.X ever since the fuzzy like this search was removed and I was really looking forward to being able to get back on the upgrade trains with #9103 fixed ..."},{"date":"2017-01-23T18:15:54Z","author":"jeantil","text":"The more I think about this the worse it sounds : \r\n\"A correctly spelled but rarer term is less relevant than a misspelled but more frequent term ? \" that's one hell of a confirmation bias ... you would never see rarer results shadowed by a more frequent spelling. ( in my specific car example I have over 40 `911 type 991` results before the first correctly spelled case this would mean page 4 on google ...\r\n\r\nSo please consider this a feature request to be able to tweak the relevance model."},{"date":"2017-01-23T18:26:30Z","author":"clintongormley","text":"@jeantil it doesn't work like that.  What it should do is say: \r\n\r\n* which terms are within edit distance 1\/2 of `xxx`\r\n* find the min IDF of all the fuzzy terms\r\n* use that IDF for all terms (with a tiny boost for the current term)\r\n\r\nSo this is working correctly.  However, I think there is a different bug: the more fuzzy terms that match, the higher the score.\r\n\r\nWith explain, the score for the first two matching documents includes this:\r\n\r\n```\r\n            {\r\n              \"value\": 0.5333638,\r\n              \"description\": \"sum of:\",\r\n              \"details\": [\r\n                {\r\n                  \"value\": 0.21334553,\r\n                  \"description\": \"weight(foo:aab in 1) [PerFieldSimilarity], result of:\",\r\n                  \"details\": []\r\n                    }\r\n                  ]\r\n                },\r\n                {\r\n                  \"value\": 0.3200183,\r\n                  \"description\": \"weight(foo:abb in 1) [PerFieldSimilarity], result of:\",\r\n                  \"details\": []\r\n                }\r\n              ]\r\n            }\r\n```\r\n\r\nWhile the last matching doc has just this:\r\n\r\n```\r\n            {\r\n              \"value\": 0.3200183,\r\n              \"description\": \"sum of:\",\r\n              \"details\": [\r\n                {\r\n                  \"value\": 0.3200183,\r\n                  \"description\": \"weight(foo:abb in 3) [PerFieldSimilarity], result of:\",\r\n                  \"details\": []\r\n                }\r\n              ]\r\n            }\r\n```\r\n\r\nThat's just wrong.  Instead, we should be taking the max score for each of these fuzzy \"synonyms\".\r\n\r\n@jimczi what do you think?"},{"date":"2017-01-23T19:10:40Z","author":"clintongormley","text":"Oh I'm stupid - `abb` is matching both terms (`aab` and `abb`), that's why it is being summed.\r\n\r\nNot a bug.  Closing"},{"date":"2017-01-23T19:28:42Z","author":"jimczi","text":"Right the query matches the fuzzy terms twice. It's more obvious with the `validate` API:\r\n\r\n````\r\nGET \/_validate\/query?explain&rewrite\r\n{\r\n  \"query\": {\r\n    \"bool\": {\r\n      \"should\": [\r\n        {\r\n          \"multi_match\": {\r\n            \"query\": \"aae abb\",\r\n            \"fields\": \"foo\",\r\n            \"fuzziness\": \"AUTO\",\r\n            \"fuzzy_rewrite\": \"top_terms_blended_freqs_10\"\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n   \"valid\": true,\r\n   \"_shards\": {\r\n      \"total\": 1,\r\n      \"successful\": 1,\r\n      \"failed\": 0\r\n   },\r\n   \"explanations\": [\r\n      {\r\n         \"index\": \"t\",\r\n         \"valid\": true,\r\n         \"explanation\": \"((foo:aab)^0.6666666 foo:aae) ((foo:aab)^0.6666666 foo:abb)\"\r\n      }\r\n   ]\r\n}\r\n`````\r\nThe boost is computed based on the distance with the exact string.\r\n"},{"date":"2017-01-23T20:22:15Z","author":"jeantil","text":"While I understand the current logic, I still argue that it effectively ends up ranking an exact match below a fuzzy match because it matches the *same* input token twice again different document tokens even when one of the document token has an exact match with a differnt input token. \r\n\r\nhere abb in the input is an exact match for abb in the document, yet aab is fuzzy matched with both aae and abb ?\r\n \r\n@clintongormley  Is there any way to tweak the ranking algorithm to avoid scoring an input token against an output token which already has an exact match (or anything which would alway result in full exact matches to actually be ranked above partial exact match+fuzzy match really)  ?\r\n\r\n@jimczi the validate result seems very weird as it doesn't account for the exact match abb<->abb does this stem from using a specific rewrite `top_terms_blended_freqs_10` ? "},{"date":"2017-01-24T07:55:52Z","author":"jimczi","text":"@jeantil the validate query uses `top_terms_blended_freqs_10` which is the default rewrite method (except that we keep the top 50 by default). If you want to have exact match always first, you can add the same `match` query without the fuzziness and with the operator set to `AND` and boost that query with a ridiculous number. Though your example is really an edge case, if your language produces only three letter words with an alphabet of two letters I understand that you may have some problems with the fuzziness but that should not be a problem with real words. You can also set the `prefix_length` > 0 in order to skip small words in the fuzzy scoring."},{"date":"2017-01-24T08:33:07Z","author":"jeantil","text":">that should not be a problem with real words\r\n\r\nyet I did provide a very real world example of why this issue is important to me. We have a search for cars from all manufacturers. You probably heard of the Porshe 911 in the real world. Except that's a fairly old line of cards so porsche created multiple versions of the Porsche 911 : \r\n* 911 type 911\r\n* 911 type 930\r\n* 911 type 964\r\n* 911 type 991\r\n* 911 type 993\r\n* 911 type 996\r\n* 911 type 997\r\n\r\nAccording to our domain experts, people \"in the know\" talk and search these cars using `911 997`guess what happens ? using ES 5.x our search returns the 40 different motorisations of 911 type 991 before the first 911 type 997.\r\n\r\nThis is not the only brand causing issues. BMW classifies cars as Serie 1, Serie 2, Serie 3, etc ... then goes to decline the series in different categories (E30) Berline, (E36) Berline, (E46) cabrio, etc\r\nBut that's only in the catalog, on the cards they write 320d, 318i, 316xxx\r\n\r\nAnd people use that to refer to their cars and therefore to search for it, I'll let you guess what happened to my test cases when I switched to 1.7.x with the now retired Fuzzy Like This to 5.x with a fuzzy multimatch. At the same time, in the same index, we have brands who enjoy much longer names in which typos are easily found. \r\n\r\n\r\nI took the time to create an easily reproductible test case but that was driven by a very real world issue. \r\n\r\nby the way our current query is already heavily tweaked: \r\n\r\n```\r\nGET vehicle_fr_fr\/cartype\/_search?search_type=dfs_query_then_fetch\r\n{\r\n  \"size\" : 100,\r\n  \"query\" : {\r\n    \"bool\" : {\r\n      \"should\" : [ {\r\n        \"flt\" : {\r\n          \"fields\" : [ \"motor_search\", \"motor_keywords\", \"cartype_search\", \"cartype_keywords\", \"segment_search\", \"segment_keywords\", \"maker_search\", \"maker_keywords\" ],\r\n          \"like_text\" : \"energy saver+\",\r\n          \"fuzziness\" : \"AUTO\",\r\n          \"ignore_tf\" : true,\r\n          \"boost\" : 0.5\r\n        }\r\n      }, {\r\n        \"constant_score\" : {\r\n          \"query\" : {\r\n            \"multi_match\" : {\r\n              \"query\" : \"energy saver+\",\r\n              \"fields\" : [ \"phrase\" ],\r\n              \"type\" : \"phrase\"\r\n            }\r\n          },\r\n          \"boost\" : 3.0\r\n        }\r\n      }, {\r\n        \"constant_score\" : {\r\n          \"query\" : {\r\n            \"multi_match\" : {\r\n              \"query\" : \"energy saver+\",\r\n              \"fields\" : [ \"motor_search\", \"cartype_search\", \"segment_search\", \"maker_search\" ]\r\n            }\r\n          },\r\n          \"boost\" : 3.0\r\n        }\r\n      }, {\r\n        \"constant_score\" : {\r\n          \"query\" : {\r\n            \"multi_match\" : {\r\n              \"query\" : \"energy saver+\",\r\n              \"fields\" : [ \"motor_keywords\", \"cartype_keywords\", \"segment_keywords\", \"maker_keywords\" ]\r\n            }\r\n          },\r\n          \"boost\" : 3.0\r\n        }\r\n      } ]\r\n    }\r\n  },\r\n  \"highlight\" : {\r\n    \"pre_tags\" : [ \"{\" ],\r\n    \"post_tags\" : [ \"}\" ],\r\n    \"require_field_match\" : true,\r\n    \"fields\" : {\r\n      \"phrase\" : {\r\n        \"type\" : \"fvh\"\r\n      },\r\n      \"motor_search\" : {\r\n        \"type\" : \"fvh\"\r\n      },\r\n      \"motor_keywords\" : {\r\n        \"type\" : \"fvh\"\r\n      },\r\n      \"cartype_search\" : {\r\n        \"type\" : \"fvh\"\r\n      },\r\n      \"cartype_keywords\" : {\r\n        \"type\" : \"fvh\"\r\n      },\r\n      \"segment_search\" : {\r\n        \"type\" : \"fvh\"\r\n      },\r\n      \"segment_keywords\" : {\r\n        \"type\" : \"fvh\"\r\n      },\r\n      \"maker_search\" : {\r\n        \"type\" : \"fvh\"\r\n      },\r\n      \"maker_keywords\" : {\r\n        \"type\" : \"fvh\"\r\n      }\r\n    }\r\n  }\r\n}\r\n```"},{"date":"2017-01-24T09:00:37Z","author":"jimczi","text":"Sorry my wording was not clear enough. You should not try to apply fuzziness to a three letter word. If people search for `911 997` you can maybe make the words optional but applying fuzziness to this query is problematic for scoring even when the max distance is 1.\r\nI can see how the \"rank exact match first\" can be useful but it would require a special query for this purpose. Currently each clause is independent from each other so they can only boost exact match at the word level. Having multiple clauses that match the same fuzzy word is an edge case that you can counterbalance by removing problematic words.  \r\nWe can continue the discussion on the forum if you want:\r\nhttps:\/\/discuss.elastic.co\/c\/elasticsearch\r\nYou have some nice cars in your parking ;)\r\n"},{"date":"2017-01-24T09:49:44Z","author":"jeantil","text":"Thanks for taking the time. I fail to understand how I can \"remove the problematic words\" since they convey 100% of the information I am trying to retrieve. I have created a topic on the forum to continue this disussion: \r\nhttps:\/\/discuss.elastic.co\/t\/fuzzy-query-ranks-misspellings-over-exact-for-repeated-close-tokens\/72605\r\n\r\nThanks again for your help. "}],"reopen_on":"2017-01-23T18:21:05Z","opened_by":"jeantil","closed_on":"2017-01-23T19:10:40Z","description":"**Elasticsearch version**: 5.1.2\r\n\r\n**Plugins installed**: [] (no plugins installed)\r\n\r\n**JVM version** :OpenJDK 64-Bit Server VM\/1.8.0_111\/25.111-b14\r\n\r\n**OS version**: MacOS sierra 10.12.2 (16C67)\r\n\r\n**Description of the problem including expected versus actual behavior**: \r\nDespite #9103, match\/multimatch ranks misspellings over exact matches in some fairly precise conditions.\r\n\r\n**Steps to reproduce**:\r\n```\r\nDELETE \/t\r\n\r\nPUT \/t\r\n{\r\n  \"settings\": {\r\n    \"number_of_shards\": 1\r\n  }\r\n}\r\n# You must have at least one non matching document to trigger the problem. The more non matching documents in the index, the worse the score difference.\r\nPOST \/t\/t\r\n{\"foo\": \"foo bar\"}\r\n\r\nPOST \/t\/t\r\n{\"foo\": \"aab abb\"}\r\nPOST \/t\/t\r\n{\"foo\": \"aab abb\"}\r\n\r\nPOST \/t\/t\r\n{\"foo\": \"aae abb\"}\r\n\r\n\r\nGET \/_search\r\n{\r\n  \"query\": {\r\n    \"bool\": {\r\n      \"should\": [\r\n        {\r\n          \"multi_match\": {\r\n            \"query\": \"aae abb\",\r\n            \"fields\": \"foo\",\r\n            \"fuzziness\": \"AUTO\"\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nI expect\r\n```\r\n{\r\n  \"took\": 3,\r\n  \"timed_out\": false,\r\n  \"_shards\": {\r\n    \"total\": 2,\r\n    \"successful\": 2,\r\n    \"failed\": 0\r\n  },\r\n  \"hits\": {\r\n    \"total\": 3,\r\n    \"max_score\": 0.94192845,\r\n    \"hits\": [\r\n      {\r\n        \"_index\": \"t\",\r\n        \"_type\": \"t\",\r\n        \"_id\": \"AVnLTHvstUtjT09CXNTY\",\r\n        \"_score\":  0.94192845,\r\n        \"_source\": {\r\n          \"foo\": \"aae abb\"\r\n        },\r\n      {\r\n        \"_index\": \"t\",\r\n        \"_type\": \"t\",\r\n        \"_id\": \"AVnLTHsZtUtjT09CXNTW\",\r\n        \"_score\": 0,73462508,\r\n        \"_source\": {\r\n          \"foo\": \"aab abb\"\r\n        }\r\n      },\r\n      {\r\n        \"_index\": \"t\",\r\n        \"_type\": \"t\",\r\n        \"_id\": \"AVnLTHuGtUtjT09CXNTX\",\r\n        \"_score\": 0,73462508,\r\n        \"_source\": {\r\n          \"foo\": \"aab abb\"\r\n        }\r\n      }\r\n      }\r\n    ]\r\n  }\r\n}\r\n``` \r\n\r\nI get \r\n```\r\n{\r\n  \"took\": 3,\r\n  \"timed_out\": false,\r\n  \"_shards\": {\r\n    \"total\": 2,\r\n    \"successful\": 2,\r\n    \"failed\": 0\r\n  },\r\n  \"hits\": {\r\n    \"total\": 3,\r\n    \"max_score\": 0.9479706,\r\n    \"hits\": [\r\n      {\r\n        \"_index\": \"t\",\r\n        \"_type\": \"t\",\r\n        \"_id\": \"AVnLTHsZtUtjT09CXNTW\",\r\n        \"_score\": 0.9479706,\r\n        \"_source\": {\r\n          \"foo\": \"aab abb\"\r\n        }\r\n      },\r\n      {\r\n        \"_index\": \"t\",\r\n        \"_type\": \"t\",\r\n        \"_id\": \"AVnLTHuGtUtjT09CXNTX\",\r\n        \"_score\": 0.9479706,\r\n        \"_source\": {\r\n          \"foo\": \"aab abb\"\r\n        }\r\n      },\r\n      {\r\n        \"_index\": \"t\",\r\n        \"_type\": \"t\",\r\n        \"_id\": \"AVnLTHvstUtjT09CXNTY\",\r\n        \"_score\": 0.94192845,\r\n        \"_source\": {\r\n          \"foo\": \"aae abb\"\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\nLooking at the explain. The norms factor is always the same: 0.89722675 (I'll call it $norms)\r\n it seems that for `aab abb` the score `0.9479706`\r\n```\r\n{\r\n  0.6931472 ~ [aab]*0.6666666 ~ [fuzzy of aae?])*$norms + \r\n {\r\n  0.35667494 ~ [aab]*0.6666666 ~ [fuzzy of abb?]*$norms +\r\n  0.35667494 ~ [exact abb]*$norms\r\n } \r\n}\r\n```\r\n\r\nWhereas for `aae abb`  I get a score of `0.9419285` as\r\n```\r\n{\r\n  0.6931472~[exact aae])*$norms +  \r\n  0.35667494~[exact abb]*$norms\r\n}\r\n```","id":"202540035","title":"Fuzzy query ranks misspellings over exact for repeated \"close\" tokens","reopen_by":"clintongormley","opened_on":"2017-01-23T13:56:21Z","closed_by":"clintongormley"},{"number":"22500","comments":[{"date":"2017-01-09T15:30:28Z","author":"clintongormley","text":"Hi @Rogier75 \r\n\r\nCould you try removing the aggs and see if that resolves the issue?  Also, if you're able to provide a set of working documents and mappings etc that reliably trigger this failure, it would be very helpful."},{"date":"2017-01-10T09:01:32Z","author":"Rogier75","text":"I have not yet been able to produce any test data because any new index that I create does not trigger the bug.\r\n\r\nWhat I can see is the following:\r\n\r\n**Search and scroll _with_ aggregation**:\r\na. Old index with session.id type is string: search is successful, but scroll returns 0 hits.\r\nb. Old index with session.id type is text: search is successful, but scroll gets ReduceSearchPhaseException caused by NullPointerException\r\nc. New index >= 5.0 search and scroll are successful\r\n\r\n**Search and scroll _without_ aggregation**:\r\nSearch and scroll without aggregation is successful on both old and new indexes\r\n\r\nI now plan to reindex my data and or remove old data. "},{"date":"2017-01-10T12:33:40Z","author":"clintongormley","text":"@jpountz any ideas what might be happening here?"},{"date":"2017-01-11T10:43:04Z","author":"Rogier75","text":"I was able to reproduce the empty scroll after a successful search. This has worked for me in the past even though mentioned [here](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/search-request-scroll.html) is:\r\n\r\n> If the request specifies aggregations, only the initial search response will contain the aggregations results.\r\n\r\nTo reproduce the empty scroll result on Elasticsearch 5.1.1:\r\n\r\n**Test data**: contains 18 documents with unique session.id\r\nhttps:\/\/gist.github.com\/Rogier75\/27136d1996dfc50c90c31ad07c75a1e9\r\n\r\n**Data mapping** (generated):\r\nhttps:\/\/gist.github.com\/Rogier75\/dfe6a6b0bcec81075a880d6090096193\r\n\r\n**Search query**: GET github_22500\/_search\/?scroll=1m\r\nhttps:\/\/gist.github.com\/Rogier75\/cd515adc269159ab08b45de29bd3333c\r\n\r\n**Search result**: successfully returns 10 hits\r\nhttps:\/\/gist.github.com\/Rogier75\/d4c135bfcc0a29b8daf0d27ff9f714eb\r\n\r\n**Scroll**: GET _search\/scroll?scroll=1m\r\nDnF1ZXJ5VGhlbkZldGNoAgAAAAAAB7uQFjczYWFZNzVCU2MtSTA0TGQwbkpDaGcAAAAAAAezeBZkSUhyYXI4NVE3S19TX0cwRU9PdDhR\r\n\r\n**Scroll result**: is empty where expected 8 hits\r\n```\r\n{\r\n   \"_scroll_id\":\"DnF1ZXJ5VGhlbkZldGNoAgAAAAAAB7uQFjczYWFZNzVCU2MtSTA0TGQwbkpDaGcAAAAAAAezeBZkSUhyYXI4NVE3S19TX0cwRU9PdDhR\",\r\n   \"took\":1,\r\n   \"timed_out\":false,\r\n   \"_shards\":{\r\n      \"total\":2,\r\n      \"successful\":2,\r\n      \"failed\":0\r\n   },\r\n   \"hits\":{\r\n      \"total\":18,\r\n      \"max_score\":0,\r\n      \"hits\":[\r\n\r\n      ]\r\n   }\r\n}\r\n```\r\n"},{"date":"2017-01-11T11:02:26Z","author":"jimczi","text":"I don't understand what you're trying to achieve. If you send a scroll query with `size:0` it is expected that you get no hits. In fact with `size:0` you are sure that the first query will return no hits and then your scroll is considered as finish.\r\nRegarding the aggregation only the initial query will return result for the aggs, subsequent queries with `_scroll` will not compute the aggregations. Can you explain why you use `size:0` and aggregations in a `scroll` query ? This is not intended to be used like this so unless you find a reproduction for the NPE I think we can consider that this issue is solved."},{"date":"2017-01-11T11:48:19Z","author":"Rogier75","text":"@jimczi \r\nPlease bear with me for one second and let me explain what I'm trying to do. If you tell me that it is impossible than I will implement another solution:\r\n\r\nImagine the following 4 documents exist with a timestamp and session.id:\r\n1. timestamp: 18:50 session.id: 1\r\n2. timestamp: 18:55 session.id: 1\r\n3. timestamp: 19:00 session.id: 2\r\n4. timestamp: 19:05 session.id: 2\r\n\r\nI want to search and scroll and want to find one document for each unique session.id but get only the ones with the earliest timestamp. So in the above case I want to get document 1 and 3.\r\n\r\nIf I remove the size:0 I can scroll further but since there is no aggregated result I don't get the documents that I am querying for.\r\n\r\nI guess that it would be best for me to create a separate index to contain only (a duplicate of) those unique documents by session.id with the earliest time stamp. On that index I could then search and scroll without aggregation.\r\n\r\n"},{"date":"2017-01-11T12:18:52Z","author":"jimczi","text":"You'll have to make the collapsing on the client side if you want to use the scroll. You could for instance do a query like:\r\n````\r\nGET _search?scroll=1m\r\n{\r\n  \"query\": {\r\n    \"exists\": {\r\n        \"field\": \"session\"\r\n    }\r\n  },\r\n  \"sort\": [\r\n     {\r\n        \"session\": {\r\n           \"order\": \"desc\"\r\n        },\r\n        \"timestamp\": {\r\n            \"order\": \"asc\"\r\n        }\r\n     }\r\n  ]\r\n}\r\n````\r\n... and then for each result set you would keep the first result of each session id.\r\nWe're also iterating on a collapse feature for search requests but only pagination would be possible (no scroll):\r\nhttps:\/\/github.com\/elastic\/elasticsearch\/pull\/22337"},{"date":"2017-01-11T13:32:04Z","author":"Rogier75","text":"Thanks @jimczi \r\n\r\n#22337 does look interesting as it supports pagination. Until that's final I will create a new index (or new type in an existing index) that contains the unique session documents with earliest time stamp. It's a small bit of duplication but it allows for search and scroll and pagination."},{"date":"2017-01-11T15:51:00Z","author":"jimczi","text":"Though we should continue to investigate what's causing the NPE. I am reopening this issue because the NPE may be due to a bug and it would be beneficial to know what's causing it. Can you share your mapping and the steps you followed to reach the state where you get exceptions ? You're mentioning that you use an old index ? What does that mean ? You created the index in an older version and you've upgraded to 5.x ?"},{"date":"2017-01-11T16:55:33Z","author":"Rogier75","text":"To get the exact mapping at the time I wrote this issue I need to put back an old snapshot on a different host. I'll try to that tomorrow. Since then I have re-indexed my data and fields that had previously been generated as type text have now become type string. I actually think that the _string_ vs _text_ type has something to do with the issue.\r\n\r\nThe index was created in an older version after which I upgraded to 5.x. Is there a way to see in which version the index was created?\r\n\r\n"},{"date":"2017-01-11T20:11:28Z","author":"Rogier75","text":"@jimczi \r\nI have recreated the NullPointerException.\r\n\r\nmapping:\r\nhttps:\/\/gist.github.com\/Rogier75\/752507cd3affd4235a249a9d27abedd2#file-github_22500_mapping-txt\r\n\r\ndata:\r\nhttps:\/\/gist.github.com\/Rogier75\/27136d1996dfc50c90c31ad07c75a1e9\r\n\r\nsearch query:\r\nhttps:\/\/gist.github.com\/Rogier75\/cd515adc269159ab08b45de29bd3333c\r\n\r\nSearch is successful\r\n\r\nScroll fails with ReduceSearchPhaseException Caused by NullPointerException\r\n\r\n\r\n_Update_:\r\nI deleted the index, created it again and now the NullPointerException does not happen. Sorry, but this is proving to be harder then I thought."},{"date":"2017-01-11T22:12:02Z","author":"Rogier75","text":"What we can learn from this is that it can happen on a newly created index in Elasticsearch 5.1.1 and is not related to the index being created in an older version."},{"date":"2017-01-11T22:15:42Z","author":"jimczi","text":"I cannot reproduce with my single node test and your recreation. If you reproduce can you keep the stack trace of the NPE and copy it here."},{"date":"2017-01-11T22:21:37Z","author":"Rogier75","text":"```\r\n[2017-01-11T20:59:29,168][WARN ][r.suppressed             ] path: \/_search\/scroll, params: {scroll=1m}\r\norg.elasticsearch.action.search.ReduceSearchPhaseException: [reduce] inner finish failed\r\n        at org.elasticsearch.action.search.SearchScrollQueryThenFetchAsyncAction.finishHim(SearchScrollQueryThenFetchAsyncAction.java:217) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at org.elasticsearch.action.search.SearchScrollQueryThenFetchAsyncAction.executeFetchPhase(SearchScrollQueryThenFetchAsyncAction.java:175) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at org.elasticsearch.action.search.SearchScrollQueryThenFetchAsyncAction.access$000(SearchScrollQueryThenFetchAsyncAction.java:44) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at org.elasticsearch.action.search.SearchScrollQueryThenFetchAsyncAction$1.onResponse(SearchScrollQueryThenFetchAsyncAction.java:135) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at org.elasticsearch.action.search.SearchScrollQueryThenFetchAsyncAction$1.onResponse(SearchScrollQueryThenFetchAsyncAction.java:129) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:46) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:978) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at org.elasticsearch.transport.TransportService$DirectResponseChannel.processResponse(TransportService.java:1055) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1039) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1029) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at org.elasticsearch.transport.DelegatingTransportChannel.sendResponse(DelegatingTransportChannel.java:58) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at org.elasticsearch.transport.RequestHandlerRegistry$TransportChannelWrapper.sendResponse(RequestHandlerRegistry.java:111) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at org.elasticsearch.action.search.SearchTransportService$8.messageReceived(SearchTransportService.java:317) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at org.elasticsearch.action.search.SearchTransportService$8.messageReceived(SearchTransportService.java:313) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:69) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at org.elasticsearch.transport.TransportService$6.doRun(TransportService.java:577) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:527) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0]\r\n        at java.lang.Thread.run(Thread.java:744) [?:1.8.0]\r\nCaused by: java.lang.NullPointerException\r\n```"}],"reopen_on":"2017-01-11T15:51:00Z","opened_by":"Rogier75","closed_on":null,"description":"**Elasticsearch version**: 5.1.1\r\n```[root@atom1 jenkins]# curl -XGET 'localhost:9200'\r\n{\r\n  \"name\" : \"atom1\",\r\n  \"cluster_name\" : \"atomcluster\",\r\n  \"cluster_uuid\" : \"Ax602ATwRUanUplRc3d5iA\",\r\n  \"version\" : {\r\n    \"number\" : \"5.1.1\",\r\n    \"build_hash\" : \"5395e21\",\r\n    \"build_date\" : \"2016-12-06T12:36:15.409Z\",\r\n    \"build_snapshot\" : false,\r\n    \"lucene_version\" : \"6.3.0\"\r\n  },\r\n  \"tagline\" : \"You Know, for Search\"\r\n}\r\n```\r\n\r\n**Plugins installed**: []\r\nnone\r\n\r\n**JVM version**: 1.8.0\r\n[root@atom1 jenkins]# java -version\r\njava version \"1.8.0\"\r\nJava(TM) SE Runtime Environment (build 1.8.0-b132)\r\nJava HotSpot(TM) 64-Bit Server VM (build 25.0-b70, mixed mode)\r\n\r\n**OS version**: CentOS release 6.8 (Final) Linux 64bit\r\n[root@atom1 jenkins]# cat \/etc\/redhat-release\r\nCentOS release 6.8 (Final)\r\n[root@atom1 jenkins]# uname -a\r\nLinux atom1 2.6.32-642.11.1.el6.x86_64 #1 SMP Fri Nov 18 19:25:05 UTC 2016 x86_64 x86_64 x86_64 GNU\/Linux\r\n\r\n**Description of the problem including expected versus actual behavior**:\r\n\r\n**Steps to reproduce**:\r\n 1. Search and scroll using the following query.\r\n_search\/?scroll=1m\r\n```\r\n{\r\n  \"size\": 0,\r\n  \"query\": {\r\n    \"constant_score\": {\r\n      \"filter\": {\r\n        \"bool\": {\r\n          \"must\": [\r\n            {\r\n              \"range\": {\r\n                \"@timestamp\": {\r\n                  \"gte\" : \"now-1d\/d\",\r\n                  \"lt\" :  \"now\/d\"\r\n                }\r\n              }\r\n            },\r\n            {\r\n              \"exists\": {\r\n                \"field\": \"session\"\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  },\r\n  \"aggs\": {\r\n    \"2\": {\r\n      \"terms\": {\r\n        \"field\": \"session.id.raw\"\r\n      },\r\n      \"aggs\": {\r\n        \"3\": {\r\n          \"terms\": {\r\n            \"field\": \"@timestamp\",\r\n            \"size\": 1,\r\n            \"order\": {\r\n              \"_term\": \"asc\"\r\n            }\r\n          },\r\n          \"aggs\": {\r\n            \"4\": {\r\n              \"top_hits\": {\r\n                \"size\": 1\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n 2. Initial search request is successful. Second search, now with scroll_id as returned in step 1 fails.\r\n_search\/scroll?scroll=1m\r\n```\r\n{\r\n   \"error\":{\r\n      \"root_cause\":[\r\n\r\n      ],\r\n      \"type\":\"reduce_search_phase_exception\",\r\n      \"reason\":\"[reduce] inner finish failed\",\r\n      \"phase\":\"fetch\",\r\n      \"grouped\":true,\r\n      \"failed_shards\":[\r\n\r\n      ],\r\n      \"caused_by\":{\r\n         \"type\":\"null_pointer_exception\",\r\n         \"reason\":null\r\n      }\r\n   },\r\n   \"status\":503\r\n}\r\n```\r\n\r\n**Cluster health**:\r\n[root@atom1 jenkins]# curl -XGET http:\/\/localhost:9200\/_cluster\/health?pretty=true\r\n```\r\n{\r\n  \"cluster_name\" : \"atomcluster\",\r\n  \"status\" : \"green\",\r\n  \"timed_out\" : false,\r\n  \"number_of_nodes\" : 2,\r\n  \"number_of_data_nodes\" : 2,\r\n  \"active_primary_shards\" : 165,\r\n  \"active_shards\" : 330,\r\n  \"relocating_shards\" : 0,\r\n  \"initializing_shards\" : 0,\r\n  \"unassigned_shards\" : 0,\r\n  \"delayed_unassigned_shards\" : 0,\r\n  \"number_of_pending_tasks\" : 0,\r\n  \"number_of_in_flight_fetch\" : 0,\r\n  \"task_max_waiting_in_queue_millis\" : 0,\r\n  \"active_shards_percent_as_number\" : 100.0\r\n}\r\n```\r\n\r\n**Cluster log**:\r\n```\r\n[2017-01-09T13:48:06,677][DEBUG][o.e.i.f.p.SortedSetDVOrdinalsIndexFieldData] global-ordinals [session.id.raw][1972] took [3.6ms]\r\n[2017-01-09T13:48:06,779][DEBUG][o.e.i.f.p.SortedSetDVOrdinalsIndexFieldData] global-ordinals [session.id.raw][2017] took [15.5ms]\r\n[2017-01-09T13:48:06,914][WARN ][r.suppressed             ] path: \/_search\/scroll, params: {scroll=1m}\r\norg.elasticsearch.action.search.ReduceSearchPhaseException: [reduce] inner finish failed\r\n        at org.elasticsearch.action.search.SearchScrollQueryThenFetchAsyncAction.finishHim(SearchScrollQueryThenFetchAsyncAction.java:217) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at org.elasticsearch.action.search.SearchScrollQueryThenFetchAsyncAction.executeFetchPhase(SearchScrollQueryThenFetchAsyncAction.java:175) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at org.elasticsearch.action.search.SearchScrollQueryThenFetchAsyncAction.access$000(SearchScrollQueryThenFetchAsyncAction.java:44) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at org.elasticsearch.action.search.SearchScrollQueryThenFetchAsyncAction$1.onResponse(SearchScrollQueryThenFetchAsyncAction.java:135) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at org.elasticsearch.action.search.SearchScrollQueryThenFetchAsyncAction$1.onResponse(SearchScrollQueryThenFetchAsyncAction.java:129) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at org.elasticsearch.action.ActionListenerResponseHandler.handleResponse(ActionListenerResponseHandler.java:46) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:978) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at org.elasticsearch.transport.TcpTransport$1.doRun(TcpTransport.java:1289) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at org.elasticsearch.common.util.concurrent.EsExecutors$1.execute(EsExecutors.java:109) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at org.elasticsearch.transport.TcpTransport.handleResponse(TcpTransport.java:1281) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at org.elasticsearch.transport.TcpTransport.messageReceived(TcpTransport.java:1250) [elasticsearch-5.1.1.jar:5.1.1]\r\n        at org.elasticsearch.transport.netty4.Netty4MessageChannelHandler.channelRead(Netty4MessageChannelHandler.java:74) [transport-netty4-5.1.1.jar:5.1.1]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:373) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:359) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:351) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n        at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:293) [netty-codec-4.1.6.Final.jar:4.1.6.Final]\r\n        at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:280) [netty-codec-4.1.6.Final.jar:4.1.6.Final]\r\n        at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:396) [netty-codec-4.1.6.Final.jar:4.1.6.Final]\r\n        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:248) [netty-codec-4.1.6.Final.jar:4.1.6.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:373) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:359) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:351) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1334) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:373) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:359) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:926) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:129) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:651) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:536) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:490) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:450) [netty-transport-4.1.6.Final.jar:4.1.6.Final]\r\n        at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:873) [netty-common-4.1.6.Final.jar:4.1.6.Final]\r\n        at java.lang.Thread.run(Thread.java:744) [?:1.8.0]\r\nCaused by: java.lang.NullPointerException\r\n```\r\n\r\nThe _session_ field as well as the _@timestamp_ field exists in all documents during the time period of now - 1d.\r\nIs there anything that can be done to fix this? Please let me know what info you would need to help you in solving this issue.","id":"199548925","title":"ReduceSearchPhaseException Caused by NullPointerException","reopen_by":"jimczi","opened_on":"2017-01-09T13:12:12Z","closed_by":"jimczi"},{"number":"22373","comments":[{"date":"2016-12-29T18:49:40Z","author":"gmoskovicz","text":"I tried to use templates prior to shrink, and got the following exception:\r\n\r\n```\r\n{\r\n  \"error\": {\r\n    \"root_cause\": [\r\n      {\r\n        \"type\": \"illegal_argument_exception\",\r\n        \"reason\": \"mappings are not allowed when shrinking indices, all mappings are copied from the source index\"\r\n      }\r\n    ],\r\n    \"type\": \"illegal_argument_exception\",\r\n    \"reason\": \"mappings are not allowed when shrinking indices, all mappings are copied from the source index\"\r\n  },\r\n  \"status\": 400\r\n}\r\n```\r\n\r\nIf the mappings are always copied from the source index, then it means that there is no workaround for this issue."},{"date":"2016-12-30T10:01:53Z","author":"GlenRSmith","text":"Would using the reindex API be a workaround @gmoskovicz ?"},{"date":"2016-12-30T11:02:31Z","author":"gmoskovicz","text":"Reindexing *is* a workaround. But i meant that there is no workaround as previous steps to actually use the shrink API with some previous steps."},{"date":"2017-01-06T10:33:59Z","author":"s1monw","text":"I think the issue here is that we don't use the `index_version_created` for the target index which we should. I will look into this in a bit."},{"date":"2017-01-06T16:04:11Z","author":"s1monw","text":"I just build an ES version from a snapshot and confirmed that the bug is fixed. thanks for reporting this @gmoskovicz "},{"date":"2017-01-06T16:55:01Z","author":"gmoskovicz","text":"Thanks for fixing it @s1monw "},{"date":"2017-02-21T13:06:32Z","author":"gmoskovicz","text":"@s1monw still happening in v5.2.1:\r\n\r\n```\r\n[2017-02-21T11:06:41,686][WARN ][o.e.c.a.s.ShardStateAction] [e9D0OYU] [doc_shrinked_index][0] received shard failed for shard id [[doc_shrinked_index][0]], allocation id [9DidZyR5R1GbdtJLfrM2Tw], primary term [0], message [failed recovery], failure [RecoveryFailedException[[doc_shrinked_index][0]: Recovery failed on {e9D0OYU}{e9D0OYUURbClpiOZmPE1Hg}{Icm2utt3Q4SFK5YHut-a2Q}{127.0.0.1}{127.0.0.1:9300}]; nested: MapperParsingException[Mapping definition for [@version] has unsupported parameters:  [fielddata : false]]; ]\r\norg.elasticsearch.indices.recovery.RecoveryFailedException: [doc_shrinked_index][0]: Recovery failed on {e9D0OYU}{e9D0OYUURbClpiOZmPE1Hg}{Icm2utt3Q4SFK5YHut-a2Q}{127.0.0.1}{127.0.0.1:9300}\r\n\tat org.elasticsearch.index.shard.IndexShard.lambda$startRecovery$4(IndexShard.java:1537) [elasticsearch-5.2.1.jar:5.2.1]\r\n\tat org.elasticsearch.index.shard.IndexShard$$Lambda$1580\/1463687544.run(Unknown Source) [elasticsearch-5.2.1.jar:5.2.1]\r\n\tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:527) [elasticsearch-5.2.1.jar:5.2.1]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_45]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_45]\r\n\tat java.lang.Thread.run(Thread.java:745) [?:1.8.0_45]\r\nCaused by: org.elasticsearch.index.mapper.MapperParsingException: Mapping definition for [@version] has unsupported parameters:  [fielddata : false]\r\n\tat org.elasticsearch.index.mapper.DocumentMapperParser.checkNoRemainingFields(DocumentMapperParser.java:151) ~[elasticsearch-5.2.1.jar:5.2.1]\r\n\tat org.elasticsearch.index.mapper.DocumentMapperParser.checkNoRemainingFields(DocumentMapperParser.java:145) ~[elasticsearch-5.2.1.jar:5.2.1]\r\n\tat org.elasticsearch.index.mapper.ObjectMapper$TypeParser.parseProperties(ObjectMapper.java:289) ~[elasticsearch-5.2.1.jar:5.2.1]\r\n\tat org.elasticsearch.index.mapper.ObjectMapper$TypeParser.parseObjectOrDocumentTypeProperties(ObjectMapper.java:203) ~[elasticsearch-5.2.1.jar:5.2.1]\r\n\tat org.elasticsearch.index.mapper.RootObjectMapper$TypeParser.parse(RootObjectMapper.java:102) ~[elasticsearch-5.2.1.jar:5.2.1]\r\n\tat org.elasticsearch.index.mapper.DocumentMapperParser.parse(DocumentMapperParser.java:111) ~[elasticsearch-5.2.1.jar:5.2.1]\r\n\tat org.elasticsearch.index.mapper.DocumentMapperParser.parse(DocumentMapperParser.java:91) ~[elasticsearch-5.2.1.jar:5.2.1]\r\n\tat org.elasticsearch.index.mapper.MapperService.parse(MapperService.java:602) ~[elasticsearch-5.2.1.jar:5.2.1]\r\n\tat org.elasticsearch.cluster.metadata.MetaDataMappingService$PutMappingExecutor.applyRequest(MetaDataMappingService.java:264) ~[elasticsearch-5.2.1.jar:5.2.1]\r\n\tat org.elasticsearch.cluster.metadata.MetaDataMappingService$PutMappingExecutor.execute(MetaDataMappingService.java:230) ~[elasticsearch-5.2.1.jar:5.2.1]\r\n\tat org.elasticsearch.cluster.service.ClusterService.executeTasks(ClusterService.java:674) ~[elasticsearch-5.2.1.jar:5.2.1]\r\n\tat org.elasticsearch.cluster.service.ClusterService.calculateTaskOutputs(ClusterService.java:653) ~[elasticsearch-5.2.1.jar:5.2.1]\r\n\tat org.elasticsearch.cluster.service.ClusterService.runTasks(ClusterService.java:612) ~[elasticsearch-5.2.1.jar:5.2.1]\r\n\tat org.elasticsearch.cluster.service.ClusterService$UpdateTask.run(ClusterService.java:1112) ~[elasticsearch-5.2.1.jar:5.2.1]\r\n\tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:527) ~[elasticsearch-5.2.1.jar:5.2.1]\r\n\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:238) ~[elasticsearch-5.2.1.jar:5.2.1]\r\n\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:201) ~[elasticsearch-5.2.1.jar:5.2.1]\r\n\t... 3 more\r\n```\r\n\r\nSame repro steps."},{"date":"2017-02-21T13:17:35Z","author":"gmoskovicz","text":"Looks like we need to backport this fix into 5.2, it was backported to 5.1 and 5.3 already."},{"date":"2017-02-21T13:28:49Z","author":"s1monw","text":"@gmoskovicz thanks for reopening see https:\/\/github.com\/elastic\/elasticsearch\/pull\/22469#issuecomment-281344080"},{"date":"2017-02-21T13:31:26Z","author":"gmoskovicz","text":"Roger that! Thanks for the quick fix @s1monw !"}],"reopen_on":"2017-02-21T13:06:32Z","opened_by":"gmoskovicz","closed_on":"2017-02-21T13:28:49Z","description":"**Elasticsearch version**: 5.0.0\r\n\r\n**Plugins installed**: none\r\n\r\n**JVM version**: any\r\n\r\n**OS version**: any\r\n\r\n**Description of the problem including expected versus actual behavior**:\r\n\r\nWhen you upgrade elasticsearch with an index using a string field with fielddata disabled from 2.x to 5.x, and you try to shrink it, it fails saying that the index has unsupported parameters:  `[fielddata : false]`.\r\n\r\nBasically it looks like when we upgrade an elasticsearch from 2.x to 5.x it changes the following:\r\n\r\n```\r\n{\r\n  \"mappings\": {\r\n    \"type1\": {\r\n      \"properties\": {\r\n        \"@version\": {\r\n          \"type\": \"string\",\r\n          \"index\": \"not_analyzed\",\r\n          \"fielddata\": {\r\n            \"format\": \"disabled\"\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nInto a 5.x mapping withe the following structure:\r\n\r\n```\r\n{\r\n  \"doc\": {\r\n    \"mappings\": {\r\n      \"type1\": {\r\n        \"properties\": {\r\n          \"@version\": {\r\n            \"type\": \"string\",\r\n            \"index\": \"not_analyzed\",\r\n            \"fielddata\": false\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nThen you try to shrink that index, and since string doesn't exist anymore it doesn't know what to do. We should consider either add a note that it's required to shrink an index that it was created in 5.x, or fix this bug by shrinking old indices as well.\r\n\r\n**Steps to reproduce**:\r\n\r\n[1] Create an index in 2.x with the following:\r\n\r\n```\r\nPUT doc\r\n{\r\n  \"mappings\": {\r\n    \"type1\": {\r\n      \"properties\": {\r\n        \"@version\": {\r\n          \"type\": \"string\",\r\n          \"index\": \"not_analyzed\",\r\n          \"fielddata\": {\r\n            \"format\": \"disabled\"\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n[2] Upgrade this elasticsearch instance, or copy the index into the data directory of a 5.x instance so it's picked up as dangling index.\r\n\r\n[3] Verify that the index was upgraded:\r\n\r\n```\r\nGET doc\/type1\/_mapping\r\n\r\n{\r\n  \"doc\": {\r\n    \"mappings\": {\r\n      \"type1\": {\r\n        \"properties\": {\r\n          \"@version\": {\r\n            \"type\": \"string\",\r\n            \"index\": \"not_analyzed\",\r\n            \"fielddata\": false\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n[4] Shrink the index:\r\n\r\n```\r\nPUT doc\/_settings\r\n{\r\n  \"settings\": {\r\n    \"index.blocks.write\": true \r\n  }\r\n}\r\n\r\nPOST doc\/_shrink\/doc_shrinked_index\r\n{\r\n  \"settings\": {\r\n    \"index.number_of_replicas\": 1,\r\n    \"index.number_of_shards\": 1, \r\n    \"index.codec\": \"best_compression\" \r\n  }\r\n}\r\n```\r\n\r\n[5] Verify logs\r\n\r\n**Provide logs (if relevant)**:\r\n\r\n```\r\n[2016-12-29T16:14:00,908][WARN ][o.e.c.a.s.ShardStateAction] [K-E6Kbx] [doc_shrinked_index][0] received shard failed for shard id [[doc_shrinked_index][0]], allocation id [wbxqZDZSRZ6YCYq2iCIfEQ], primary term [0], message [failed recovery], failure [RecoveryFailedException[[doc_shrinked_index][0]: Recovery failed from null into {K-E6Kbx}{K-E6Kbx4TryyqoNBVUZF-w}{C3ZFU6q-QxK01plsEjaRyQ}{127.0.0.1}{127.0.0.1:9300}{box_type=medium}]; nested: MapperParsingException[Mapping definition for [@version] has unsupported parameters:  [fielddata : false]]; ]\r\norg.elasticsearch.indices.recovery.RecoveryFailedException: [doc_shrinked_index][0]: Recovery failed from null into {K-E6Kbx}{K-E6Kbx4TryyqoNBVUZF-w}{C3ZFU6q-QxK01plsEjaRyQ}{127.0.0.1}{127.0.0.1:9300}{box_type=medium}\r\n\tat org.elasticsearch.index.shard.IndexShard.lambda$startRecovery$4(IndexShard.java:1536) [elasticsearch-5.0.0.jar:5.0.0]\r\n\tat org.elasticsearch.index.shard.IndexShard$$Lambda$1653\/403275356.run(Unknown Source) [elasticsearch-5.0.0.jar:5.0.0]\r\n\tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:444) [elasticsearch-5.0.0.jar:5.0.0]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_45]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_45]\r\n\tat java.lang.Thread.run(Thread.java:745) [?:1.8.0_45]\r\nCaused by: org.elasticsearch.index.mapper.MapperParsingException: Mapping definition for [@version] has unsupported parameters:  [fielddata : false]\r\n\tat org.elasticsearch.index.mapper.DocumentMapperParser.checkNoRemainingFields(DocumentMapperParser.java:146) ~[elasticsearch-5.0.0.jar:5.0.0]\r\n\tat org.elasticsearch.index.mapper.DocumentMapperParser.checkNoRemainingFields(DocumentMapperParser.java:141) ~[elasticsearch-5.0.0.jar:5.0.0]\r\n\tat org.elasticsearch.index.mapper.ObjectMapper$TypeParser.parseProperties(ObjectMapper.java:289) ~[elasticsearch-5.0.0.jar:5.0.0]\r\n\tat org.elasticsearch.index.mapper.ObjectMapper$TypeParser.parseObjectOrDocumentTypeProperties(ObjectMapper.java:203) ~[elasticsearch-5.0.0.jar:5.0.0]\r\n\tat org.elasticsearch.index.mapper.RootObjectMapper$TypeParser.parse(RootObjectMapper.java:102) ~[elasticsearch-5.0.0.jar:5.0.0]\r\n\tat org.elasticsearch.index.mapper.DocumentMapperParser.parse(DocumentMapperParser.java:110) ~[elasticsearch-5.0.0.jar:5.0.0]\r\n\tat org.elasticsearch.index.mapper.DocumentMapperParser.parse(DocumentMapperParser.java:91) ~[elasticsearch-5.0.0.jar:5.0.0]\r\n\tat org.elasticsearch.index.mapper.MapperService.parse(MapperService.java:508) ~[elasticsearch-5.0.0.jar:5.0.0]\r\n\tat org.elasticsearch.cluster.metadata.MetaDataMappingService$PutMappingExecutor.applyRequest(MetaDataMappingService.java:276) ~[elasticsearch-5.0.0.jar:5.0.0]\r\n\tat org.elasticsearch.cluster.metadata.MetaDataMappingService$PutMappingExecutor.execute(MetaDataMappingService.java:241) ~[elasticsearch-5.0.0.jar:5.0.0]\r\n\tat org.elasticsearch.cluster.service.ClusterService.runTasksForExecutor(ClusterService.java:555) ~[elasticsearch-5.0.0.jar:5.0.0]\r\n\tat org.elasticsearch.cluster.service.ClusterService$UpdateTask.run(ClusterService.java:894) ~[elasticsearch-5.0.0.jar:5.0.0]\r\n\tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:444) ~[elasticsearch-5.0.0.jar:5.0.0]\r\n\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:237) ~[elasticsearch-5.0.0.jar:5.0.0]\r\n\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:200) ~[elasticsearch-5.0.0.jar:5.0.0]\r\n\t... 3 more\r\n[2016-12-29T16:14:00,911][INFO ][o.e.c.r.a.AllocationService] [K-E6Kbx] Cluster health status changed from [YELLOW] to [RED] (reason: [shards failed [[doc_shrinked_index][0]] ...]).\r\n```\r\n\r\n*Note that it went from YELLOW to RED since the shard was not able to start\r\n","id":"198048352","title":"Shrinking index upgraded from 2.x fails","reopen_by":"gmoskovicz","opened_on":"2016-12-29T18:24:06Z","closed_by":"s1monw"},{"number":"21636","comments":[{"date":"2016-11-17T20:59:22Z","author":"jimczi","text":"Hi @nostrebor,\nPlease ask questions like these on the discussion forum: https:\/\/discuss.elastic.co\/\nWe reserve Github for issues and feature requests.\nRegarding your issue each field is highlighted using its own analyzer. Since you're using a keyword analyzer the only query that can be highlighted would be the entire value of the field. So it would only work with the query \"Kostya Test\".\n"},{"date":"2016-11-17T21:23:45Z","author":"nostrebor","text":"I thought that might be the case so I tried it. As I mentioned in my original post, the same can be repeated where the search is an exact match for the field, and no highlighting is done then either.\n"},{"date":"2016-11-18T08:32:04Z","author":"jimczi","text":"Ok now I understand the problem @nostrebor \nThis simple recreation exhibits the problem:\n\n```\nPUT t\n{\n    \"mappings\": {\n        \"t\": {\n            \"properties\": {\n                \"message\": {\n                    \"type\": \"keyword\",\n                    \"store\": true\n                }\n            }\n        }\n    }\n}\n\nPUT t\/t\/1\n{\n    \"message\": \"foo\"\n}\n\nGET _search\n{\n   \"size\": 1,\n   \"stored_fields\": \"message\",\n   \"query\": {\n      \"match\": {\n         \"message\": \"foo\"\n      }\n   },\n   \"highlight\": {\n      \"fields\": {\n         \"message\": {\n             \"type\": \"plain\",\n             \"no_match_size\": 10\n         }\n      }\n   }\n}\n```\n\n... returns:\n\n```\n{\n   \"took\": 1,\n   \"timed_out\": false,\n   \"_shards\": {\n      \"total\": 5,\n      \"successful\": 5,\n      \"failed\": 0\n   },\n   \"hits\": {\n      \"total\": 1,\n      \"max_score\": 0.2876821,\n      \"hits\": [\n         {\n            \"_index\": \"t\",\n            \"_type\": \"t\",\n            \"_id\": \"1\",\n            \"_score\": 0.2876821,\n            \"fields\": {\n               \"message\": [\n                  \"foo\"\n               ]\n            },\n            \"highlight\": {\n               \"message\": [\n                  \"[66 6f 6f]\"\n               ]\n            }\n         }\n      ]\n   }\n}\n```\n\nThis is due to the fact that the keyword field is stored as a binary field. The highlighter does not convert the binary value into a valid string. \nI'll work on a fix. \nAs a workaround you can use a text field with a keyword analyzer:\n\n```\n\"message\": {\n  \"type\": \"text\",\n  \"analyzer\": \"keyword\",\n  \"store\": true\n}\n```\n\n... or you can force highlighting on source:\nhttps:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/search-request-highlighting.html#_force_highlighting_on_source\n"},{"date":"2016-11-18T09:31:42Z","author":"nostrebor","text":"For posterity, there was a separate issue of searching vs all converting the string to lowercase which would not match the keyword data iirc, which has an existing flag already. Thanks for looking into the other issue though!\n"}],"reopen_on":"2016-11-18T08:32:04Z","opened_by":"nostrebor","closed_on":"2016-11-21T09:29:30Z","description":"<!--\r\nGitHub is reserved for bug reports and feature requests. The best place\r\nto ask a general question is at the Elastic Discourse forums at\r\nhttps:\/\/discuss.elastic.co. If you are in fact posting a bug report or\r\na feature request, please include one and only one of the below blocks\r\nin your new issue. Note that whether you're filing a bug report or a\r\nfeature request, ensure that your submission is for an\r\n[OS that we support](https:\/\/www.elastic.co\/support\/matrix#show_os).\r\nBug reports on an OS that we do not support or feature requests\r\nspecific to an OS that we do not support will be closed.\r\n-->\r\n\r\n<!--\r\nIf you are filing a bug report, please remove the below feature\r\nrequest block and provide responses for all of the below items.\r\n-->\r\n\r\n**Elasticsearch version**: 5.0\r\n\r\n**Plugins installed**: N\/A\r\n\r\n**JVM version**: 1.8u112\r\n\r\n**OS version**: Windows Server 2012\r\n\r\n**Description of the problem including expected versus actual behavior**: Highlighting dynamic fields which are stored as keyword is not working as expected. When searching over _all I would expect highlighting to occur on all fields of the result when using `require_field_match: false`. Example query can be found here:\r\n\r\nhttp:\/\/pastebin.com\/sAGczFhU\r\n\r\nMy use case is that nearly every search is done for an exact value or over _all. I could get extreme performance gains by shifting dynamically created fields to Keyword, and then if full-text search is needed, defining them explicitly in the mapping. However, search highlighting is still an important part of our workflow.\r\n\r\nDo I have a misunderstanding of how highlighting works? My interpretation is that\r\n1. We search over _all\r\n2. Hits are selected for highlighting as a postprocessing step\r\n3. Each field is matched against the original search query if require_field_match: false is set to true and the highlighting_query option is unused.\r\n\r\n**Steps to reproduce**:\r\n 1. Create a new index with a mapping that sets dynamic fields to type Keyword\r\n 2. Add three new documents with `\"message\": \"Kostya Test\"`\r\n 3. Run the previous query (fixing the date range)\r\n\r\nExpected: There should be highlighted search text extracted from the _source fields that are loaded at highlight-time\r\nActual: No text is highlighted\r\n\r\nThe same can be repeated where the search is an exact match for the field, and no highlighting is done then either.\r\n","id":"190156501","title":"Highlighting does not work when all fields are type keyword","reopen_by":"jimczi","opened_on":"2016-11-17T20:35:43Z","closed_by":"jimczi"},{"number":"21550","comments":[{"date":"2016-11-15T11:06:07Z","author":"cbuescher","text":"@jpountz thanks for the review, I hope I answered your question and just pushed another commit addressing your other comment. Since this is not a very prominent problem I'm not sure this should be part of the next patch release, wdyt?\n"},{"date":"2016-11-15T17:55:12Z","author":"cbuescher","text":"Merged on 5.x with 5846c7bb\n"}],"reopen_on":"2016-11-15T17:23:41Z","opened_by":"cbuescher","closed_on":"2016-11-15T17:23:47Z","description":"When using TimeUnitRounding with a DAY_OF_MONTH unit, failing tests in #20833 uncovered an issue when the DST shift happenes just one hour after midnight local time and sets back the clock to midnight, leading to an overlap. Previously this would lead to two different rounding values, depending on whether a date before or after the transition was rounded. This change detects this special case and correct for it by using the previous rounding date for both cases.\r\n\r\nIn this particular case (tz: \"Atlantic\/Azores\", dates around the DST transition on e.g. 2000-10-29T01:00:00.000Z, unit: DAY_OF_MONTH) we currently get the overlapping part during the DST transition as a separate rounding interval as illustrated in this table:\r\n\r\n| date                                      | round(date)                          | nextRoundingValue(date)      |\r\n| --------------------------------|---------------------------------|---------------------------------|\r\n| 2000-10-28T22:00:00.000Z | 2000-10-28T00:00:00.000Z | 2000-10-29T00:00:00.000Z |\r\n| 2000-10-28T23:00:00.000Z | 2000-10-28T00:00:00.000Z | 2000-10-29T00:00:00.000Z |\r\n| 2000-10-29T00:00:00.000Z | **2000-10-29T00:00:00.000Z** | 2000-10-30T00:00:00.000-01:00 |\r\n| 2000-10-29T00:00:00.000-01:00 | **2000-10-29T00:00:00.000-01:00** | 2000-10-30T00:00:00.000-01:00 |\r\n| 2000-10-29T01:00:00.000-01:00 | 2000-10-29T00:00:00.000-01:00 | 2000-10-30T00:00:00.000-01:00 |\r\n| 2000-10-29T02:00:00.000-01:00 | 2000-10-29T00:00:00.000-01:00 | 2000-10-30T00:00:00.000-01:00 |\r\n\r\nAccording to https:\/\/www.timeanddate.com\/time\/change\/portugal\/horta?year=2000 the DST change happenes on Oct-29th at 1am local time, turning back the clock one hour (to offset -01:00). Currently the dates between \"2000-10-29T00:00:00.000Z\" and \"2000-10-29T01:00:00.000Z\" all round down to \"2000-10-29T00:00:00.000Z\" (before the transition) and the dates after but before next midnight round down to \"2000-10-29T00:00:00.000-01:00\". For a day rounding we would prefer a 25h hour bucket for \"2000-10-29\". \r\n\r\nWith this fix, the situation above changes to \r\n\r\n| date                                      | round(date)                          | nextRoundingValue(date)      |\r\n| --------------------------------|---------------------------------|---------------------------------|\r\n| 2000-10-28T22:00:00.000Z | 2000-10-28T00:00:00.000Z | 2000-10-29T00:00:00.000Z |\r\n| 2000-10-28T23:00:00.000Z | 2000-10-28T00:00:00.000Z | 2000-10-29T00:00:00.000Z |\r\n| 2000-10-29T00:00:00.000Z | 2000-10-29T00:00:00.000Z | 2000-10-30T00:00:00.000-01:00 |\r\n| 2000-10-29T00:00:00.000-01:00 | 2000-10-29T00:00:00.000Z | 2000-10-30T00:00:00.000-01:00 |\r\n| 2000-10-29T01:00:00.000-01:00 | 2000-10-29T00:00:00.000Z | 2000-10-30T00:00:00.000-01:00 |\r\n| 2000-10-29T02:00:00.000-01:00 | 2000-10-29T00:00:00.000Z | 2000-10-30T00:00:00.000-01:00 |\r\n\r\nSo now everything on \"2000-10-29\" gets rounded down to \"2000-10-29T00:00:00.000Z\".\r\n\r\nCloses #20833","id":"189178197","title":"Fix time zone rounding edge case for DST overlaps","reopen_by":"cbuescher","opened_on":"2016-11-14T17:45:26Z","closed_by":"cbuescher"},{"number":"20877","comments":[{"date":"2016-10-12T16:37:13Z","author":"jpountz","text":"Can you also share the explanation of a document that gets ranked higher even though it should be worse?\n"},{"date":"2016-10-13T10:57:10Z","author":"drount","text":"Here is the top match of the search:\n\nhttp:\/\/pastebin.com\/x3caXrVg\n"},{"date":"2016-10-14T15:32:15Z","author":"jpountz","text":"`explain` returns a different score, which I think is the right one and would have ranked this document better. This is probably due to https:\/\/issues.apache.org\/jira\/browse\/LUCENE-7132, a pretty bad bug which is only fixed in recent Lucene versions (Lucene 6.1+). Maybe we should backport it and do a new 5.5 release.\n"},{"date":"2016-10-14T15:37:49Z","author":"jpountz","text":"For the record, I _think_ this can be worked around by adding a `FILTER` clause with a query that matches all docs without being a `match_all` query, eg.\n\n```\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        { \"match\" : { \"title\" : \"todo el mundo lo sabe\" } }\n      ],\n      \"filter\": [\n        { \"exists\" : { \"field\" : \"title\" } }\n      ]\n    }\n  }\n}\n```\n"},{"date":"2016-10-14T15:54:18Z","author":"drount","text":"I reindexed the data in a local Elasticsearch 2.4 and can not reproduce the bug. Is the Lucene version updated in 2.4?\n\nOther than that... Looks like that wrong-scored document does not use coord (maybe because it is a perfect match). \n\nI'll try your \"patch\".\n\nIt is unfortunate that due to this bug I cannot use AWS Elastichsearch. \n"},{"date":"2016-10-14T15:57:41Z","author":"jpountz","text":"2.4 should have the bug too, but in order to reproduce the bug you need documents to be indexed in a certain order, so it does not necessarily reproduce if you reindex.\n"},{"date":"2016-10-14T17:15:06Z","author":"drount","text":"I see.\n\nMaybe I'm wrong but... is this a huge problem in general for ES?\n"},{"date":"2016-10-14T19:37:56Z","author":"jpountz","text":"It may be indeed. It had been thought as quite an exotic bug until now but the fact that you hit it is making us reconsider.\n"},{"date":"2016-10-14T19:52:09Z","author":"drount","text":"Any chance ES will update the Lucene dependency? \n\nDue to this bug, I may have to reconsider ES for my use case. \n"},{"date":"2016-10-14T20:48:20Z","author":"jpountz","text":"@drount The 5.0 rc1 release has the fix (it is on Lucene 6.2), but we still need a plan in order to get this bug fixed on the 2.x branch, which cannot move from Lucene 5.5.\n"},{"date":"2016-10-17T08:36:18Z","author":"drount","text":"I will try today the \n\n`\n\"filter\": [\n        { \"exists\" : { \"field\" : \"title\" } }\n      ]\n`\n\nworkaround.\n\nWould you mind elaborating why this may avoid the error?\n"},{"date":"2016-10-17T13:26:35Z","author":"drount","text":"I tested the suggested workaround. It now does return the correct _score. However, the query times is 10x more so it is basically unacceptable.\n\nAny other idea?\n\nThank you.\n"},{"date":"2016-10-17T13:43:43Z","author":"jpountz","text":"> Would you mind elaborating why this may avoid the error?\n\nThe bug is located in an optimization to pure disjunctions (a top-level `A OR B OR C OR ...`). By adding a filter, the optimization gets disabled since it is not a top-level pure disjunction anymore.\n\n> Any other idea?\n\nWe need to get the bugfix backported and released in Lucene so that we can do a new Elasticsearch 2.x release on a fixed version of Lucene.\n"},{"date":"2016-10-17T14:00:38Z","author":"jpountz","text":"Nevermind what I said above. I was reading change logs more carefully and this bug should be fixed in Elasticsearch 2.4.0+ since it is based on Lucene 5.5.2 which has the fix. Would you mind giving it a try?\n"},{"date":"2016-10-17T14:03:39Z","author":"drount","text":"I indexed 3 times in 2.4 (locally) and I seems that works.\n\nShouldn't 2.3 be urgently deprecated?\n"},{"date":"2016-10-17T16:24:03Z","author":"clintongormley","text":"@drount We only maintain the 2.4 branch in the 2.x series now, and the fix is already released. This bug was highly situation dependent.  it doesn't require an urgent (or non-urgent) deprecation\n"}],"reopen_on":"2016-10-17T10:29:28Z","opened_by":"drount","closed_on":"2016-10-17T16:24:03Z","description":"**Elasticsearch version**: 2.3\n\n**Plugins installed**: []\n\n**JVM version**:\nDeployed on AWS\n**OS version**:\nDeployed on AWS\n**Description of the problem including expected versus actual behavior**:\n\nI have a database deployed in Amazon Elasticsearch with about 20M documents. The document schema defines simple analysers and fields are all are strings. There is 1 replica and 1 shard. _all is disabled.\n\nA query to a field with exactly the same value as the field in document I am looking for results in other documents being scored higher. Doing an explain, results in a weird results:\n\nQuery:\n\n{'query': {'bool': {'must': [{'match': {'title': 'todo el mundo lo sabe'}}]}}}\n\nResult:\nhttp:\/\/pastebin.com\/vdxzc6nM\n\nAs you can see, the result of the last value of the explain and the final _score is different.\n\nFor other documents, this is not the case. The result of explain is the same as the result of _score.\n\nIs this a bug?\n\nThank you. \n","id":"182460578","title":"Scoring bug in 2.3","reopen_by":"drount","opened_on":"2016-10-12T08:06:03Z","closed_by":"clintongormley"},{"number":"20482","comments":[{"date":"2016-09-14T15:09:56Z","author":"nik9000","text":"I know @jimferenczi's been working on source filtering lately but the completion suggester is @areek's baby. It looks like this is simply a case of the parsing not being implemented which isn't super difficult to fix so I expect anyone can take it.\n"},{"date":"2016-09-14T15:44:42Z","author":"nik9000","text":"I can also take this if everyone else is busy!\n"},{"date":"2016-09-14T16:38:34Z","author":"jimczi","text":"I don't think it is worth implementing this since the _suggest endpoint will be deprecated in 5.0 (https:\/\/github.com\/elastic\/elasticsearch\/pull\/20435) and that it works with the _search API ?\n"},{"date":"2016-09-14T16:44:58Z","author":"nik9000","text":"Perfect! So long as we use the _search API in the docs we're all good then.\n"},{"date":"2016-11-02T08:26:24Z","author":"russcam","text":"@nik9000 Completion Suggester documentation still says `_suggest` supports `_source` filtering.\n\nSince `_suggest` endpoint still exists in 5.0, I think source filtering should be included as per the current documentation, to be consistent with suggestions when using the `_search` endpoint.\n"},{"date":"2016-11-02T13:36:57Z","author":"nik9000","text":"Yeah, if we're going to remove it in 6.0 we don't need docks there but we should fix the docs in the 5.0 and 5.x branch. @russcam, are you willing to open a PR to do it?\n"},{"date":"2016-11-02T14:07:43Z","author":"nik9000","text":"> Yeah, if we're going to remove it in 6.0 we don't need docks there but we should fix the docs in the 5.0 and 5.x branch. @russcam, are you willing to open a PR to do it?\n\nIgnore that. I've found a moment so I'll just do it now.\n"},{"date":"2016-11-02T14:36:49Z","author":"nik9000","text":"@russcam I opened #21268.\n"}],"reopen_on":"2016-11-02T13:36:01Z","opened_by":"nik9000","closed_on":"2016-11-07T01:15:45Z","description":"I found this while going through snippets that aren't marked `\/\/ CONSOLE`. In this case the [this](https:\/\/github.com\/elastic\/elasticsearch\/blame\/master\/docs\/reference\/search\/suggesters\/completion-suggest.asciidoc#L201-L222) snippet looks to suggest something that isn't implemented.\n","id":"176933246","title":"Completion suggester docs claim `_source` filters the source but this isn't implemented","reopen_by":"nik9000","opened_on":"2016-09-14T15:07:57Z","closed_by":"nik9000"},{"number":"20429","comments":[{"date":"2016-09-12T20:05:18Z","author":"bleskes","text":"I'm not an expert here, but the approach looks like the best we had so far. Left some minor comments.\n"},{"date":"2016-09-13T18:44:53Z","author":"bleskes","text":"LGTM. Thanks @jasontedor . Another fun adventure.\n"},{"date":"2016-09-13T18:46:08Z","author":"jasontedor","text":"Thank you @bleskes and @mikemccand for careful reviews.\n"}],"reopen_on":"2016-09-12T20:05:22Z","opened_by":"jasontedor","closed_on":"2016-09-13T18:46:34Z","description":"Today we add a prefix when logging within Elasticsearch. This prefix\ncontains the node name, and index and shard-level components if\nappropriate.\n\nDue to some implementation details with Log4j 2 , this does not work for\nintegration tests; instead what we see is the node name for the last\nnode to startup. The implementation detail here is that Log4j 2 there is\nonly one logger for a name, message factory pair, and the key derived\nfrom the message factory is the class name of the message factory. So,\nwhen the last node starts up and starts setting prefixes on its message\nfactories, it will impact the loggers for the other nodes.\n\nAdditionally, the prefixes are lost when logging an exception. This is\ndue to another implementation detail in Log4j 2. Namely, since we log\nexceptions using a parameterized message, Log4j 2 decides that that\nmeans that we do not want to use the message factory that we have\nprovided (the prefix message factory) and so logs the exception without\nthe prefix.\n\nThis commit fixes both of these issues.\n","id":"176453842","title":"Fix prefix logging","reopen_by":"bleskes","opened_on":"2016-09-12T18:50:08Z","closed_by":"jasontedor"},{"number":"20161","comments":[{"date":"2016-08-25T14:18:06Z","author":"clintongormley","text":"Hi @apidruchny \n\nI can reproduce this failure - something funny in the parsing of the `not` query.  That said, `and`, `or`, and `not` are deprecated and have been removed in 5.0.  You should rewrite this query to use `bool` instead:\n\n```\nPOST _search\n{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"post_filter\": {\n    \"bool\": {\n      \"must_not\": [\n        {\n          \"term\": {\n            \"field1\": \"1234\"\n          }\n        }\n      ]\n    }\n  }\n}\n```\n\nBy the way, use `post_filter` ONLY if you want to include documents in your aggregation that you then want to exclude from the returned `hits`.  Using it as a generic filtering mechanism is very inefficient\n"},{"date":"2016-08-25T14:34:40Z","author":"apidruchny","text":"Yes, I know that boolean filters are deprecated, and post_filter is inefficient. This is just a distilled example. In reality our query is a much more complicated filtered query (no post_filter) and we are trying to migrate from version 1.7.5 to 2.3.5. Could this issue please be re-opened? Even being deprecated, the \"not\" filter should still work, to allow legacy applications to migrate to the current version of Elasticsearch. And this is exactly what we are trying to do.\n"},{"date":"2016-08-25T14:36:57Z","author":"clintongormley","text":"Sure I can reopen it, but we're unlikely to fix deprecated functionality unless somebody sends a PR.  Why migrate to `not` when you're just going to have to repeat the migration for the next version anyway?\n"},{"date":"2016-08-25T15:16:00Z","author":"apidruchny","text":"Thanks. We are not migrating to `not`, we are migrating to Elasticsearch 2.3.5 and the `not` filter is already used in a legacy application. Sure, it will be necessary to replace the `not` filter eventually, but we would prefer to not have to do it now when we are still on version 1.7.5.\n"},{"date":"2016-08-30T18:31:53Z","author":"apidruchny","text":"There is a very easy workaround (adding \"filter\" element under \"not\"). I am going to close this issue. Thanks for looking at this issue. Following works:\n\n```\nPOST _search\n{\n    \"query\" : {\n        \"match_all\" : {}\n    },\n    \"post_filter\" : {\n        \"not\" : {\n            \"filter\" : {\n                \"and\" : [\n                    {\n                        \"query\" : {\n                            \"term\" : {\n                                \"field1\" : \"1234\"\n                            }\n                        }\n                    }\n                ]\n            }\n        }\n    }\n}\n```\n"}],"reopen_on":"2016-08-25T14:36:10Z","opened_by":"apidruchny","closed_on":"2016-08-30T18:31:54Z","description":"Elasticsearch version: 2.3.5\nGetting unexpected errors from some queries with filters. These queries worked with Elasticsearch version 1.7.5, but fail on version 2.3.5. Below are steps to reproduce.\n1. Create an index\n\n```\nPUT testindex\n{\n    \"mappings\" : {\n        \"type1\" : {\n            \"properties\" : {\n                \"field1\" : { \"type\" : \"string\", \"index\" : \"not_analyzed\" }\n            }\n        }\n    }\n}\n```\n1. Issue the following query:\n\n```\nPOST\n{\n    \"query\" : {\n        \"match_all\" : {}\n    },\n    \"post_filter\" : {\n        \"not\" : {\n            \"and\" : [\n                {\n                    \"query\" : {\n                        \"term\" : {\n                            \"field1\" : \"1234\"\n                        }\n                    }\n                }\n            ]\n        }\n    }\n}\n```\n\nAnd the result is:\n\n```\n{\n  \"error\" : {\n    \"root_cause\" : [ {\n      \"type\" : \"query_parsing_exception\",\n      \"reason\" : \"[and] query does not support [field1]\",\n      \"index\" : \"testindex\",\n      \"line\" : 11,\n      \"col\" : 29\n    } ],\n    \"type\" : \"search_phase_execution_exception\",\n    \"reason\" : \"all shards failed\",\n    \"phase\" : \"query_fetch\",\n    \"grouped\" : true,\n    \"failed_shards\" : [ {\n      \"shard\" : 0,\n      \"index\" : \"testindex\",\n      \"node\" : \"0JJWSN2WRwaX1dvaW3yPfA\",\n      \"reason\" : {\n        \"type\" : \"query_parsing_exception\",\n        \"reason\" : \"[and] query does not support [field1]\",\n        \"index\" : \"testindex\",\n        \"line\" : 11,\n        \"col\" : 29\n      }\n    } ]\n  },\n  \"status\" : 400\n}\n```\n\nExpected result, as it also works in Elasticsearch version 1.7.5:\n\n```\n{\n  \"took\" : 1,\n  \"timed_out\" : false,\n  \"_shards\" : {\n    \"total\" : 5,\n    \"successful\" : 5,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : 0,\n    \"max_score\" : null,\n    \"hits\" : [ ]\n  }\n}\n```\n","id":"173201752","title":"Some boolean filters result in errors like \"[and] query does not support [field1]\"","reopen_by":"clintongormley","opened_on":"2016-08-25T13:22:55Z","closed_by":"apidruchny"},{"number":"19857","comments":[{"date":"2016-08-09T13:00:39Z","author":"jimczi","text":"I tested your recreation with ES 2.3.3 and it worked fine. The index is created as expected and it fails if `action.auto_create_index` is set to false.  I am closing the issue since I am not able to reproduce, feel free to reopen if you find a recreation that does not work.\n"},{"date":"2016-08-17T20:18:24Z","author":"mouadino","text":"@jimferenczi Sadly I cannot reopen the issue but I am updating here with reproducible steps.\n\n### Configuration\n\nInstall both elasticsearch versions 2.1.0 and 2.3.0, for each do the following:\n\nChange configuration so that `action.auto_create_index: true` and `index.mapper.dynamic: false`. \n\nTrying to create now a document fail as expected, \n\n```\nPOST \/twitter:0\/tweet\/1\/_update\n{\n  \"doc\": {\n    \"message\": \"test\"\n  },\n  \"doc_as_upsert\": true\n}\n\n{\n  \"error\": {\n    \"root_cause\": [\n      {\n        \"type\": \"type_missing_exception\",\n        \"reason\": \"type[[tweet, trying to auto create mapping, but dynamic mapping is disabled]] missing\",\n        \"index\": \"twitter:0\"\n      }\n    ],\n    \"type\": \"type_missing_exception\",\n    \"reason\": \"type[[tweet, trying to auto create mapping, but dynamic mapping is disabled]] missing\",\n    \"index\": \"twitter:0\"\n  },\n  \"status\": 404\n}    \n```\n\nCreate a template: \n\n```\nPUT \/_template\/twitter\n{\n  \"template\": \"*\",\n  \"order\": 0,\n  \"mappings\": {\n    \"tweet\": {\n      \"_source\": {\n        \"enabled\": true\n      },\n      \"properties\": {\n        \"message\": {\n          \"type\": \"string\"\n        }\n      }\n    }\n  }\n}\n```\n\n### Issue:\n\nNow this is what happen when doing an upsert with both elasticsearch versions:\n\n```\nPOST \/twitter:0\/tweet\/1\/_update\n{\n  \"doc\": {\n    \"message\": \"test\"\n  },\n  \"doc_as_upsert\": true\n}\n```\n\n**Elasticsearch 2.1.0:**\n\n```\n{\n  \"_index\": \"twitter:0\",\n  \"_type\": \"tweet\",\n  \"_id\": \"1\",\n  \"_version\": 1,\n  \"_shards\": {\n    \"total\": 2,\n    \"successful\": 1,\n    \"failed\": 0\n  }\n}\n```\n\n**Elasticsearch 2.3.0:**\n\n```\n{\n  \"error\": {\n    \"root_cause\": [\n      {\n        \"type\": \"index_not_found_exception\",\n        \"reason\": \"no such index\",\n        \"resource.type\": \"index_expression\",\n        \"resource.id\": \"twitter:0\",\n        \"index\": \"twitter:0\"\n      }\n    ],\n    \"type\": \"index_not_found_exception\",\n    \"reason\": \"no such index\",\n    \"resource.type\": \"index_expression\",\n    \"resource.id\": \"twitter:0\",\n    \"index\": \"twitter:0\"\n  },\n  \"status\": 404\n}\n```\n\nThanks again for looking at this bug\n"},{"date":"2016-08-18T09:43:49Z","author":"clintongormley","text":"thanks for the recreation @mouadino \n\nI hoped this would be fixed by https:\/\/github.com\/elastic\/elasticsearch\/pull\/19478 but apparently not.  This still fails on 2.4.0.  \n\nI think that theoretically this should work, but it might be complicated.  The logic should be:\n- does the index exist?\n- if not, can i create the index? (check auto create)\n- is there a matching template?\n- create the index with the mappings\n- does the mapping exist?\n- if not, can i create the mapping? (check index.mapper.dynamic)\n- create the mapping\n\nThe downside here is that (with auto create true, and mapper dynamic false) you can end up creating an index even though you can't create the type.  But i think that makes sense.\n\n@jimferenczi could you take a look please?\n"},{"date":"2016-08-18T10:11:27Z","author":"jimczi","text":"> I hoped this would be fixed by #19478 but apparently not. This still fails on 2.4.0.\n\nWhen set at the node level we don't check if a template would have matched.  https:\/\/github.com\/elastic\/elasticsearch\/pull\/19478 fixed the case where it is set at the index level for instance within the template:\n\n```\n{\n  \"template\": \"*\",\n  \"order\":0,\n  \"settings\": {\n    \"index.mapper.dynamic\": false\n  },\n  \"mappings\": {\n    \"tweet\": {\n      \"_source\":{\n        \"enabled\":true\n      },\n      \"properties\": {\n        \"message\": {\n          \"type\": \"string\"\n        }\n      }\n    }\n  }\n}\n```\n\n... this works with the downside that @clintongormley mentioned:\n\n> The downside here is that (with auto create true, and mapper dynamic false) you can end up creating an index even though you can't create the type.\n\nSince it is not possible to set this setting at the node level in 5.x I don't think we should try to fix it in 2.x but rather document the alternative ?\n"},{"date":"2016-08-18T10:53:36Z","author":"clintongormley","text":"> Since it is not possible to set this setting at the node level in 5.x I don't think we should try to fix it in 2.x but rather document the alternative ?\n\nAhhh thanks, I'd forgotten about that. Same reason i closed a similar issue previously.\n\n+1 to docs\n"}],"reopen_on":"2016-08-18T09:39:52Z","opened_by":"mouadino","closed_on":"2016-08-18T10:53:36Z","description":"<!--\nGitHub is reserved for bug reports and feature requests. The best place\nto ask a general question is at the Elastic Discourse forums at\nhttps:\/\/discuss.elastic.co. If you are in fact posting a bug report or\na feature request, please include one and only one of the below blocks\nin your new issue. Note that whether you're filing a bug report or a\nfeature request, ensure that your submission is for an\n[OS that we support](https:\/\/www.elastic.co\/support\/matrix#show_os).\nBug reports on an OS that we do not support or feature requests\nspecific to an OS that we do not support will be closed.\n-->\n\n<!--\nIf you are filing a bug report, please remove the below feature\nrequest block and provide responses for all of the below items.\n-->\n\n**Elasticsearch version**: 2.3.3 and 2.1.1\n\n**Plugins installed**: []\n\n**JVM version**: 1.8.0_92\n\n**OS version**: OS X El Capitan 10.11.5\n\n**Description of the problem including expected versus actual behavior**:\n\nWhen migrating an Elasticsearch cluster from 2.1.1 to 2.3.3, a previously working code started to failing and I looked to https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/breaking-changes-2.3.html trying to make sense of the failure sadly didn't see any mention of the change so I am unsure whether this is a regression bug or documentation issue.\n\nThe problem that I am referring to is when using the update API, which we use to do upsert of documents which works okay in  2.1.1 when index is not created already (given that `action.auto_create_index: true`) but fail with 2.3.3 with same configuration complaining that index was not found.\n\n**Steps to reproduce**:\n1. Using 2.3.3 run `curl -XPOST http:\/\/localhost:9200\/index\/doc_type\/1\/_update -d '{\"doc\": {\"name\" : \"test\"}, \"doc_as_upsert\": true}'`\n2. Using 2.1.1 try same query as before.\n\nWith 2.3.3 you get:\n\n```\n{\"error\":{\"root_cause\":[{\"type\":\"index_not_found_exception\",\"reason\":\"no such index\",\"resource.type\":\"index_expression\",\"resource.id\":\"...\",\"index\":\"...\"}],\"type\":\"index_not_found_exception\",\"reason\":\"no such index\",\"resource.type\":\"index_expression\",\"resource.id\":\"...\",\"index\":\"...\"},\"status\":404}\n```\n\nWith 2.1.1 it works as expected.\n\n**Provide logs (if relevant)**:\n\n```\n[dummy-index] IndexNotFoundException[no such index]\n    at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:151)\n    at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:95)\n    at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteSingleIndex(IndexNameExpressionResolver.java:208)\n    at org.elasticsearch.action.support.single.instance.TransportInstanceSingleOperationAction$AsyncSingleAction.doStart(TransportInstanceSingleOperationAction.java:138)\n    at org.elasticsearch.action.support.single.instance.TransportInstanceSingleOperationAction$AsyncSingleAction.start(TransportInstanceSingleOperationAction.java:123)\n    at org.elasticsearch.action.support.single.instance.TransportInstanceSingleOperationAction.doExecute(TransportInstanceSingleOperationAction.java:73)\n    at org.elasticsearch.action.update.TransportUpdateAction.innerExecute(TransportUpdateAction.java:147)\n    at org.elasticsearch.action.update.TransportUpdateAction.doExecute(TransportUpdateAction.java:142)\n    at org.elasticsearch.action.update.TransportUpdateAction.doExecute(TransportUpdateAction.java:66)\n    at org.elasticsearch.action.support.TransportAction.doExecute(TransportAction.java:149)\n    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:137)\n    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:85)\n    at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:58)\n    at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:359)\n    at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:52)\n    at org.elasticsearch.rest.BaseRestHandler$HeadersAndContextCopyClient.doExecute(BaseRestHandler.java:83)\n    at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:359)\n    at org.elasticsearch.client.support.AbstractClient.update(AbstractClient.java:396)\n    at org.elasticsearch.rest.action.update.RestUpdateAction.handleRequest(RestUpdateAction.java:126)\n    at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:54)\n    at org.elasticsearch.rest.RestController.executeHandler(RestController.java:205)\n    at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:166)\n    at org.elasticsearch.http.HttpServer.internalDispatchRequest(HttpServer.java:128)\n    at org.elasticsearch.http.HttpServer$Dispatcher.dispatchRequest(HttpServer.java:86)\n    at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:449)\n    at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpRequestHandler.java:61)\n    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.messageReceived(HttpPipeliningHandler.java:60)\n    at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)\n    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)\n    at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:194)\n    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n    at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:135)\n    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)\n    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)\n    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)\n    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)\n    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)\n    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)\n    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)\n    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)\n    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\n    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)\n    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)\n    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\n    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\n    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n```\n\n<!--\nIf you are filing a feature request, please remove the above bug\nreport block and provide responses for all of the below items.\n-->\n","id":"169897127","title":"Update API doesn't work anymore when index is not created yet","reopen_by":"clintongormley","opened_on":"2016-08-08T11:03:47Z","closed_by":"clintongormley"},{"number":"19616","comments":[{"date":"2016-07-28T08:55:17Z","author":"HonzaKral","text":"wow, didn't know github works this way, sorry about the noise.\n"},{"date":"2016-07-28T09:04:51Z","author":"martijnvg","text":"@HonzaKral np :) - This is totally unexpected.\n"},{"date":"2016-07-29T07:48:14Z","author":"jpountz","text":"LGTM\n"}],"reopen_on":"2016-07-28T08:50:00Z","opened_by":"martijnvg","closed_on":"2016-07-29T11:01:19Z","description":"The plain highlighter fails when it tries to select the fragments based on a query containing either a `has_child` or `has_parent` query.\n\nThe plain highlighter should just ignore parent\/child queries as it makes no sense to highlight a parent match with a `has_child` as the child documents are not available at highlight time. Instead if child document should be highlighted inner hits should be used.\n\nParent\/child queries already have no effect when the `fvh` or `postings` highligher is used. The test added in this PR verifies that.\n\nPR for #14999\n","id":"167809656","title":"Plain highlighter should ignore parent\/child queries","reopen_by":"martijnvg","opened_on":"2016-07-27T09:25:39Z","closed_by":"martijnvg"},{"number":"19353","comments":[{"date":"2016-07-11T07:48:24Z","author":"jimczi","text":"The `has_child` query operates at the index level, you cannot restrict the type of your query like you're doing in your snippet: \n`GET \/sandbox\/questions\/_search` \nTry with\n`\/sandbox\/_search`\n"},{"date":"2016-07-11T08:02:41Z","author":"martijnvg","text":"> The has_child query operates at the index level, you cannot restrict the type of your query like you're doing in your snippet: \n\nThis is possible, since 2.0 if I recall correctly. \n\nThis really seems to be caused by a bug in: [HasChildQueryBuilder#rewrite(...)](https:\/\/github.com\/elastic\/elasticsearch\/blob\/8c40b2b54eac3e3ab3c41ece5c758be75173191b\/core\/src\/main\/java\/org\/elasticsearch\/index\/query\/HasChildQueryBuilder.java#L477)\n\nFixing that makes the query work as expected. I'll open a PR.\n"}],"reopen_on":"2016-07-11T08:02:42Z","opened_by":"ayadav77","closed_on":"2016-07-12T14:27:58Z","description":"**Elasticsearch version**: 5.0.0-alpha4\n\n**JVM version**: 1.8.0_60\n\n**Description of the problem including expected versus actual behavior**:\nThe has_child query does not return expected results when it contains a date based range query. I created a document type called **questions** which has a child document type called **answers**. The answers document has a **date** field. Once I insert sample data in elasticsearch, I can query the **answers** document successfully with a date range query. However, the has_child query does not return any results when the date range query utilizes any date value that precedes the answer dates. The following steps explain the issue further.\n\n**Steps to reproduce**:\n1) Create an index called \"sandbox\"\n\n```\nPUT \/sandbox\n```\n\n2) Create a document mapping for \"answers\" with a parent type of \"questions\".\n\n```\nPUT \/sandbox\/_mapping\/answers\n{\n  \"_parent\": {\n      \"type\": \"questions\" \n  }\n  , \"properties\": {\n    \"answer\" : {\"type\": \"text\"},\n    \"date\" : {\"type\": \"date\"}\n  }\n}\n```\n\n3) Add a \"questions\" document\n\n```\nPUT \/sandbox\/questions\/1\n{\n  \"question\" : \"Why is the sky blue?\"\n}\n```\n\n4) Add a couple of \"answers\" document\n\n```\nPUT \/sandbox\/answers\/1?parent=1\n{\n  \"answer\" : \"Due to scattering of sunlight\",\n  \"date\" : \"2016-05-01\"\n}\n```\n\n```\nPUT \/sandbox\/answers\/2?parent=1\n{\n  \"answer\" : \"Due to refraction of light\",\n  \"date\" : \"2016-06-01\"\n}\n```\n\n5) Find all answers with a date value of gte \"2016-04-01\". \nTHIS WORKS AS EXPECTED. It returns the 2 answers documents as expected.\n\n```\nGET \/sandbox\/answers\/_search\n{\n  \"query\" : {\n    \"range\": {\n      \"date\": {\n        \"gte\": \"2016-04-01\"\n      }\n    }\n  }\n}\n```\n\n6) Now use the has_child query to get questions with answers that have date gte \"2016-04-01\". \n**DOES NOT WORK AS EXPECTED**. This returns 0 hits. I expected the 1 \"questions\" document in the response.\n\n```\nGET \/sandbox\/questions\/_search\n{\n  \"query\" : {\n    \"has_child\": {\n      \"type\": \"answers\",\n      \"query\": {\n        \"range\": {\n          \"date\": {\n            \"gte\": \"2016-04-01\"\n          }\n        }\n      }\n    }\n  }\n}\n```\n","id":"164730742","title":"has_child query returns no results when used with date range query","reopen_by":"martijnvg","opened_on":"2016-07-10T19:46:23Z","closed_by":"martijnvg"},{"number":"19026","comments":[{"date":"2016-06-22T17:01:22Z","author":"jasontedor","text":"> If we decide that upgrading from alpha3 to newer alpha version does not need to work, than the last part of this can potentially be ignored, however we still need to ensure that upgrading from any older (non alpha) elasticsearch version to a newer one works.\n\nI don't think we need to support upgrading between pre-releses. The packaging tests [do test upgrading from the previous major version](https:\/\/github.com\/elastic\/elasticsearch\/blob\/6671c0cf09ffe9dbc31936b25524df88d29612ed\/qa\/vagrant\/src\/test\/resources\/packaging\/scripts\/80_upgrade.bats). Right now they test upgrading from 2.0.0; I will open a PR to move this to 2.3.3.\n"},{"date":"2016-06-22T17:19:26Z","author":"jasontedor","text":"I opened #19029.\n"},{"date":"2016-06-23T06:59:00Z","author":"spinscale","text":"IMO we should still ensure that uninstalling an RPM cleans up `\/usr\/share\/elasticsearch\/modules`\n\nOtherwise this just postpones the problem until an official release uses different modules.\n"},{"date":"2016-06-23T11:56:10Z","author":"clintongormley","text":"> IMO we should still ensure that uninstalling an RPM cleans up \/usr\/share\/elasticsearch\/modules\n> Otherwise this just postpones the problem until an official release uses different modules.\n\nAgreed!\n"}],"reopen_on":"2016-06-23T11:55:54Z","opened_by":"spinscale","closed_on":null,"description":"When trying to upgrade from alpha3 to a more recent (snapshot) version\n\n**Elasticsearch version**: 5.0.0-alpha3\n\n**OS version**: Centos 7\n\n**Steps to reproduce**:\n\n```\nrpm -i elasticsearch-5.0.0-alpha3.rpm\nsystemctl daemon-reload\nsystemctl start elasticsearch\n# use journalctl -f to check that alpha3 has started, msut be aborted\n journalctl -f -n 40\n\nrpm -Uvh elasticsearch-5.0.0-alpha4.rpm\nsystemctl daemon-reload\nsystemctl restart elasticsearch\n```\n\nOutput of journalctl\n\n```\nJun 22 12:44:56 localhost.localdomain elasticsearch[31947]: Exception in thread \"main\" java.lang.IllegalStateException: Unable to access 'path.scripts' (\/etc\/elasticsearch\/scripts)\n```\n\nWhen installing alpha3 first, then `\/etc\/elasticsearch\/scripts` is not created, neither it is on upgrade to alpha4. However, there are additional problems when creating that directory\n\n```\nmkdir \/etc\/elasticsearch\/scripts\nsystemctl start elasticsearch\njournalctl -f -n 40\n```\n\noutput is\n\n```\nJun 22 12:49:01 localhost.localdomain elasticsearch[31996]: [2016-06-22 12:49:01,953][INFO ][node                     ] [Hank McCoy] version[5.0.0-alpha4], pid[31996], build[b0da471\/2016-06-22T12:33:48.164Z], OS[Linux\/3.10.0-327.18.2.el7.x86_64\/amd64], JVM[Oracle Corporation\/OpenJDK 64-Bit Server VM\/1.8.0_91\/25.91-b14]\nJun 22 12:49:01 localhost.localdomain elasticsearch[31996]: [2016-06-22 12:49:01,954][INFO ][node                     ] [Hank McCoy] initializing ...\nJun 22 12:49:01 localhost.localdomain elasticsearch[31996]: Exception in thread \"main\" java.lang.IllegalStateException: Unable to initialize modules\nJun 22 12:49:01 localhost.localdomain elasticsearch[31996]: Likely root cause: java.nio.file.NoSuchFileException: \/usr\/share\/elasticsearch\/modules\/ingest-grok\/plugin-descriptor.properties\nJun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)\nJun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\nJun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\nJun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)\nJun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at java.nio.file.Files.newByteChannel(Files.java:361)\nJun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at java.nio.file.Files.newByteChannel(Files.java:407)\nJun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)\nJun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at java.nio.file.Files.newInputStream(Files.java:152)\nJun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.plugins.PluginInfo.readFromProperties(PluginInfo.java:74)\nJun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.plugins.PluginsService.getModuleBundles(PluginsService.java:327)\nJun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.plugins.PluginsService.<init>(PluginsService.java:131)\nJun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.node.Node.<init>(Node.java:211)\nJun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.node.Node.<init>(Node.java:172)\nJun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.bootstrap.Bootstrap$5.<init>(Bootstrap.java:175)\nJun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:175)\nJun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:250)\nJun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:96)\nJun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:91)\nJun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.cli.SettingCommand.execute(SettingCommand.java:54)\nJun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:91)\nJun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.cli.Command.main(Command.java:53)\nJun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:70)\nJun 22 12:49:01 localhost.localdomain elasticsearch[31996]: at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:63)\nJun 22 12:49:01 localhost.localdomain elasticsearch[31996]: Refer to the log for complete error details.\n```\n\nThe issue here seems to be that the `ingest-grok` module does not exist after alpha3 anymore (has been moved into `ingest-common`, but it was not cleaned up properly on upgrade.\n\nIf we decide that upgrading from alpha3 to newer alpha version does not need to work, than the last part of this can potentially be ignored, however we still need to ensure that upgrading from any older (non alpha) elasticsearch version to a newer one works.\n","id":"161732912","title":"Packaging: Upgrading from alpha3 using RPM does not work","reopen_by":"clintongormley","opened_on":"2016-06-22T16:51:10Z","closed_by":"jasontedor"},{"number":"18974","comments":[{"date":"2016-06-20T11:00:16Z","author":"jpountz","text":"I tried the following locally on a fresh download of 5.0 alpha3\n\n```\nPUT test\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"path_analyzer\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"path_tokenizer\"\n        }\n      },\n      \"tokenizer\": {\n        \"path_tokenizer\": {\n          \"type\": \"path_hierarchy\"\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"test\": {\n      \"properties\": {\n        \"foo\": {\n          \"type\": \"string\",\n          \"analyzer\": \"path_analyzer\"\n        }\n      }\n    }\n  }\n}\n\nGET test\/_mapping\n```\n\nwhich returned\n\n```\n{\n  \"test\": {\n    \"mappings\": {\n      \"test\": {\n        \"properties\": {\n          \"foo\": {\n            \"type\": \"text\",\n            \"analyzer\": \"path_analyzer\"\n          }\n        }\n      }\n    }\n  }\n}\n```\n\nThis looks correct to me. Could you provide a recreation for the issue that you are seeing?\n"},{"date":"2016-06-20T13:15:03Z","author":"isaias","text":"@jpountz I've tried to reproduce on my environment and it didn't happen anymore. The only difference was that I updated from Alpha 2 to Aplha3 using RPM. I've tried to do it again but the error didn't happen. I'll close. Thanks.\n"},{"date":"2016-06-20T22:05:35Z","author":"isaias","text":"@jpountz, I reproduced the error with a fresh Alpha 3, see the \"bar\" definition.\n\n```\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"path_analyzer\": {\n          \"type\": \"custom\",\n          \"tokenizer\": \"path_tokenizer\"\n        }\n      },\n      \"tokenizer\": {\n        \"path_tokenizer\": {\n          \"type\": \"path_hierarchy\"\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"test\": {\n      \"properties\": {\n        \"foo\": {\n          \"type\": \"string\",\n          \"analyzer\": \"path_analyzer\"\n        },\n        \"bar\": {\n            \"type\": \"string\",\n            \"index\": \"not_analyzed\",\n            \"fields\": {\n              \"tree\": {\n                \"type\": \"string\",\n                \"analyzer\": \"path_analyzer\"\n              }\n            },\n            \"include_in_all\": false,\n            \"fielddata\": false\n          }\n      }\n    }\n  }\n}\n```\n"},{"date":"2016-06-21T09:28:28Z","author":"clintongormley","text":"Actually, this  is just to do with `include_in_all`:\n\n```\nPUT t\n{\n  \"mappings\": {\n    \"test\": {\n      \"properties\": {\n        \"foo\": {\n          \"type\": \"string\",\n          \"include_in_all\": false\n        }\n      }\n    }\n  }\n}\n```\n"},{"date":"2016-06-21T12:51:40Z","author":"jpountz","text":"OK, so we need to add `include_in_all` to the whitelist of parameters we automatically upgrade.\n"}],"reopen_on":"2016-06-20T22:05:35Z","opened_by":"isaias","closed_on":"2016-06-21T16:00:24Z","description":"**Elasticsearch version**: \n5.0 A3\n\n**JVM version**: \n1.8.0_91\n\n**OS version**:\nCentOS 7\n\n**Description of the problem including expected versus actual behavior**:\nOn 5.0 A2 the automatic upgrade https:\/\/github.com\/elastic\/elasticsearch\/pull\/17861 was working fine for a mapped string using an analyzer (Path Hierarchy Tokenizer), after upgrade to 5.0 A3 it didn't work, so I think we have a regression at that point.\n\n**Steps to reproduce**:\n1. Define an analyser for a path_hierarchy\n\n\"analyzer\": {\n        \"paths\": {\n          \"tokenizer\": \"path_hierarchy\"\n        }\n1. Set it for an analyzed string on mapping \n\n**Provide logs (if relevant)**:\n","id":"161163322","title":"Possible Regression on 5.0A3 for string to text conversion","reopen_by":"isaias","opened_on":"2016-06-20T10:23:02Z","closed_by":"jpountz"},{"number":"18935","comments":[{"date":"2016-06-17T03:39:31Z","author":"spalger","text":"Just confirmed this is not broken in gradle 2.13\n"},{"date":"2016-06-17T06:01:21Z","author":"xuzhou2020","text":"I have the save question as you when building with gradle\n"},{"date":"2016-06-17T07:31:14Z","author":"javanna","text":"Related: https:\/\/discuss.gradle.org\/t\/gradle-2-14-breaks-plugins-using-consolerenderer\/18045 .\n"},{"date":"2016-06-18T17:19:08Z","author":"rjernst","text":"This is actually worse than just the class being moved. Apparently they considered the package org.gradle.logging to be \"internal\", and in 2.14 internal classes are finally not available to plugins (and this class move makes it truly internal). So until they add back ProgressLogger as part of the plugin API, all our nice logging would disappear...\n\nI'm going to add a check for now in BuildPlugin init that the gradle version is equal exactly to 2.13...\n"},{"date":"2016-06-18T17:21:20Z","author":"rjernst","text":"Ah and of course this is hard to check for 2.13 because the failure happens inside buildSrc before we even get to check the gradle version...\n"},{"date":"2016-06-18T17:29:37Z","author":"rjernst","text":"I opened #18955 as a stopgap so at least the error message is clear when trying to use 2.14\n"},{"date":"2016-06-24T13:28:15Z","author":"jprante","text":"Due to gradle core developer Adrian Kelly\n\nhttps:\/\/discuss.gradle.org\/t\/bug-in-gradle-2-14-rc1-no-service-of-type-styledtextoutputfactory\/17638\/3\n\nthere is no big chance that ProgressLogger will be available (again). So my suggestion is to adapt to Gradle 2.14 (including upcoming Gradle 3) as soon as possible by aligning the Elasticsearch build scripts\/plugins to the reduced capabilities in https:\/\/docs.gradle.org\/current\/userguide\/logging.html\n"},{"date":"2016-07-13T21:48:32Z","author":"mfussenegger","text":"Any chance to reconsider https:\/\/github.com\/elastic\/elasticsearch\/pull\/13744 due to this issue here?\nI'm not sure if keeping a 50kb blob out of the repo is worth forcing potential contributors to either downgrade system gradle or start keeping around a bunch of gradle versions that happen to work with ES.\n"},{"date":"2016-07-13T22:19:54Z","author":"rjernst","text":"@jprante \n\n> there is no big chance that ProgressLogger will be available (again)\n\nThat is simply not true. I spoke with developers at gradle during Gradle Summit and they understand that progress logger is important. I expect it to come back, in some form, in the future:\nhttps:\/\/discuss.gradle.org\/t\/can-gradle-team-add-progresslogger-and-progressloggerfactory-to-public-api\/18176\/6\n\n@mfussenegger \n\n> Any chance to reconsider #13744 due to this issue here?\n\nThe size is not the issue there. It is that we do not want _binary_ blobs in our repo. I would be ok with a custom equivalent of the gradle wrapper that depended on java 8 and jjs to download the gradle binary, but I have not investigated the real feasibility of such a solution. In the meantime, you don't need to manage \"a bunch\" of versions, just two, 2.13 and whatever other version you are on. You can add your own gradle wrapper file then that just runs gradle 2.13 wherever it is on your system. I would even be ok with adding this to the gitignore so that you can update the repo without it looking like some outlier file.\n"},{"date":"2016-07-13T22:39:38Z","author":"mfussenegger","text":"> It is that we do not want binary blobs in our repo.\n\nAren't the zip files for bwc testing also binary files?\n\n>  In the meantime, you don't need to manage \"a bunch\" of versions, just two, 2.13 \n\nI'm probably being a bit too pessimistic here and exaggerating.\nAnyway, it's not much of a problem for me personally. Just wanted to bring it up because it definetly _is_ a stepping stone.\n"},{"date":"2016-10-03T17:40:22Z","author":"manterfield","text":"I think it would be helpful to add the requirement for Gradle 2.13 to the docs for contribution and to make it more explicit that it is required in the main readme. Currently the readme says: \n\n> You\u2019ll need to have a modern version of Gradle installed \u2013 2.13 should do.\n\nWhich makes it sound like 2.13 or upwards is fine. \n\nThere's no mention of version on the contribution doc.\n\nIt's only a small issue and the error message makes it very clear what has gone wrong, but it could save the time of people like me, as I just downloaded the latest version of Gradle purely for the sake of contributing to the project.\n\nI'd be happy to make the change myself since I was after something simple first anyway. Is it precisely version 2.13 that works, or can slightly older Gradle versions work too? \n"},{"date":"2016-10-03T17:41:57Z","author":"rjernst","text":"@manterfield Please do make a PR! I agree we should update the readme\/contrib doc wording given our current limitation.\n"},{"date":"2016-10-03T17:42:29Z","author":"rjernst","text":"And it must be 2.13 at this time.\n"},{"date":"2016-10-06T10:20:58Z","author":"manterfield","text":"Thanks @rjernst, made a PR (#20776) with doc updates in. \n"},{"date":"2017-01-19T11:20:27Z","author":"ywelsch","text":"Closed by #22669. The docs will be updated once we have moved our builds to use Gradle 3.x and feel comfortable removing support for 2.13."},{"date":"2017-01-20T23:18:36Z","author":"jasontedor","text":"Sorry, this has to be reopened, IntelliJ is unhappy with the change."},{"date":"2017-01-24T13:04:17Z","author":"ywelsch","text":"Pushed a fix for IntelliJ."}],"reopen_on":"2017-01-20T23:18:22Z","opened_by":"spalger","closed_on":"2017-01-24T13:04:17Z","description":"**Elasticsearch version**: master\n\n**JVM version**:\n\n```\njava version \"1.8.0_77\"\nJava(TM) SE Runtime Environment (build 1.8.0_77-b03)\nJava HotSpot(TM) 64-Bit Server VM (build 25.77-b03, mixed mode)\n```\n\n**OS version**: OS X 10.11.5\n\n**Description of the problem including expected versus actual behavior**:\nTrying to run `gradle build` but getting an error instead of build output, console output below. It looks like the `org.gradle.logging.progress` package was [added in 2.14](https:\/\/github.com\/gradle\/gradle\/commit\/a8be591089bbf9df86fcc58fc155b8e1329df524) and moved the `org.gradle.logging.ProgressLogger` class in the process.\n\n**Steps to reproduce**:\n1. Install gradle 2.14\n2. Checkout `master`\n3. Run `gradle build`\n\n**Provide logs (if relevant)**:\n\n``` sh\nelasticsearch [master] $ gradle build\n:buildSrc:clean\n:buildSrc:compileJava\n:buildSrc:compileGroovy\nstartup failed:\n\/Users\/spalger\/dev\/es\/elasticsearch\/buildSrc\/src\/main\/groovy\/com\/carrotsearch\/gradle\/junit4\/TestProgressLogger.groovy: 28: unable to resolve class org.gradle.logging.ProgressLogger\n @ line 28, column 1.\n   import org.gradle.logging.ProgressLogger\n   ^\n\n\/Users\/spalger\/dev\/es\/elasticsearch\/buildSrc\/src\/main\/groovy\/org\/elasticsearch\/gradle\/vagrant\/TapLoggerOutputStream.groovy: 25: unable to resolve class org.gradle.logging.ProgressLogger\n @ line 25, column 1.\n   import org.gradle.logging.ProgressLogger\n   ^\n\n\/Users\/spalger\/dev\/es\/elasticsearch\/buildSrc\/src\/main\/groovy\/org\/elasticsearch\/gradle\/vagrant\/VagrantLoggerOutputStream.groovy: 23: unable to resolve class org.gradle.logging.ProgressLogger\n @ line 23, column 1.\n   import org.gradle.logging.ProgressLogger\n   ^\n\n3 errors\n\n:buildSrc:compileGroovy FAILED\n\nFAILURE: Build failed with an exception.\n\n* What went wrong:\nExecution failed for task ':compileGroovy'.\n> Compilation failed; see the compiler error output for details.\n\n* Try:\nRun with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.\n\nBUILD FAILED\n\nTotal time: 5.056 secs\n```\n","id":"160804922","title":"Gradle 2.14 compatibility?","reopen_by":"jasontedor","opened_on":"2016-06-17T03:24:51Z","closed_by":"ywelsch"},{"number":"18539","comments":[{"date":"2016-05-24T09:36:50Z","author":"clintongormley","text":"@dadoonet could you take a look please?\n"},{"date":"2016-06-22T17:36:38Z","author":"jsnod","text":"Is this blocking the release of v2.3.4?\n"},{"date":"2016-06-23T20:45:36Z","author":"dadoonet","text":"@yamap77 Did you set any specific metadata for the S3 bucket: `test-snapshot`?\nCould you also try to explicitly set a region like `\"region\": \"us-west\"`?\n"},{"date":"2016-06-23T21:03:52Z","author":"yamap77","text":"Hi,\n\nI had fixed this issue. It turned out that I have to disable the security manager. After I disable it, I can sent data to s3 now.\n"},{"date":"2016-06-23T21:24:50Z","author":"dadoonet","text":"I'm reopening it. You should never disable the security manager. We need to understand what is happening.\n\nI'll try to reproduce but if you have any details which would help to understand I'd appreciate a lot!\n"},{"date":"2016-11-03T07:16:22Z","author":"mahesh-maney","text":"Hi,\nCurious to know if this issue is resolved in 2.3.5? \n"},{"date":"2016-11-03T07:31:24Z","author":"dadoonet","text":"@mahesh-maney I don't think so. It's marked as 2.4.0 and 5.0.0 and according to the commits it has not been back ported in 2.3 branch.\n"}],"reopen_on":"2016-06-23T21:24:50Z","opened_by":"yamap77","closed_on":"2016-07-01T08:43:16Z","description":"**Elasticsearch version**: 2.3.2\n\n**JVM version**: 25.91-b14\n\n**OS version**: \n\n**Description of the problem including expected versus actual behavior**:\n\n**Steps to reproduce**:\n\n``` sh\ncurl -XPUT 'http:\/\/localhost:9200\/_snapshot\/test-snapshot' -d '{\n    \"type\": \"s3\",\n    \"settings\": {\n        \"bucket\": \"test-snapshot\",\n         \"access_key\": \"XXXXX\", \n         \"secret_key\": \"XXXX\"    }\n}'\n```\n\n**Provide logs (if relevant)**:\n\n```\n[2016-05-24 00:12:38,425][WARN ][repositories             ] [54.210.62.226 (m3.xlarge) - i-4e4e7ac9] [awsmt-deviceprofile-es-snapshot] failed to verify repository\nBlobStoreException[failed to check if blob exists]; nested: AmazonClientException[Error while loading partitions file from com\/amazonaws\/partitions\/endpoints.json]; nested: JsonMappingException[Can not access public com.amazonaws.partitions.model.Partitions(java.lang.String,java.util.List) (from class com.amazonaws.partitions.model.Partitions; failed to set access: access denied (\"java.lang.reflect.ReflectPermission\" \"suppressAccessChecks\")]; nested: IllegalArgumentException[Can not access public com.amazonaws.partitions.model.Partitions(java.lang.String,java.util.List) (from class com.amazonaws.partitions.model.Partitions; failed to set access: access denied (\"java.lang.reflect.ReflectPermission\" \"suppressAccessChecks\")];\n        at org.elasticsearch.cloud.aws.blobstore.S3BlobContainer.blobExists(S3BlobContainer.java:65)\n        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.verify(BlobStoreIndexShardRepository.java:240)\n        at org.elasticsearch.repositories.VerifyNodeRepositoryAction.doVerify(VerifyNodeRepositoryAction.java:121)\n        at org.elasticsearch.repositories.VerifyNodeRepositoryAction.verify(VerifyNodeRepositoryAction.java:86)\n        at org.elasticsearch.repositories.RepositoriesService.verifyRepository(RepositoriesService.java:214)\n        at org.elasticsearch.repositories.RepositoriesService$VerifyingRegisterRepositoryListener.onResponse(RepositoriesService.java:436)\n        at org.elasticsearch.repositories.RepositoriesService$VerifyingRegisterRepositoryListener.onResponse(RepositoriesService.java:421)\n        at org.elasticsearch.cluster.AckedClusterStateUpdateTask.onAllNodesAcked(AckedClusterStateUpdateTask.java:63)\n        at org.elasticsearch.cluster.service.InternalClusterService$SafeAckedClusterStateTaskListener.onAllNodesAcked(InternalClusterService.java:733)\n        at org.elasticsearch.cluster.service.InternalClusterService$AckCountDownListener.onNodeAck(InternalClusterService.java:1013)\n        at org.elasticsearch.cluster.service.InternalClusterService$DelegetingAckListener.onNodeAck(InternalClusterService.java:952)\n        at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:637)\n        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:772)\n        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: com.amazonaws.AmazonClientException: Error while loading partitions file from com\/amazonaws\/partitions\/endpoints.json\n        at com.amazonaws.partitions.PartitionsLoader.loadPartitionFromStream(PartitionsLoader.java:99)\n        at com.amazonaws.partitions.PartitionsLoader.build(PartitionsLoader.java:88)\n        at com.amazonaws.regions.RegionMetadataFactory.create(RegionMetadataFactory.java:30)\n        at com.amazonaws.regions.RegionUtils.initialize(RegionUtils.java:66)\n        at com.amazonaws.regions.RegionUtils.getRegionMetadata(RegionUtils.java:54)\n        at com.amazonaws.regions.RegionUtils.getRegion(RegionUtils.java:104)\n        at com.amazonaws.services.s3.AmazonS3Client.resolveServiceEndpoint(AmazonS3Client.java:4195)\n        at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1006)\n        at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:991)\n        at org.elasticsearch.cloud.aws.blobstore.S3BlobContainer.blobExists(S3BlobContainer.java:60)\n        ... 17 more\nCaused by: com.fasterxml.jackson.databind.JsonMappingException: Can not access public com.amazonaws.partitions.model.Partitions(java.lang.String,java.util.List) (from class com.amazonaws.partitions.model.Partitions; failed to set access: access denied (\"java.lang.reflect.ReflectPermission\" \"suppressAccessChecks\")\n        at com.fasterxml.jackson.databind.deser.DeserializerCache._createAndCache2(DeserializerCache.java:269)\n        at com.fasterxml.jackson.databind.deser.DeserializerCache._createAndCacheValueDeserializer(DeserializerCache.java:244)\n        at com.fasterxml.jackson.databind.deser.DeserializerCache.findValueDeserializer(DeserializerCache.java:142)\n        at com.fasterxml.jackson.databind.DeserializationContext.findRootValueDeserializer(DeserializationContext.java:461)\n        at com.fasterxml.jackson.databind.ObjectMapper._findRootDeserializer(ObjectMapper.java:3833)\n        at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:3727)\n        at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2794)\n        at com.amazonaws.partitions.PartitionsLoader.loadPartitionFromStream(PartitionsLoader.java:96)\n        ... 26 more\nCaused by: java.lang.IllegalArgumentException: Can not access public com.amazonaws.partitions.model.Partitions(java.lang.String,java.util.List) (from class com.amazonaws.partitions.model.Partitions; failed to set access: access denied (\"java.lang.reflect.ReflectPermission\" \"suppressAccessChecks\")\n        at com.fasterxml.jackson.databind.util.ClassUtil.checkAndFixAccess(ClassUtil.java:513)\n        at com.fasterxml.jackson.databind.deser.impl.CreatorCollector._fixAccess(CreatorCollector.java:280)\n        at com.fasterxml.jackson.databind.deser.impl.CreatorCollector.verifyNonDup(CreatorCollector.java:327)\n        at com.fasterxml.jackson.databind.deser.impl.CreatorCollector.addPropertyCreator(CreatorCollector.java:184)\n        at com.fasterxml.jackson.databind.deser.BasicDeserializerFactory._addDeserializerConstructors(BasicDeserializerFactory.java:493)\n        at com.fasterxml.jackson.databind.deser.BasicDeserializerFactory._constructDefaultValueInstantiator(BasicDeserializerFactory.java:324)\n        at com.fasterxml.jackson.databind.deser.BasicDeserializerFactory.findValueInstantiator(BasicDeserializerFactory.java:254)\n        at com.fasterxml.jackson.databind.deser.BeanDeserializerFactory.buildBeanDeserializer(BeanDeserializerFactory.java:222)\n        at com.fasterxml.jackson.databind.deser.BeanDeserializerFactory.createBeanDeserializer(BeanDeserializerFactory.java:142)\n        at com.fasterxml.jackson.databind.deser.DeserializerCache._createDeserializer2(DeserializerCache.java:403)\n        at com.fasterxml.jackson.databind.deser.DeserializerCache._createDeserializer(DeserializerCache.java:352)\n        at com.fasterxml.jackson.databind.deser.DeserializerCache._createAndCache2(DeserializerCache.java:264)\n        ... 33 more\n```\n","id":"156417953","title":"Jackson databind Exception when creating repository\/sending snapshot to s3","reopen_by":"dadoonet","opened_on":"2016-05-24T04:32:55Z","closed_by":"tlrx"},{"number":"18436","comments":[{"date":"2016-05-25T09:23:48Z","author":"dadoonet","text":"This has been fixed in master but I'm reopening as it now needs to be backported in 2.x, 2.3 and for elasticsearch 1.7.\n"},{"date":"2016-05-27T15:28:56Z","author":"dadoonet","text":"Closed with:\n- master (5.0.0): elastic\/elasticsearch#18451 \n- 2.x (2.4.0): elastic\/elasticsearch#18571\n- 1.7 (2.8.4 - old versioning): https:\/\/github.com\/elastic\/elasticsearch-cloud-azure\/pull\/117\n"}],"reopen_on":"2016-05-25T09:23:48Z","opened_by":"dadoonet","closed_on":"2016-05-27T15:28:56Z","description":"**Elasticsearch version**: tested with 2.4.0-SNAPSHOT\n\n**Description of the problem including expected versus actual behavior**: \n\nWhen you create your azure repository, you get a message `The specified blob does not exist`\n\n**Steps to reproduce**:\n1. `bin\/plugin install cloud-azure`\n2. modify `config\/elasticsearch.yml` to set `cloud.azure.storage.my_account.account: xxx` and `cloud.azure.storage.my_account.key: yyy`\n3. `bin\/elasticsearch`\n4. Run the following script:\n\n``` sh\n# Clean existing repo if any\ncurl -XDELETE localhost:9200\/_snapshot\/my_backup1?pretty\n\n# Create azure repo\n# This call gives in log: The specified blob does not exist\ncurl -XPUT localhost:9200\/_snapshot\/my_backup1?pretty -d '{\n  \"type\": \"azure\"\n}'\n```\n\n**Provide logs (if relevant)**:\n\n```\n[2016-05-18 11:03:24,914][WARN ][org.elasticsearch.cloud.azure.blobstore] [azure] can not remove [tests-ilmRPJ8URU-sh18yj38O6g\/] in container {elasticsearch-snapshots}: The specified blob does not exist.\n```\n\nNote that there is a trailing `\/` at the end which might explain the issue.\n","id":"155456220","title":"Azure repository does not remove temporary dir","reopen_by":"dadoonet","opened_on":"2016-05-18T09:32:52Z","closed_by":"dadoonet"},{"number":"18158","comments":[{"date":"2016-05-05T15:52:06Z","author":"jasontedor","text":"This is the expected behavior in your circumstance. When `rpm -U` is invoked, there are different scenarios. The scenarios come from there being three possible config files: the original config file distributed with alpha1, your config file with edits, and the config file that is distributed with alpha2. In this situation, you have alpha1 = x, your config = y and it turns out that alpha2 = z (there was a change to the shipped config file between alpha1 and alpha2). In this case, rpm assumes that z must be used with the new package (it can not safely assume that either x or y are safe to use with the new package). That is why it will not produce an `rpmnew` here.\n"},{"date":"2016-05-05T15:56:19Z","author":"Martin-Logan","text":"Would it not be better then to produce an rpmold rather than entirely lose the configuration?\n"},{"date":"2016-05-05T15:59:05Z","author":"jasontedor","text":"> Would it not be better then to produce an rpmold rather than entirely lose the configuration?\n\nIt should have produced an `rpmsave`. Are you saying that it did not? If not, that is a bug that we should fix.\n"},{"date":"2016-05-05T16:01:57Z","author":"Martin-Logan","text":"It indeed did not\n"},{"date":"2016-05-05T16:04:18Z","author":"jasontedor","text":"> It indeed did not\n\nYes, I just reproduced this as well. Thanks for reporting.\n"},{"date":"2016-05-06T15:40:40Z","author":"jasontedor","text":"@Martin-Logan I've marked you as eligible for the [Pioneer Program](https:\/\/www.elastic.co\/blog\/elastic-pioneer-program) and opened #18188.\n"}],"reopen_on":"2016-05-05T15:59:05Z","opened_by":"Martin-Logan","closed_on":"2016-05-06T17:24:54Z","description":"**Elasticsearch version**:\n5.0 Alpha 2\n**JVM version**:\nopenjdk version \"1.8.0_91\"\n**OS version**:\nCentos 7\n**Description of the problem including expected versus actual behavior**:\ninplace upgrade using 64bit rpm (e.g. rpm -U elasticsearch-5.0.0-alpha2.rpm) overwrites the elasticsearch.yml file . Expect it to create an rpmnew file. \n**Steps to reproduce**:\n 1.install alpha 1\n 2.edit elasticsearch.yml\n 3.upgrade with rpm -U elasticsearch-5.0.0-alpha2.rpm\n\n**Provide logs (if relevant)**:\n","id":"153259598","title":"rpm -U deletes elasticsearch.yml","reopen_by":"jasontedor","opened_on":"2016-05-05T15:40:23Z","closed_by":"jasontedor"},{"number":"18135","comments":[{"date":"2016-05-04T16:15:36Z","author":"clintongormley","text":"Trying to work up a recreation\n"},{"date":"2016-05-04T17:05:41Z","author":"clintongormley","text":"OK - simple recreation.  Start a cluster with two nodes, then run the following:\n\n```\nDELETE t\n\nPOST t\/t\/_bulk\n{\"index\":{}}\n{\"@timestamp\":\"2000-01-01T00:00:00.0+00\"}\n{\"index\":{}}\n{\"@timestamp\":\"2000-01-01T00:00:00.0+00\"}\n{\"index\":{}}\n{\"@timestamp\":\"2000-01-01T00:00:00.0+00\"}\n{\"index\":{}}\n{\"@timestamp\":\"2000-01-01T00:00:00.0+00\"}\n{\"index\":{}}\n{\"@timestamp\":\"2000-01-01T00:00:00.0+00\"}\n{\"index\":{}}\n{\"@timestamp\":\"2000-01-01T00:00:00.0+00\"}\n{\"index\":{}}\n{\"@timestamp\":\"2000-01-01T00:00:00.0+00\"}\n{\"index\":{}}\n{\"@timestamp\":\"2000-01-01T00:00:00.0+00\"}\n{\"index\":{}}\n{\"@timestamp\":\"2000-01-01T00:00:00.0+00\"}\n\nGET t\/_field_stats?level=indices&fields=@timestamp\n```\n\nThe above returns:\n\n```\n{\n   \"error\": {\n      \"root_cause\": [\n         {\n            \"type\": \"illegal_state_exception\",\n            \"reason\": \"trying to merge the field stats of field [@timestamp] from index [t] but the field type is incompatible, try to set the 'level' option to 'indices'\"\n         }\n      ],\n      \"type\": \"illegal_state_exception\",\n      \"reason\": \"trying to merge the field stats of field [@timestamp] from index [t] but the field type is incompatible, try to set the 'level' option to 'indices'\"\n   },\n   \"status\": 500\n}\n```\n\nWith the following stack trace:\n\n```\n[2016-05-04 19:04:43,705][WARN ][rest.suppressed          ] \/t\/_field_stats Params: {level=indices, index=t, fields=@timestamp}\njava.lang.IllegalStateException: trying to merge the field stats of field [@timestamp] from index [t] but the field type is incompatible, try to set the 'level' option to 'indices'\n  at org.elasticsearch.action.fieldstats.TransportFieldStatsTransportAction.newResponse(TransportFieldStatsTransportAction.java:108)\n  at org.elasticsearch.action.fieldstats.TransportFieldStatsTransportAction.newResponse(TransportFieldStatsTransportAction.java:58)\n  at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$AsyncBroadcastAction.finishHim(TransportBroadcastAction.java:248)\n  at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$AsyncBroadcastAction.onOperation(TransportBroadcastAction.java:213)\n  at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$AsyncBroadcastAction$1.handleResponse(TransportBroadcastAction.java:193)\n  at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$AsyncBroadcastAction$1.handleResponse(TransportBroadcastAction.java:180)\n  at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:789)\n  at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:178)\n  at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:143)\n  at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n  at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n  at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n  at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)\n  at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)\n  at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)\n  at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)\n  at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n  at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n  at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)\n  at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)\n  at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)\n  at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\n  at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)\n  at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)\n  at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\n  at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\n  at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n  at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n  at java.lang.Thread.run(Thread.java:745)\n```\n"},{"date":"2016-05-04T21:28:35Z","author":"jimczi","text":"This is a serialization bug in the field stats transport that was introduced when the new point api was added. It occurs only with 5.0.0-alpha2 and has been fixed in master with this commit https:\/\/github.com\/elastic\/elasticsearch\/commit\/f600c4ab9c4a575a269287bb5aaa22d828c56496\n"},{"date":"2016-05-11T15:36:32Z","author":"nellicus","text":"@jimferenczi this effectively prevents, at least in my environment, any use in kibana of the data coming from logstash. just raising a concern that this might impede proper testing of alpha2 through our user base\n"},{"date":"2016-05-12T10:01:59Z","author":"clintongormley","text":"@nellicus yeah - not much we can do about it until the next release\n"},{"date":"2016-05-13T14:33:55Z","author":"anhlqn","text":"This also occurred when I tried to use the winlogbeat indexes on a fresh installation without Logstash\n"},{"date":"2016-05-27T00:33:37Z","author":"gluckspilz","text":"Will adding \"format\" : \"date_time\" to the mapping for the index created by logstash stop this error?\n"},{"date":"2016-05-30T19:30:57Z","author":"michalterbert","text":"@gluckspilz: for me doesn't work 👎 \n"},{"date":"2016-06-01T16:28:37Z","author":"clintongormley","text":"This is fixed in 5.0.0-alpha3, which is out already\n"}],"reopen_on":"2016-05-04T16:14:50Z","opened_by":"nellicus","closed_on":"2016-05-04T21:28:35Z","description":"linked to https:\/\/github.com\/elastic\/kibana\/issues\/7127\n\n**Elasticsearch version**:\n5.0.0-alpha2\n\n**JVM version**:\njava version \"1.8.0_45\"\n**OS version**:\nLinux\n**Description of the problem including expected versus actual behavior**:\nError when calling _field_stats API on Logstash 5.0.0-alpha2 generated index\n\n```\nabonuccelli@w530 \/opt\/elk\/PROD\/scripts $ curl -XGET \"https:\/\/192.168.1.105:9200\/logstash-syslog-2016.05.04\/_field_stats?fields=@timestamp&level=indices&pretty\" -k --cacert \/opt\/elk\/PROD\/FS\/secure\/cacert.pem  -u elastic:xxxxxx\n{\n  \"error\" : {\n    \"root_cause\" : [ {\n      \"type\" : \"illegal_state_exception\",\n      \"reason\" : \"trying to merge the field stats of field [@timestamp] from index [logstash-syslog-2016.05.04] but the field type is incompatible, try to set the 'level' option to 'indices'\"\n    } ],\n    \"type\" : \"illegal_state_exception\",\n    \"reason\" : \"trying to merge the field stats of field [@timestamp] from index [logstash-syslog-2016.05.04] but the field type is incompatible, try to set the 'level' option to 'indices'\"\n  },\n  \"status\" : 500\n}\n```\n\ncalling the same on .monitoring-es-\\* index timestamp field works ok\n\n```\nabonuccelli@w530 \/opt\/elk\/PROD\/scripts $  curl -XGET \"https:\/\/192.168.1.105:9200\/.monitoring-es-2-2016.05.04\/_field_stats?fields=timestamp&level=indices&pretty\" -k --cacert \/opt\/elk\/PROD\/FS\/secure\/cacert.pem  -u elastic:xxxxxx\n{\n  \"_shards\" : {\n    \"total\" : 1,\n    \"successful\" : 1,\n    \"failed\" : 0\n  },\n  \"indices\" : {\n    \".monitoring-es-2-2016.05.04\" : {\n      \"fields\" : {\n        \"timestamp\" : {\n          \"max_doc\" : 21360,\n          \"doc_count\" : 21360,\n          \"density\" : 100,\n          \"sum_doc_freq\" : -1,\n          \"sum_total_term_freq\" : 21360,\n          \"min_value\" : 1462353531772,\n          \"min_value_as_string\" : \"2016-05-04T09:18:51.772Z\",\n          \"max_value\" : 1462370153801,\n          \"max_value_as_string\" : \"2016-05-04T13:55:53.801Z\"\n        }\n      }\n    }\n  }\n\n}\n```\n\n**Provide logs (if relevant)**:\n\n```\n[2016-05-04 15:50:22,052][WARN ][rest.suppressed          ] \/logstash-syslog-2016.05.04\/_field_stats Params: {pretty=, level=indices, index=logstash-syslog-2016.05.04, fields=@timestamp}\njava.lang.IllegalStateException: trying to merge the field stats of field [@timestamp] from index [logstash-syslog-2016.05.04] but the field type is incompatible, try to set the 'level' option to 'indices'\n    at org.elasticsearch.action.fieldstats.TransportFieldStatsTransportAction.newResponse(TransportFieldStatsTransportAction.java:108)\n    at org.elasticsearch.action.fieldstats.TransportFieldStatsTransportAction.newResponse(TransportFieldStatsTransportAction.java:58)\n    at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$AsyncBroadcastAction.finishHim(TransportBroadcastAction.java:248)\n    at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$AsyncBroadcastAction.onOperation(TransportBroadcastAction.java:213)\n    at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$AsyncBroadcastAction$1.handleResponse(TransportBroadcastAction.java:193)\n    at org.elasticsearch.action.support.broadcast.TransportBroadcastAction$AsyncBroadcastAction$1.handleResponse(TransportBroadcastAction.java:180)\n    at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:789)\n    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:178)\n    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:143)\n    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)\n    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)\n    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)\n    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)\n    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\n    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)\n    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)\n    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)\n    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)\n    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\n    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)\n    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)\n    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)\n    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\n    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)\n    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)\n    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\n    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\n    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n\n```\n\n**other info\n\n@timestamp mapping for problematic logstash-syslog-\\* index\n\n```\nlogstash-syslog-* \n\n \"timestamp\" : {\n            \"type\" : \"date\",\n           }\n```\n\ntimestamp mapping for .monitoring-es-\\* index \n\n```\n \"timestamp\" : {\n            \"type\" : \"date\",\n            \"format\" : \"date_time\"\n          }\n```\n","id":"153019392","title":"trying to merge the field stats of field but the field type is incompatible","reopen_by":"clintongormley","opened_on":"2016-05-04T14:01:13Z","closed_by":"jimczi"},{"number":"17920","comments":[{"date":"2016-04-25T18:07:50Z","author":"clintongormley","text":"@hrfuller the `format` parameter in the agg is for the display format only.  For input formats, you can add `epoch_seconds` to your field mapping, eg:\n\n```\n\"mappings\": {\n  \"t\": {\n    \"properties\": {\n      \"date\": {\n        \"type\": \"date\",\n        \"format\": \"strict_date_optional_time||epoch_millis\"\n      }\n    }\n  }\n}\n```\n"},{"date":"2016-04-25T23:15:33Z","author":"hrfuller","text":"@clintongormley My mapping has a date property formatted as `epoch_seconds`. When using a date-range aggregation and passing a date in `epoch_seconds` as the value of the `to` and `from` fields.\nI.e\n\n``` python\n'date_range': {\n    'field': 'dims.time',\n    'ranges': [\n        {\n            'from': 1454463970,\n            'to': 1455150295,\n        }\n    ]\n},\n```\n\n the date-range aggregator interprets the timestamps as `epoch_millis`. Of course I can easily convert the `epoch_seconds` to `epoch_millis`.\n\nThis was a feature request because it seems like it would improve the consistency of the elasticsearch API if date ranges were query-able\/aggregate-able using the date input format specified in the date mapping.\n\n```\n\"version\" : {\n    \"number\" : \"2.3.1\",\n    \"build_hash\" : \"bd980929010aef404e7cb0843e61d0665269fc39\",\n    \"build_timestamp\" : \"2016-04-04T12:25:05Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"5.5.0\"\n  },\n```\n"},{"date":"2016-04-26T14:10:43Z","author":"clintongormley","text":"Sorry, I completely misread the original description.  I thought you were setting the format in the agg, not in the mapping.  This is a bug.\n"}],"reopen_on":"2016-04-26T14:10:08Z","opened_by":"hrfuller","closed_on":null,"description":"<!--\nGitHub is reserved for bug reports and feature requests. The best place\nto ask a general question is at the Elastic Discourse forums at\nhttps:\/\/discuss.elastic.co. If you are in fact posting a bug report or\na feature request, please include one and only one of the below blocks\nin your new issue.\n-->\n\n<!--\nIf you are filing a feature request, please remove the above bug\nreport block and provide responses for all of the below items.\n-->\n\n**Describe the feature**:\n\nWhen aggregating documents with a date field mapping using `format: epoch_seconds` I would like to use epoch seconds in the `to` and `from` fields of a date-range aggregation. At present dates passed in `to` and `from` are not converted to milliseconds before the date-range aggregation.\n\nThis seems inconsistent with how dates are converted from the mapping format to the internal usage format when PUTing documents when the date-field format is specified.\n","id":"150200146","title":"Use epoch seconds in `to` and `from` fields of date range aggregation `ranges` objects.","reopen_by":"clintongormley","opened_on":"2016-04-21T21:50:52Z","closed_by":"clintongormley"},{"number":"17592","comments":[{"date":"2016-04-07T16:06:03Z","author":"clintongormley","text":"Related to https:\/\/github.com\/elastic\/elasticsearch\/issues\/17561\n\nThe difference is that, in this example, `index.mapper.dynamic` is set at the index level rather than in the config file in #17561.\n"},{"date":"2016-04-07T16:34:48Z","author":"clintongormley","text":"Probably caused by https:\/\/github.com\/elastic\/elasticsearch\/pull\/15424\n"},{"date":"2016-05-05T08:03:11Z","author":"clintongormley","text":"Delete previous comment - wrong issue\n"},{"date":"2016-05-05T08:08:30Z","author":"clintongormley","text":"Turns out this has always worked this way, it isn't a regression but it is a bug and could be improved.\n"}],"reopen_on":"2016-05-05T08:07:27Z","opened_by":"tom-mi","closed_on":"2016-07-19T07:03:46Z","description":"**Elasticsearch version**:  2.3.1\n\n**JVM version**: 1.8.0_66\n\n**OS version**: CentOS 7\n\n**Description of the problem including expected versus actual behavior**:\nWhen a new index is auto-created (by indexing a document) using an index template, the setting `index.mapper.dynamic` is not honored: If the document has an unmapped type, it gets indexed nevertheless and the additional type is added to the index.\n\n_Expected result:_ The indexing operation should be rejected as dynamic mapping is disabled for the index created from the template.\n\n**Steps to reproduce**:\n\nCreate template:\n\n```\ncurl -XPUT localhost:9200\/_template\/test?pretty -d'\n{\n  \"template\": \"test_*\",\n  \"settings\": {\n    \"number_of_shards\": 1,\n    \"index.mapper.dynamic\": false\n  },\n  \"mappings\": {\n    \"foo\": {\n      \"properties\": {\n        \"name\": {\n          \"type\": \"string\"\n        }\n      }\n    }\n  }\n}'\n```\n\nIndex data that does not match the mapping:\n\n```\ncurl -XPOST localhost:9200\/test_1\/bar?pretty -d'\n{\n    \"abc\": \"def\"\n}'\n```\n\nNow, an index with an additional type \"bar\" has been created from the template, although dynamic mapping is set to false.\n\n```\ncurl -XGET localhost:9200\/test_1?pretty\n{\n  \"test_1\" : {\n    \"aliases\" : { },\n    \"mappings\" : {\n      \"foo\" : {\n        \"properties\" : {\n          \"name\" : {\n            \"type\" : \"string\"\n          }\n        }\n      },\n      \"bar\" : {\n        \"properties\" : {\n          \"abc\" : {\n            \"type\" : \"string\"\n          }\n        }\n      }\n    },\n    \"settings\" : {\n      \"index\" : {\n        \"mapper\" : {\n          \"dynamic\" : \"false\"\n        },\n        \"creation_date\" : \"1460026755596\",\n        \"number_of_shards\" : \"1\",\n        \"number_of_replicas\" : \"1\",\n        \"uuid\" : \"EGWEI-iARECmY__EEa9jHg\",\n        \"version\" : {\n          \"created\" : \"2030199\"\n        }\n      }\n    },\n    \"warmers\" : { }\n  }\n}\n```\n","id":"146588636","title":"index.mapper.dynamic is not honored during auto-creation of an index from template","reopen_by":"clintongormley","opened_on":"2016-04-07T11:32:16Z","closed_by":"jpountz"},{"number":"17537","comments":[{"date":"2016-04-06T11:08:13Z","author":"clintongormley","text":"@jaksmid could you provide some documents and the stack trace that is produced when you see this exception please?\n"},{"date":"2016-04-06T11:09:22Z","author":"clintongormley","text":"@jpountz given that this only happens with `size` > 0, I'm wondering if this highlighting trying to highlight the geo field? Perhaps with no documents on a particular shard?\n\n\/cc @nknize \n"},{"date":"2016-04-07T07:17:50Z","author":"rmuir","text":"I can reproduce something that looks just like this with a lucene test if you apply the patch on https:\/\/issues.apache.org\/jira\/browse\/LUCENE-7185\n\nI suspect it may happen with extreme values such as latitude = 90 or longitude = 180 which are used much more in tests with the patch. See seed:\n\n```\n  [junit4] Suite: org.apache.lucene.spatial.geopoint.search.TestGeoPointQuery\n   [junit4] IGNOR\/A 0.01s J1 | TestGeoPointQuery.testRandomBig\n   [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())\n   [junit4] IGNOR\/A 0.00s J1 | TestGeoPointQuery.testRandomDistanceHuge\n   [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestGeoPointQuery -Dtests.method=testAllLonEqual -Dtests.seed=4ABB96AB44F4796E -Dtests.locale=id-ID -Dtests.timezone=Pacific\/Fakaofo -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n   [junit4] ERROR   0.35s J1 | TestGeoPointQuery.testAllLonEqual <<<\n   [junit4]    > Throwable #1: java.lang.IllegalArgumentException: Illegal shift value, must be 32..63; got shift=0\n   [junit4]    >    at __randomizedtesting.SeedInfo.seed([4ABB96AB44F4796E:DBB16756B45E397A]:0)\n   [junit4]    >    at org.apache.lucene.spatial.util.GeoEncodingUtils.geoCodedToPrefixCodedBytes(GeoEncodingUtils.java:109)\n   [junit4]    >    at org.apache.lucene.spatial.util.GeoEncodingUtils.geoCodedToPrefixCoded(GeoEncodingUtils.java:89)\n   [junit4]    >    at org.apache.lucene.spatial.geopoint.search.GeoPointPrefixTermsEnum$Range.fillBytesRef(GeoPointPrefixTermsEnum.java:236)\n   [junit4]    >    at org.apache.lucene.spatial.geopoint.search.GeoPointTermsEnum.nextRange(GeoPointTermsEnum.java:71)\n   [junit4]    >    at org.apache.lucene.spatial.geopoint.search.GeoPointPrefixTermsEnum.nextRange(GeoPointPrefixTermsEnum.java:171)\n   [junit4]    >    at org.apache.lucene.spatial.geopoint.search.GeoPointPrefixTermsEnum.nextSeekTerm(GeoPointPrefixTermsEnum.java:190)\n   [junit4]    >    at org.apache.lucene.index.FilteredTermsEnum.next(FilteredTermsEnum.java:212)\n   [junit4]    >    at org.apache.lucene.spatial.geopoint.search.GeoPointTermQueryConstantScoreWrapper$1.scorer(GeoPointTermQueryConstantScoreWrapper.java:110)\n   [junit4]    >    at org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)\n   [junit4]    >    at org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.bulkScorer(LRUQueryCache.java:644)\n   [junit4]    >    at org.apache.lucene.search.AssertingWeight.bulkScorer(AssertingWeight.java:68)\n   [junit4]    >    at org.apache.lucene.search.BooleanWeight.optionalBulkScorer(BooleanWeight.java:231)\n   [junit4]    >    at org.apache.lucene.search.BooleanWeight.booleanScorer(BooleanWeight.java:297)\n   [junit4]    >    at org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:364)\n   [junit4]    >    at org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.bulkScorer(LRUQueryCache.java:644)\n   [junit4]    >    at org.apache.lucene.search.AssertingWeight.bulkScorer(AssertingWeight.java:68)\n   [junit4]    >    at org.apache.lucene.search.AssertingWeight.bulkScorer(AssertingWeight.java:68)\n   [junit4]    >    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:666)\n   [junit4]    >    at org.apache.lucene.search.AssertingIndexSearcher.search(AssertingIndexSearcher.java:91)\n   [junit4]    >    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:473)\n   [junit4]    >    at org.apache.lucene.spatial.util.BaseGeoPointTestCase.verifyRandomRectangles(BaseGeoPointTestCase.java:835)\n   [junit4]    >    at org.apache.lucene.spatial.util.BaseGeoPointTestCase.verify(BaseGeoPointTestCase.java:763)\n   [junit4]    >    at org.apache.lucene.spatial.util.BaseGeoPointTestCase.testAllLonEqual(BaseGeoPointTestCase.java:495)\n\n```\n"},{"date":"2016-04-07T07:20:27Z","author":"jaksmid","text":"Hi @clintongormley, thank you for your message. \n\nThe stack trace is as follows:\n`RemoteTransportException[[elasticsearch_4][172.17.0.2:9300][indices:data\/read\/search[phase\/fetch\/id]]]; nested: FetchPhaseExecutionException[Fetch Failed [Failed to highlight field [cyberdyne_metadata.ner.mitie.model.DISEASE.tag]]]; nested: NumberFormatException[Invalid shift value (65) in prefixCoded bytes (is encoded value really a geo point?)];\nCaused by: FetchPhaseExecutionException[Fetch Failed [Failed to highlight field [cyberdyne_metadata.ner.mitie.model.DISEASE.tag]]]; nested: NumberFormatException[Invalid shift value (65) in prefixCoded bytes (is encoded value really a geo point?)];\n    at org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:123)\n    at org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:126)\n    at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:188)\n    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:592)\n    at org.elasticsearch.search.action.SearchServiceTransportAction$FetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:408)\n    at org.elasticsearch.search.action.SearchServiceTransportAction$FetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:405)\n    at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)\n    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)\n    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:300)\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.NumberFormatException: Invalid shift value (65) in prefixCoded bytes (is encoded value really a geo point?)\n    at org.apache.lucene.spatial.util.GeoEncodingUtils.getPrefixCodedShift(GeoEncodingUtils.java:134)\n    at org.apache.lucene.spatial.geopoint.search.GeoPointPrefixTermsEnum.accept(GeoPointPrefixTermsEnum.java:219)\n    at org.apache.lucene.index.FilteredTermsEnum.next(FilteredTermsEnum.java:232)\n    at org.apache.lucene.search.TermCollectingRewrite.collectTerms(TermCollectingRewrite.java:67)\n    at org.apache.lucene.search.ScoringRewrite.rewrite(ScoringRewrite.java:108)\n    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:220)\n    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:227)\n    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:113)\n    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:113)\n    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.getWeightedSpanTerms(WeightedSpanTermExtractor.java:505)\n    at org.apache.lucene.search.highlight.QueryScorer.initExtractor(QueryScorer.java:218)\n    at org.apache.lucene.search.highlight.QueryScorer.init(QueryScorer.java:186)\n    at org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:195)\n    at org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:108)\n    ... 12 more`\n\nThe field cyberdyne_metadata.ner.mitie.model.DISEASE.tag should not be a geopoint according to the dynamic template.\n"},{"date":"2016-04-07T07:33:06Z","author":"jpountz","text":"@rmuir oh, good catch\n@clintongormley The stack trace indeed suggests that the issue is with highlighting on the geo field. Regardless of this bug, I wonder that we should fail early when highlighting on anything but text fields and\/or exclude non-text fields from wildcard matching.\n"},{"date":"2016-04-07T08:40:43Z","author":"dadoonet","text":"> I wonder that we should fail early when highlighting on anything but text fields and\/or exclude non-text fields from wildcard matching.\n\n+1 to fail early if the user explicitly defined a non text field to highlight on and exclude non text fields when using wildcards\n"},{"date":"2016-04-17T08:09:07Z","author":"rodgermoore","text":"I was running into this bug during a live demo... Yes I know, I've should have tested all demo scenario's after updating ES :grimacing: . Anyway, +1 for fixing this!\n"},{"date":"2016-04-18T21:45:45Z","author":"fclotet","text":"-I´m having the same error. It's happends with doc having location and trying to use \n\"highlight\": {... \"require_field_match\": false ...}\n\nthanks!\n"},{"date":"2016-05-03T01:52:24Z","author":"dellis23","text":"I'm unclear as to what exactly is going on here, but I'm running into the same issue.  I'm attempting to do a geo bounding box in Kibana while viewing the results in the Discover tab.  Disabling highlighting in Kibana fixes the issue, but I would actually like to keep highlighting enabled, since it's super useful otherwise.\n\nIt sounds from what others are saying that this should fail when querying on _any_ non-string field, but I am not getting the same failure on numeric fields.  Is it just an issue with geoip fields?  I suppose another nice thing would be to explicitly allow for configuration of which fields should be highlighted in Kibana.\n"},{"date":"2016-05-03T10:40:19Z","author":"vingrad","text":"Please fix this issue.\n"},{"date":"2016-05-04T17:50:50Z","author":"brwe","text":"I wrote two tests so that everyone can reproduce what happens easily: https:\/\/github.com\/brwe\/elasticsearch\/commit\/ffa242941e4ede34df67301f7b9d46ea8719cc22\n\nIn brief:\nThe plain highlighter tries to highlight whatever the BBQuery provides as terms in the text \"60,120\" if that is how the `geo_point` was indexed (if the point was indexed with `{\"lat\": 60, \"lon\": 120}` nothing will happen because we cannot even extract anything from the source). The terms in the text are provided to Lucene as a token steam with a keyword analyzer.\nIn Lucene, this token stream is converted this via a longish call stack into a terms enum. But this terms enum is pulled from the query that contains the terms that are to be highlighted. In this case we call `GeoPointMultiTermQuery.getTermsEnum(terms)` which wraps the term in a `GeoPointTermsEnum`. This enum tries to convert a prefix coded geo term back to something else but because it is really just the string  \"60,120\" it throws the exception we see. \n\nI am unsure yet how a correct fix would look like but do wonder why we try highlingting on numeric and geo fields at all? If anyone has an opinion let me know.\n"},{"date":"2016-05-04T17:57:01Z","author":"brwe","text":"I missed @jpountz comment:\n\n> Regardless of this bug, I wonder that we should fail early when highlighting on anything but text fields and\/or exclude non-text fields from wildcard matching.\n\nI agree. Will make a pr for that.\n"},{"date":"2016-05-05T08:17:58Z","author":"clintongormley","text":"@brwe you did something similar before: https:\/\/github.com\/elastic\/elasticsearch\/pull\/11364 - i would have thought that that PR should have fixed this issue?\n"},{"date":"2016-05-05T09:15:10Z","author":"brwe","text":"@clintongormley Yes you are right. #11364 only addresses problems one gets when the way text is indexed is not compatible with the highlighter used. I do not remember why I did not exclude numeric fields then. \n"},{"date":"2016-05-07T13:15:25Z","author":"rodgermoore","text":"Great work. Tnx \n\n:sunglasses: \n"},{"date":"2016-05-19T07:09:10Z","author":"rodgermoore","text":"This is not fixed in 2.3.3 yet, correct?\n"},{"date":"2016-05-19T07:13:30Z","author":"jpountz","text":"@rodgermoore It should be fixed in 2.3.3, can you still reproduce the problem?\n"},{"date":"2016-05-19T11:44:32Z","author":"rodgermoore","text":"Ubuntu 14.04-04\nElasticsearch 2.3.3\nKibana 4.5.1\nJVM 1.8.0_66\n\nI am still able to reproduce this error in Kibana 4.5.1. I have a dashboard with a search panel with highlighting enabled. On the same Dashboard I have a tile map and after selecting an area in this map using the select function (draw a rectangle) I got the \"Invalid shift value (xx) in prefixCoded bytes (is encoded value really a geo point?)\" error.\n\nWhen I alter the json settings file of the search panel and remove highlighting the error does not pop-up.\n"},{"date":"2016-05-19T13:07:51Z","author":"brwe","text":"@rodgermoore I cannot reproduce this but I might do something different from you. Here is my dashboard:\n\n![image](https:\/\/cloud.githubusercontent.com\/assets\/4320215\/15393472\/bd2b4cf2-1dcd-11e6-8ac1-cf6ba5e995b7.png)\n\nIs that what you did?\nCan you attach the whole stacktrace from the elasticsearch logs again? If you did not change the logging config the full search request should be in there. Also, if you can please add an example document.\n"},{"date":"2016-05-19T13:12:50Z","author":"rodgermoore","text":"I see you used \"text:blah\". I did not enter a search at all (so used the default wildcard) and then did the aggregation on the tile map. This resulted in the error. \n"},{"date":"2016-05-19T13:16:46Z","author":"brwe","text":"I can remove the query and still get a result. Can you please attach the relevant part of the elasticsearch log? \n"},{"date":"2016-05-19T13:37:15Z","author":"rodgermoore","text":"Here you go:\n\n```\n[2016-05-19 15:23:08,270][DEBUG][action.search            ] [Black King] All shards failed for phase: [query_fetch]\nRemoteTransportException[[Black King][192.168.48.18:9300][indices:data\/read\/search[phase\/query+fetch]]]; nested: FetchPhaseExecutionException[Fetch Failed [Failed to highlight field [tags.nl]]]; nested: NumberFormatException[Invalid shift value (115) in prefixCoded bytes (is encoded value really a geo point?)];\nCaused by: FetchPhaseExecutionException[Fetch Failed [Failed to highlight field [tags.nl]]]; nested: NumberFormatException[Invalid shift value (115) in prefixCoded bytes (is encoded value really a geo point?)];\n    at org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:123)\n    at org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:140)\n    at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:188)\n    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:480)\n    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:392)\n    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:389)\n    at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)\n    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)\n    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.NumberFormatException: Invalid shift value (115) in prefixCoded bytes (is encoded value really a geo point?)\n    at org.apache.lucene.spatial.util.GeoEncodingUtils.getPrefixCodedShift(GeoEncodingUtils.java:134)\n    at org.apache.lucene.spatial.geopoint.search.GeoPointPrefixTermsEnum.accept(GeoPointPrefixTermsEnum.java:219)\n    at org.apache.lucene.index.FilteredTermsEnum.next(FilteredTermsEnum.java:232)\n    at org.apache.lucene.search.TermCollectingRewrite.collectTerms(TermCollectingRewrite.java:67)\n    at org.apache.lucene.search.ScoringRewrite.rewrite(ScoringRewrite.java:108)\n    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:220)\n    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:227)\n    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:113)\n    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:113)\n    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.getWeightedSpanTerms(WeightedSpanTermExtractor.java:505)\n    at org.apache.lucene.search.highlight.QueryScorer.initExtractor(QueryScorer.java:218)\n    at org.apache.lucene.search.highlight.QueryScorer.init(QueryScorer.java:186)\n    at org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:195)\n    at org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:108)\n    ... 12 more\n```\n\nWe are using dynamic mapping and we dynamically analyse all string fields using the Dutch language analyzer. All string fields get a non analyzed field: \"field.raw\" and a Dutch analyzed field \"field.nl\". \n"},{"date":"2016-05-19T13:46:59Z","author":"brwe","text":"Ah...I was hoping to get the actual request but it is not in the stacktrace after all. Can you also add the individual requests from the panels in your dashboard (in the spy tab) and a screenshot so I can see what the geo bounding box filter filters on? I could then try to reconstruct the request.\n\nAlso, are you sure you upgraded all nodes in the cluster? Check with `curl -XGET \"http:\/\/hostname:port\/_nodes\"`. Would be great if you could add the output of that here too just to be sure. \n"},{"date":"2016-05-19T14:17:36Z","author":"lemig","text":"I have got the exact same issue. I am running 2.3.3. All my nodes (1) are upgraded.\n"},{"date":"2016-05-19T14:42:02Z","author":"lemig","text":"<img width=\"1676\" alt=\"screen shot 2016-05-19 at 16 29 15\" src=\"https:\/\/cloud.githubusercontent.com\/assets\/78766\/15397413\/7858191c-1de0-11e6-802b-773f4a7ecf79.png\">\n"},{"date":"2016-05-19T14:42:12Z","author":"rodgermoore","text":"Here you go.\n\nTile Map Query:\n\n```\n{\n  \"query\": {\n    \"filtered\": {\n      \"query\": {\n        \"query_string\": {\n          \"analyze_wildcard\": true,\n          \"query\": \"*\"\n        }\n      },\n      \"filter\": {\n        \"bool\": {\n          \"must\": [\n            {\n              \"geo_bounding_box\": {\n                \"SomeGeoField\": {\n                  \"top_left\": {\n                    \"lat\": REMOVED,\n                    \"lon\": REMOVED\n                  },\n                  \"bottom_right\": {\n                    \"lat\": REMOVED,\n                    \"lon\": REMOVED\n                  }\n                }\n              },\n              \"$state\": {\n                \"store\": \"appState\"\n              }\n            },\n            {\n              \"query\": {\n                \"query_string\": {\n                  \"query\": \"*\",\n                  \"analyze_wildcard\": true\n                }\n              }\n            },\n            {\n              \"range\": {\n                \"@timestamp\": {\n                  \"gte\": 1458485686484,\n                  \"lte\": 1463666086484,\n                  \"format\": \"epoch_millis\"\n                }\n              }\n            }\n          ],\n          \"must_not\": []\n        }\n      }\n    }\n  },\n  \"size\": 0,\n  \"aggs\": {\n    \"2\": {\n      \"geohash_grid\": {\n        \"field\": \"SomeGeoField\",\n        \"precision\": 5\n      }\n    }\n  }\n}\n```\n\nI'm using a single node cluster, here's the info:\n\n```\n{\n  \"cluster_name\": \"elasticsearch\",\n  \"nodes\": {\n    \"RtBthRfeSOSud1XfRRAkSA\": {\n      \"name\": \"Black King\",\n      \"transport_address\": \"192.168.48.18:9300\",\n      \"host\": \"192.168.48.18\",\n      \"ip\": \"192.168.48.18\",\n      \"version\": \"2.3.3\",\n      \"build\": \"218bdf1\",\n      \"http_address\": \"192.168.48.18:9200\",\n      \"settings\": {\n        \"pidfile\": \"\/var\/run\/elasticsearch\/elasticsearch.pid\",\n        \"cluster\": {\n          \"name\": \"elasticsearch\"\n        },\n        \"path\": {\n          \"conf\": \"\/etc\/elasticsearch\",\n          \"data\": \"\/var\/lib\/elasticsearch\",\n          \"logs\": \"\/var\/log\/elasticsearch\",\n          \"home\": \"\/usr\/share\/elasticsearch\",\n          \"repo\": [\n            \"\/home\/somename\/es_backups\"\n          ]\n        },\n        \"name\": \"Black King\",\n        \"client\": {\n          \"type\": \"node\"\n        },\n        \"foreground\": \"false\",\n        \"config\": {\n          \"ignore_system_properties\": \"true\"\n        },\n        \"network\": {\n          \"host\": \"0.0.0.0\"\n        }\n      },\n      \"os\": {\n        \"refresh_interval_in_millis\": 1000,\n        \"name\": \"Linux\",\n        \"arch\": \"amd64\",\n        \"version\": \"3.19.0-59-generic\",\n        \"available_processors\": 8,\n        \"allocated_processors\": 8\n      },\n      \"process\": {\n        \"refresh_interval_in_millis\": 1000,\n        \"id\": 1685,\n        \"mlockall\": false\n      },\n      \"jvm\": {\n        \"pid\": 1685,\n        \"version\": \"1.8.0_66\",\n        \"vm_name\": \"Java HotSpot(TM) 64-Bit Server VM\",\n        \"vm_version\": \"25.66-b17\",\n        \"vm_vendor\": \"Oracle Corporation\",\n        \"start_time_in_millis\": 1463663018422,\n        \"mem\": {\n          \"heap_init_in_bytes\": 6442450944,\n          \"heap_max_in_bytes\": 6372720640,\n          \"non_heap_init_in_bytes\": 2555904,\n          \"non_heap_max_in_bytes\": 0,\n          \"direct_max_in_bytes\": 6372720640\n        },\n        \"gc_collectors\": [\n          \"ParNew\",\n          \"ConcurrentMarkSweep\"\n        ],\n        \"memory_pools\": [\n          \"Code Cache\",\n          \"Metaspace\",\n          \"Compressed Class Space\",\n          \"Par Eden Space\",\n          \"Par Survivor Space\",\n          \"CMS Old Gen\"\n        ],\n        \"using_compressed_ordinary_object_pointers\": \"true\"\n      },\n      \"thread_pool\": {\n        \"force_merge\": {\n          \"type\": \"fixed\",\n          \"min\": 1,\n          \"max\": 1,\n          \"queue_size\": -1\n        },\n        \"percolate\": {\n          \"type\": \"fixed\",\n          \"min\": 8,\n          \"max\": 8,\n          \"queue_size\": 1000\n        },\n        \"fetch_shard_started\": {\n          \"type\": \"scaling\",\n          \"min\": 1,\n          \"max\": 16,\n          \"keep_alive\": \"5m\",\n          \"queue_size\": -1\n        },\n        \"listener\": {\n          \"type\": \"fixed\",\n          \"min\": 4,\n          \"max\": 4,\n          \"queue_size\": -1\n        },\n        \"index\": {\n          \"type\": \"fixed\",\n          \"min\": 8,\n          \"max\": 8,\n          \"queue_size\": 200\n        },\n        \"refresh\": {\n          \"type\": \"scaling\",\n          \"min\": 1,\n          \"max\": 4,\n          \"keep_alive\": \"5m\",\n          \"queue_size\": -1\n        },\n        \"suggest\": {\n          \"type\": \"fixed\",\n          \"min\": 8,\n          \"max\": 8,\n          \"queue_size\": 1000\n        },\n        \"generic\": {\n          \"type\": \"cached\",\n          \"keep_alive\": \"30s\",\n          \"queue_size\": -1\n        },\n        \"warmer\": {\n          \"type\": \"scaling\",\n          \"min\": 1,\n          \"max\": 4,\n          \"keep_alive\": \"5m\",\n          \"queue_size\": -1\n        },\n        \"search\": {\n          \"type\": \"fixed\",\n          \"min\": 13,\n          \"max\": 13,\n          \"queue_size\": 1000\n        },\n        \"flush\": {\n          \"type\": \"scaling\",\n          \"min\": 1,\n          \"max\": 4,\n          \"keep_alive\": \"5m\",\n          \"queue_size\": -1\n        },\n        \"fetch_shard_store\": {\n          \"type\": \"scaling\",\n          \"min\": 1,\n          \"max\": 16,\n          \"keep_alive\": \"5m\",\n          \"queue_size\": -1\n        },\n        \"management\": {\n          \"type\": \"scaling\",\n          \"min\": 1,\n          \"max\": 5,\n          \"keep_alive\": \"5m\",\n          \"queue_size\": -1\n        },\n        \"get\": {\n          \"type\": \"fixed\",\n          \"min\": 8,\n          \"max\": 8,\n          \"queue_size\": 1000\n        },\n        \"bulk\": {\n          \"type\": \"fixed\",\n          \"min\": 8,\n          \"max\": 8,\n          \"queue_size\": 50\n        },\n        \"snapshot\": {\n          \"type\": \"scaling\",\n          \"min\": 1,\n          \"max\": 4,\n          \"keep_alive\": \"5m\",\n          \"queue_size\": -1\n        }\n      },\n      \"transport\": {\n        \"bound_address\": [\n          \"[::]:9300\"\n        ],\n        \"publish_address\": \"192.168.48.18:9300\",\n        \"profiles\": {}\n      },\n      \"http\": {\n        \"bound_address\": [\n          \"[::]:9200\"\n        ],\n        \"publish_address\": \"192.168.48.18:9200\",\n        \"max_content_length_in_bytes\": 104857600\n      },\n      \"plugins\": [],\n      \"modules\": [\n        {\n          \"name\": \"lang-expression\",\n          \"version\": \"2.3.3\",\n          \"description\": \"Lucene expressions integration for Elasticsearch\",\n          \"jvm\": true,\n          \"classname\": \"org.elasticsearch.script.expression.ExpressionPlugin\",\n          \"isolated\": true,\n          \"site\": false\n        },\n        {\n          \"name\": \"lang-groovy\",\n          \"version\": \"2.3.3\",\n          \"description\": \"Groovy scripting integration for Elasticsearch\",\n          \"jvm\": true,\n          \"classname\": \"org.elasticsearch.script.groovy.GroovyPlugin\",\n          \"isolated\": true,\n          \"site\": false\n        },\n        {\n          \"name\": \"reindex\",\n          \"version\": \"2.3.3\",\n          \"description\": \"_reindex and _update_by_query APIs\",\n          \"jvm\": true,\n          \"classname\": \"org.elasticsearch.index.reindex.ReindexPlugin\",\n          \"isolated\": true,\n          \"site\": false\n        }\n      ]\n    }\n  }\n}\n```\n\nScreenshot, I had to clear out the data:\n\n![error_es](https:\/\/cloud.githubusercontent.com\/assets\/12231719\/15397399\/668a374c-1de0-11e6-903d-f929a2d9f0b2.PNG)\n"},{"date":"2016-05-19T14:45:25Z","author":"clintongormley","text":"@rodgermoore does the query you provided work correctly? You said that it started working once you deleted the highlighting and this query doesn't contain highlighting.  Could you provide the query that doesn't work?\n"},{"date":"2016-05-19T14:51:27Z","author":"rodgermoore","text":"It does has highlighting enabled. This is the json for the search panel: \n\n```\n{\n  \"index\": \"someindex\",\n  \"query\": {\n    \"query_string\": {\n      \"query\": \"*\",\n      \"analyze_wildcard\": true\n    }\n  },\n  \"filter\": [],\n  \"highlight\": {\n    \"pre_tags\": [\n      \"@kibana-highlighted-field@\"\n    ],\n    \"post_tags\": [\n      \"@\/kibana-highlighted-field@\"\n    ],\n    \"fields\": {\n      \"*\": {}\n    },\n    \"require_field_match\": false,\n    \"fragment_size\": 2147483647\n  }\n}\n```\n\nI can't show the actual data so I selected to show only the timestamp field in the search panel in the screenshot...\n\nWhen I change the json of the search panel to:\n\n```\n{\n  \"index\": \"someindex\",\n  \"filter\": [],\n  \"query\": {\n    \"query_string\": {\n      \"query\": \"*\",\n      \"analyze_wildcard\": true\n    }\n  }\n}\n```\n\nThe error disappears.\n"},{"date":"2016-05-19T14:54:44Z","author":"dellis23","text":"If my understanding of the patch is correct, it shouldn't matter whether Kibana is including the highlighting field.  Elasticsearch should only be trying to highlight string fields, even if a wildcard is being used.\n"},{"date":"2016-05-19T16:23:44Z","author":"brwe","text":"Ok, I managed to reproduce it on 2.3.3. It happens with `\"geohash\": true` in the mapping. \n\nSteps are:\n\n```\nDELETE test\nPUT test \n{\n  \"mappings\": {\n    \"doc\": {\n      \"properties\": {\n        \"point\": {\n          \"type\": \"geo_point\",\n          \"geohash\": true\n        }\n      }\n    }\n  }\n}\n\nPUT test\/doc\/1\n{\n  \"point\": \"60.12,100.34\"\n}\n\nPOST test\/_search\n{\n  \"query\": {\n    \"geo_bounding_box\": {\n      \"point\": {\n        \"top_left\": {\n          \"lat\": 61.10078883158897,\n          \"lon\": -170.15625\n        },\n        \"bottom_right\": {\n          \"lat\": -64.92354174306496,\n          \"lon\": 118.47656249999999\n        }\n      }\n    }\n  },\n  \"highlight\": {\n    \"fields\": {\n      \"*\": {}\n    }\n  }\n}\n```\n\nSorry, I did not think of that. I work on another fix.\n"}],"reopen_on":"2016-05-19T16:23:44Z","opened_by":"jaksmid","closed_on":"2016-09-19T10:27:35Z","description":"Hi guys,\n\nwe have upgraded ElasticSearch from 2.3.0 and reindexed our geolocations so the latitude and longitude are stored separately. We have noticed that some of our visualisation started to fail after we add a filter based on geolocation rectangle. However, map visualisation are working just fine. The problem occurs when we include actual documents. In this case, we get some failed shards (usually 1 out of 5) and error: Invalid shift value (xx) in prefixCoded bytes (is encoded value really a geo point?).\n\nDetails:\nOur geolocation index is based on:\n\n```\n\"dynamic_templates\": [{\n....\n{\n        \"ner_geo\": {\n          \"mapping\": {\n            \"type\": \"geo_point\",\n            \"lat_lon\": true\n          },\n          \"path_match\": \"*.coordinates\"\n        }\n      }],\n```\n\nThe ok query with the error is as follows. If we change the query size to 0 (map visualizations example), the query completes without problem.\n\n```\n{\n  \"size\": 100,\n  \"aggs\": {\n    \"2\": {\n      \"geohash_grid\": {\n        \"field\": \"authors.affiliation.coordinates\",\n        \"precision\": 2\n      }\n    }\n  },\n  \"query\": {\n    \"filtered\": {\n      \"query\": {\n        \"query_string\": {\n          \"analyze_wildcard\": true,\n          \"query\": \"*\"\n        }\n      },\n      \"filter\": {\n        \"bool\": {\n          \"must\": [\n            {\n              \"geo_bounding_box\": {\n                \"authors.affiliation.coordinates\": {\n                  \"top_left\": {\n                    \"lat\": 61.10078883158897,\n                    \"lon\": -170.15625\n                  },\n                  \"bottom_right\": {\n                    \"lat\": -64.92354174306496,\n                    \"lon\": 118.47656249999999\n                  }\n                }\n              }\n            }\n          ],\n          \"must_not\": []\n        }\n      }\n    }\n  },\n  \"highlight\": {\n    \"pre_tags\": [\n      \"@kibana-highlighted-field@\"\n    ],\n    \"post_tags\": [\n      \"@\/kibana-highlighted-field@\"\n    ],\n    \"fields\": {\n      \"*\": {}\n    },\n    \"require_field_match\": false,\n    \"fragment_size\": 2147483647\n  }\n}\n```\n\nElasticsearch version**: 2.3.0\nOS version**: Elasticsearch docker image with head plugin, marvel and big desk installed\n\nThank you for your help,\nregards,\nJakub Smid\n","id":"145978723","title":"Invalid shift value (xx) in prefixCoded bytes (is encoded value really a geo point?)","reopen_by":"brwe","opened_on":"2016-04-05T12:52:21Z","closed_by":"brwe"},{"number":"17537","comments":[{"date":"2016-04-06T11:08:13Z","author":"clintongormley","text":"@jaksmid could you provide some documents and the stack trace that is produced when you see this exception please?\n"},{"date":"2016-04-06T11:09:22Z","author":"clintongormley","text":"@jpountz given that this only happens with `size` > 0, I'm wondering if this highlighting trying to highlight the geo field? Perhaps with no documents on a particular shard?\n\n\/cc @nknize \n"},{"date":"2016-04-07T07:17:50Z","author":"rmuir","text":"I can reproduce something that looks just like this with a lucene test if you apply the patch on https:\/\/issues.apache.org\/jira\/browse\/LUCENE-7185\n\nI suspect it may happen with extreme values such as latitude = 90 or longitude = 180 which are used much more in tests with the patch. See seed:\n\n```\n  [junit4] Suite: org.apache.lucene.spatial.geopoint.search.TestGeoPointQuery\n   [junit4] IGNOR\/A 0.01s J1 | TestGeoPointQuery.testRandomBig\n   [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())\n   [junit4] IGNOR\/A 0.00s J1 | TestGeoPointQuery.testRandomDistanceHuge\n   [junit4]    > Assumption #1: 'nightly' test group is disabled (@Nightly())\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestGeoPointQuery -Dtests.method=testAllLonEqual -Dtests.seed=4ABB96AB44F4796E -Dtests.locale=id-ID -Dtests.timezone=Pacific\/Fakaofo -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n   [junit4] ERROR   0.35s J1 | TestGeoPointQuery.testAllLonEqual <<<\n   [junit4]    > Throwable #1: java.lang.IllegalArgumentException: Illegal shift value, must be 32..63; got shift=0\n   [junit4]    >    at __randomizedtesting.SeedInfo.seed([4ABB96AB44F4796E:DBB16756B45E397A]:0)\n   [junit4]    >    at org.apache.lucene.spatial.util.GeoEncodingUtils.geoCodedToPrefixCodedBytes(GeoEncodingUtils.java:109)\n   [junit4]    >    at org.apache.lucene.spatial.util.GeoEncodingUtils.geoCodedToPrefixCoded(GeoEncodingUtils.java:89)\n   [junit4]    >    at org.apache.lucene.spatial.geopoint.search.GeoPointPrefixTermsEnum$Range.fillBytesRef(GeoPointPrefixTermsEnum.java:236)\n   [junit4]    >    at org.apache.lucene.spatial.geopoint.search.GeoPointTermsEnum.nextRange(GeoPointTermsEnum.java:71)\n   [junit4]    >    at org.apache.lucene.spatial.geopoint.search.GeoPointPrefixTermsEnum.nextRange(GeoPointPrefixTermsEnum.java:171)\n   [junit4]    >    at org.apache.lucene.spatial.geopoint.search.GeoPointPrefixTermsEnum.nextSeekTerm(GeoPointPrefixTermsEnum.java:190)\n   [junit4]    >    at org.apache.lucene.index.FilteredTermsEnum.next(FilteredTermsEnum.java:212)\n   [junit4]    >    at org.apache.lucene.spatial.geopoint.search.GeoPointTermQueryConstantScoreWrapper$1.scorer(GeoPointTermQueryConstantScoreWrapper.java:110)\n   [junit4]    >    at org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)\n   [junit4]    >    at org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.bulkScorer(LRUQueryCache.java:644)\n   [junit4]    >    at org.apache.lucene.search.AssertingWeight.bulkScorer(AssertingWeight.java:68)\n   [junit4]    >    at org.apache.lucene.search.BooleanWeight.optionalBulkScorer(BooleanWeight.java:231)\n   [junit4]    >    at org.apache.lucene.search.BooleanWeight.booleanScorer(BooleanWeight.java:297)\n   [junit4]    >    at org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:364)\n   [junit4]    >    at org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.bulkScorer(LRUQueryCache.java:644)\n   [junit4]    >    at org.apache.lucene.search.AssertingWeight.bulkScorer(AssertingWeight.java:68)\n   [junit4]    >    at org.apache.lucene.search.AssertingWeight.bulkScorer(AssertingWeight.java:68)\n   [junit4]    >    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:666)\n   [junit4]    >    at org.apache.lucene.search.AssertingIndexSearcher.search(AssertingIndexSearcher.java:91)\n   [junit4]    >    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:473)\n   [junit4]    >    at org.apache.lucene.spatial.util.BaseGeoPointTestCase.verifyRandomRectangles(BaseGeoPointTestCase.java:835)\n   [junit4]    >    at org.apache.lucene.spatial.util.BaseGeoPointTestCase.verify(BaseGeoPointTestCase.java:763)\n   [junit4]    >    at org.apache.lucene.spatial.util.BaseGeoPointTestCase.testAllLonEqual(BaseGeoPointTestCase.java:495)\n\n```\n"},{"date":"2016-04-07T07:20:27Z","author":"jaksmid","text":"Hi @clintongormley, thank you for your message. \n\nThe stack trace is as follows:\n`RemoteTransportException[[elasticsearch_4][172.17.0.2:9300][indices:data\/read\/search[phase\/fetch\/id]]]; nested: FetchPhaseExecutionException[Fetch Failed [Failed to highlight field [cyberdyne_metadata.ner.mitie.model.DISEASE.tag]]]; nested: NumberFormatException[Invalid shift value (65) in prefixCoded bytes (is encoded value really a geo point?)];\nCaused by: FetchPhaseExecutionException[Fetch Failed [Failed to highlight field [cyberdyne_metadata.ner.mitie.model.DISEASE.tag]]]; nested: NumberFormatException[Invalid shift value (65) in prefixCoded bytes (is encoded value really a geo point?)];\n    at org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:123)\n    at org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:126)\n    at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:188)\n    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:592)\n    at org.elasticsearch.search.action.SearchServiceTransportAction$FetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:408)\n    at org.elasticsearch.search.action.SearchServiceTransportAction$FetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:405)\n    at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)\n    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)\n    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:300)\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.NumberFormatException: Invalid shift value (65) in prefixCoded bytes (is encoded value really a geo point?)\n    at org.apache.lucene.spatial.util.GeoEncodingUtils.getPrefixCodedShift(GeoEncodingUtils.java:134)\n    at org.apache.lucene.spatial.geopoint.search.GeoPointPrefixTermsEnum.accept(GeoPointPrefixTermsEnum.java:219)\n    at org.apache.lucene.index.FilteredTermsEnum.next(FilteredTermsEnum.java:232)\n    at org.apache.lucene.search.TermCollectingRewrite.collectTerms(TermCollectingRewrite.java:67)\n    at org.apache.lucene.search.ScoringRewrite.rewrite(ScoringRewrite.java:108)\n    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:220)\n    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:227)\n    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:113)\n    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:113)\n    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.getWeightedSpanTerms(WeightedSpanTermExtractor.java:505)\n    at org.apache.lucene.search.highlight.QueryScorer.initExtractor(QueryScorer.java:218)\n    at org.apache.lucene.search.highlight.QueryScorer.init(QueryScorer.java:186)\n    at org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:195)\n    at org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:108)\n    ... 12 more`\n\nThe field cyberdyne_metadata.ner.mitie.model.DISEASE.tag should not be a geopoint according to the dynamic template.\n"},{"date":"2016-04-07T07:33:06Z","author":"jpountz","text":"@rmuir oh, good catch\n@clintongormley The stack trace indeed suggests that the issue is with highlighting on the geo field. Regardless of this bug, I wonder that we should fail early when highlighting on anything but text fields and\/or exclude non-text fields from wildcard matching.\n"},{"date":"2016-04-07T08:40:43Z","author":"dadoonet","text":"> I wonder that we should fail early when highlighting on anything but text fields and\/or exclude non-text fields from wildcard matching.\n\n+1 to fail early if the user explicitly defined a non text field to highlight on and exclude non text fields when using wildcards\n"},{"date":"2016-04-17T08:09:07Z","author":"rodgermoore","text":"I was running into this bug during a live demo... Yes I know, I've should have tested all demo scenario's after updating ES :grimacing: . Anyway, +1 for fixing this!\n"},{"date":"2016-04-18T21:45:45Z","author":"fclotet","text":"-I´m having the same error. It's happends with doc having location and trying to use \n\"highlight\": {... \"require_field_match\": false ...}\n\nthanks!\n"},{"date":"2016-05-03T01:52:24Z","author":"dellis23","text":"I'm unclear as to what exactly is going on here, but I'm running into the same issue.  I'm attempting to do a geo bounding box in Kibana while viewing the results in the Discover tab.  Disabling highlighting in Kibana fixes the issue, but I would actually like to keep highlighting enabled, since it's super useful otherwise.\n\nIt sounds from what others are saying that this should fail when querying on _any_ non-string field, but I am not getting the same failure on numeric fields.  Is it just an issue with geoip fields?  I suppose another nice thing would be to explicitly allow for configuration of which fields should be highlighted in Kibana.\n"},{"date":"2016-05-03T10:40:19Z","author":"vingrad","text":"Please fix this issue.\n"},{"date":"2016-05-04T17:50:50Z","author":"brwe","text":"I wrote two tests so that everyone can reproduce what happens easily: https:\/\/github.com\/brwe\/elasticsearch\/commit\/ffa242941e4ede34df67301f7b9d46ea8719cc22\n\nIn brief:\nThe plain highlighter tries to highlight whatever the BBQuery provides as terms in the text \"60,120\" if that is how the `geo_point` was indexed (if the point was indexed with `{\"lat\": 60, \"lon\": 120}` nothing will happen because we cannot even extract anything from the source). The terms in the text are provided to Lucene as a token steam with a keyword analyzer.\nIn Lucene, this token stream is converted this via a longish call stack into a terms enum. But this terms enum is pulled from the query that contains the terms that are to be highlighted. In this case we call `GeoPointMultiTermQuery.getTermsEnum(terms)` which wraps the term in a `GeoPointTermsEnum`. This enum tries to convert a prefix coded geo term back to something else but because it is really just the string  \"60,120\" it throws the exception we see. \n\nI am unsure yet how a correct fix would look like but do wonder why we try highlingting on numeric and geo fields at all? If anyone has an opinion let me know.\n"},{"date":"2016-05-04T17:57:01Z","author":"brwe","text":"I missed @jpountz comment:\n\n> Regardless of this bug, I wonder that we should fail early when highlighting on anything but text fields and\/or exclude non-text fields from wildcard matching.\n\nI agree. Will make a pr for that.\n"},{"date":"2016-05-05T08:17:58Z","author":"clintongormley","text":"@brwe you did something similar before: https:\/\/github.com\/elastic\/elasticsearch\/pull\/11364 - i would have thought that that PR should have fixed this issue?\n"},{"date":"2016-05-05T09:15:10Z","author":"brwe","text":"@clintongormley Yes you are right. #11364 only addresses problems one gets when the way text is indexed is not compatible with the highlighter used. I do not remember why I did not exclude numeric fields then. \n"},{"date":"2016-05-07T13:15:25Z","author":"rodgermoore","text":"Great work. Tnx \n\n:sunglasses: \n"},{"date":"2016-05-19T07:09:10Z","author":"rodgermoore","text":"This is not fixed in 2.3.3 yet, correct?\n"},{"date":"2016-05-19T07:13:30Z","author":"jpountz","text":"@rodgermoore It should be fixed in 2.3.3, can you still reproduce the problem?\n"},{"date":"2016-05-19T11:44:32Z","author":"rodgermoore","text":"Ubuntu 14.04-04\nElasticsearch 2.3.3\nKibana 4.5.1\nJVM 1.8.0_66\n\nI am still able to reproduce this error in Kibana 4.5.1. I have a dashboard with a search panel with highlighting enabled. On the same Dashboard I have a tile map and after selecting an area in this map using the select function (draw a rectangle) I got the \"Invalid shift value (xx) in prefixCoded bytes (is encoded value really a geo point?)\" error.\n\nWhen I alter the json settings file of the search panel and remove highlighting the error does not pop-up.\n"},{"date":"2016-05-19T13:07:51Z","author":"brwe","text":"@rodgermoore I cannot reproduce this but I might do something different from you. Here is my dashboard:\n\n![image](https:\/\/cloud.githubusercontent.com\/assets\/4320215\/15393472\/bd2b4cf2-1dcd-11e6-8ac1-cf6ba5e995b7.png)\n\nIs that what you did?\nCan you attach the whole stacktrace from the elasticsearch logs again? If you did not change the logging config the full search request should be in there. Also, if you can please add an example document.\n"},{"date":"2016-05-19T13:12:50Z","author":"rodgermoore","text":"I see you used \"text:blah\". I did not enter a search at all (so used the default wildcard) and then did the aggregation on the tile map. This resulted in the error. \n"},{"date":"2016-05-19T13:16:46Z","author":"brwe","text":"I can remove the query and still get a result. Can you please attach the relevant part of the elasticsearch log? \n"},{"date":"2016-05-19T13:37:15Z","author":"rodgermoore","text":"Here you go:\n\n```\n[2016-05-19 15:23:08,270][DEBUG][action.search            ] [Black King] All shards failed for phase: [query_fetch]\nRemoteTransportException[[Black King][192.168.48.18:9300][indices:data\/read\/search[phase\/query+fetch]]]; nested: FetchPhaseExecutionException[Fetch Failed [Failed to highlight field [tags.nl]]]; nested: NumberFormatException[Invalid shift value (115) in prefixCoded bytes (is encoded value really a geo point?)];\nCaused by: FetchPhaseExecutionException[Fetch Failed [Failed to highlight field [tags.nl]]]; nested: NumberFormatException[Invalid shift value (115) in prefixCoded bytes (is encoded value really a geo point?)];\n    at org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:123)\n    at org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:140)\n    at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:188)\n    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:480)\n    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:392)\n    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:389)\n    at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)\n    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)\n    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.NumberFormatException: Invalid shift value (115) in prefixCoded bytes (is encoded value really a geo point?)\n    at org.apache.lucene.spatial.util.GeoEncodingUtils.getPrefixCodedShift(GeoEncodingUtils.java:134)\n    at org.apache.lucene.spatial.geopoint.search.GeoPointPrefixTermsEnum.accept(GeoPointPrefixTermsEnum.java:219)\n    at org.apache.lucene.index.FilteredTermsEnum.next(FilteredTermsEnum.java:232)\n    at org.apache.lucene.search.TermCollectingRewrite.collectTerms(TermCollectingRewrite.java:67)\n    at org.apache.lucene.search.ScoringRewrite.rewrite(ScoringRewrite.java:108)\n    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:220)\n    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:227)\n    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:113)\n    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:113)\n    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.getWeightedSpanTerms(WeightedSpanTermExtractor.java:505)\n    at org.apache.lucene.search.highlight.QueryScorer.initExtractor(QueryScorer.java:218)\n    at org.apache.lucene.search.highlight.QueryScorer.init(QueryScorer.java:186)\n    at org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:195)\n    at org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:108)\n    ... 12 more\n```\n\nWe are using dynamic mapping and we dynamically analyse all string fields using the Dutch language analyzer. All string fields get a non analyzed field: \"field.raw\" and a Dutch analyzed field \"field.nl\". \n"},{"date":"2016-05-19T13:46:59Z","author":"brwe","text":"Ah...I was hoping to get the actual request but it is not in the stacktrace after all. Can you also add the individual requests from the panels in your dashboard (in the spy tab) and a screenshot so I can see what the geo bounding box filter filters on? I could then try to reconstruct the request.\n\nAlso, are you sure you upgraded all nodes in the cluster? Check with `curl -XGET \"http:\/\/hostname:port\/_nodes\"`. Would be great if you could add the output of that here too just to be sure. \n"},{"date":"2016-05-19T14:17:36Z","author":"lemig","text":"I have got the exact same issue. I am running 2.3.3. All my nodes (1) are upgraded.\n"},{"date":"2016-05-19T14:42:02Z","author":"lemig","text":"<img width=\"1676\" alt=\"screen shot 2016-05-19 at 16 29 15\" src=\"https:\/\/cloud.githubusercontent.com\/assets\/78766\/15397413\/7858191c-1de0-11e6-802b-773f4a7ecf79.png\">\n"},{"date":"2016-05-19T14:42:12Z","author":"rodgermoore","text":"Here you go.\n\nTile Map Query:\n\n```\n{\n  \"query\": {\n    \"filtered\": {\n      \"query\": {\n        \"query_string\": {\n          \"analyze_wildcard\": true,\n          \"query\": \"*\"\n        }\n      },\n      \"filter\": {\n        \"bool\": {\n          \"must\": [\n            {\n              \"geo_bounding_box\": {\n                \"SomeGeoField\": {\n                  \"top_left\": {\n                    \"lat\": REMOVED,\n                    \"lon\": REMOVED\n                  },\n                  \"bottom_right\": {\n                    \"lat\": REMOVED,\n                    \"lon\": REMOVED\n                  }\n                }\n              },\n              \"$state\": {\n                \"store\": \"appState\"\n              }\n            },\n            {\n              \"query\": {\n                \"query_string\": {\n                  \"query\": \"*\",\n                  \"analyze_wildcard\": true\n                }\n              }\n            },\n            {\n              \"range\": {\n                \"@timestamp\": {\n                  \"gte\": 1458485686484,\n                  \"lte\": 1463666086484,\n                  \"format\": \"epoch_millis\"\n                }\n              }\n            }\n          ],\n          \"must_not\": []\n        }\n      }\n    }\n  },\n  \"size\": 0,\n  \"aggs\": {\n    \"2\": {\n      \"geohash_grid\": {\n        \"field\": \"SomeGeoField\",\n        \"precision\": 5\n      }\n    }\n  }\n}\n```\n\nI'm using a single node cluster, here's the info:\n\n```\n{\n  \"cluster_name\": \"elasticsearch\",\n  \"nodes\": {\n    \"RtBthRfeSOSud1XfRRAkSA\": {\n      \"name\": \"Black King\",\n      \"transport_address\": \"192.168.48.18:9300\",\n      \"host\": \"192.168.48.18\",\n      \"ip\": \"192.168.48.18\",\n      \"version\": \"2.3.3\",\n      \"build\": \"218bdf1\",\n      \"http_address\": \"192.168.48.18:9200\",\n      \"settings\": {\n        \"pidfile\": \"\/var\/run\/elasticsearch\/elasticsearch.pid\",\n        \"cluster\": {\n          \"name\": \"elasticsearch\"\n        },\n        \"path\": {\n          \"conf\": \"\/etc\/elasticsearch\",\n          \"data\": \"\/var\/lib\/elasticsearch\",\n          \"logs\": \"\/var\/log\/elasticsearch\",\n          \"home\": \"\/usr\/share\/elasticsearch\",\n          \"repo\": [\n            \"\/home\/somename\/es_backups\"\n          ]\n        },\n        \"name\": \"Black King\",\n        \"client\": {\n          \"type\": \"node\"\n        },\n        \"foreground\": \"false\",\n        \"config\": {\n          \"ignore_system_properties\": \"true\"\n        },\n        \"network\": {\n          \"host\": \"0.0.0.0\"\n        }\n      },\n      \"os\": {\n        \"refresh_interval_in_millis\": 1000,\n        \"name\": \"Linux\",\n        \"arch\": \"amd64\",\n        \"version\": \"3.19.0-59-generic\",\n        \"available_processors\": 8,\n        \"allocated_processors\": 8\n      },\n      \"process\": {\n        \"refresh_interval_in_millis\": 1000,\n        \"id\": 1685,\n        \"mlockall\": false\n      },\n      \"jvm\": {\n        \"pid\": 1685,\n        \"version\": \"1.8.0_66\",\n        \"vm_name\": \"Java HotSpot(TM) 64-Bit Server VM\",\n        \"vm_version\": \"25.66-b17\",\n        \"vm_vendor\": \"Oracle Corporation\",\n        \"start_time_in_millis\": 1463663018422,\n        \"mem\": {\n          \"heap_init_in_bytes\": 6442450944,\n          \"heap_max_in_bytes\": 6372720640,\n          \"non_heap_init_in_bytes\": 2555904,\n          \"non_heap_max_in_bytes\": 0,\n          \"direct_max_in_bytes\": 6372720640\n        },\n        \"gc_collectors\": [\n          \"ParNew\",\n          \"ConcurrentMarkSweep\"\n        ],\n        \"memory_pools\": [\n          \"Code Cache\",\n          \"Metaspace\",\n          \"Compressed Class Space\",\n          \"Par Eden Space\",\n          \"Par Survivor Space\",\n          \"CMS Old Gen\"\n        ],\n        \"using_compressed_ordinary_object_pointers\": \"true\"\n      },\n      \"thread_pool\": {\n        \"force_merge\": {\n          \"type\": \"fixed\",\n          \"min\": 1,\n          \"max\": 1,\n          \"queue_size\": -1\n        },\n        \"percolate\": {\n          \"type\": \"fixed\",\n          \"min\": 8,\n          \"max\": 8,\n          \"queue_size\": 1000\n        },\n        \"fetch_shard_started\": {\n          \"type\": \"scaling\",\n          \"min\": 1,\n          \"max\": 16,\n          \"keep_alive\": \"5m\",\n          \"queue_size\": -1\n        },\n        \"listener\": {\n          \"type\": \"fixed\",\n          \"min\": 4,\n          \"max\": 4,\n          \"queue_size\": -1\n        },\n        \"index\": {\n          \"type\": \"fixed\",\n          \"min\": 8,\n          \"max\": 8,\n          \"queue_size\": 200\n        },\n        \"refresh\": {\n          \"type\": \"scaling\",\n          \"min\": 1,\n          \"max\": 4,\n          \"keep_alive\": \"5m\",\n          \"queue_size\": -1\n        },\n        \"suggest\": {\n          \"type\": \"fixed\",\n          \"min\": 8,\n          \"max\": 8,\n          \"queue_size\": 1000\n        },\n        \"generic\": {\n          \"type\": \"cached\",\n          \"keep_alive\": \"30s\",\n          \"queue_size\": -1\n        },\n        \"warmer\": {\n          \"type\": \"scaling\",\n          \"min\": 1,\n          \"max\": 4,\n          \"keep_alive\": \"5m\",\n          \"queue_size\": -1\n        },\n        \"search\": {\n          \"type\": \"fixed\",\n          \"min\": 13,\n          \"max\": 13,\n          \"queue_size\": 1000\n        },\n        \"flush\": {\n          \"type\": \"scaling\",\n          \"min\": 1,\n          \"max\": 4,\n          \"keep_alive\": \"5m\",\n          \"queue_size\": -1\n        },\n        \"fetch_shard_store\": {\n          \"type\": \"scaling\",\n          \"min\": 1,\n          \"max\": 16,\n          \"keep_alive\": \"5m\",\n          \"queue_size\": -1\n        },\n        \"management\": {\n          \"type\": \"scaling\",\n          \"min\": 1,\n          \"max\": 5,\n          \"keep_alive\": \"5m\",\n          \"queue_size\": -1\n        },\n        \"get\": {\n          \"type\": \"fixed\",\n          \"min\": 8,\n          \"max\": 8,\n          \"queue_size\": 1000\n        },\n        \"bulk\": {\n          \"type\": \"fixed\",\n          \"min\": 8,\n          \"max\": 8,\n          \"queue_size\": 50\n        },\n        \"snapshot\": {\n          \"type\": \"scaling\",\n          \"min\": 1,\n          \"max\": 4,\n          \"keep_alive\": \"5m\",\n          \"queue_size\": -1\n        }\n      },\n      \"transport\": {\n        \"bound_address\": [\n          \"[::]:9300\"\n        ],\n        \"publish_address\": \"192.168.48.18:9300\",\n        \"profiles\": {}\n      },\n      \"http\": {\n        \"bound_address\": [\n          \"[::]:9200\"\n        ],\n        \"publish_address\": \"192.168.48.18:9200\",\n        \"max_content_length_in_bytes\": 104857600\n      },\n      \"plugins\": [],\n      \"modules\": [\n        {\n          \"name\": \"lang-expression\",\n          \"version\": \"2.3.3\",\n          \"description\": \"Lucene expressions integration for Elasticsearch\",\n          \"jvm\": true,\n          \"classname\": \"org.elasticsearch.script.expression.ExpressionPlugin\",\n          \"isolated\": true,\n          \"site\": false\n        },\n        {\n          \"name\": \"lang-groovy\",\n          \"version\": \"2.3.3\",\n          \"description\": \"Groovy scripting integration for Elasticsearch\",\n          \"jvm\": true,\n          \"classname\": \"org.elasticsearch.script.groovy.GroovyPlugin\",\n          \"isolated\": true,\n          \"site\": false\n        },\n        {\n          \"name\": \"reindex\",\n          \"version\": \"2.3.3\",\n          \"description\": \"_reindex and _update_by_query APIs\",\n          \"jvm\": true,\n          \"classname\": \"org.elasticsearch.index.reindex.ReindexPlugin\",\n          \"isolated\": true,\n          \"site\": false\n        }\n      ]\n    }\n  }\n}\n```\n\nScreenshot, I had to clear out the data:\n\n![error_es](https:\/\/cloud.githubusercontent.com\/assets\/12231719\/15397399\/668a374c-1de0-11e6-903d-f929a2d9f0b2.PNG)\n"},{"date":"2016-05-19T14:45:25Z","author":"clintongormley","text":"@rodgermoore does the query you provided work correctly? You said that it started working once you deleted the highlighting and this query doesn't contain highlighting.  Could you provide the query that doesn't work?\n"},{"date":"2016-05-19T14:51:27Z","author":"rodgermoore","text":"It does has highlighting enabled. This is the json for the search panel: \n\n```\n{\n  \"index\": \"someindex\",\n  \"query\": {\n    \"query_string\": {\n      \"query\": \"*\",\n      \"analyze_wildcard\": true\n    }\n  },\n  \"filter\": [],\n  \"highlight\": {\n    \"pre_tags\": [\n      \"@kibana-highlighted-field@\"\n    ],\n    \"post_tags\": [\n      \"@\/kibana-highlighted-field@\"\n    ],\n    \"fields\": {\n      \"*\": {}\n    },\n    \"require_field_match\": false,\n    \"fragment_size\": 2147483647\n  }\n}\n```\n\nI can't show the actual data so I selected to show only the timestamp field in the search panel in the screenshot...\n\nWhen I change the json of the search panel to:\n\n```\n{\n  \"index\": \"someindex\",\n  \"filter\": [],\n  \"query\": {\n    \"query_string\": {\n      \"query\": \"*\",\n      \"analyze_wildcard\": true\n    }\n  }\n}\n```\n\nThe error disappears.\n"},{"date":"2016-05-19T14:54:44Z","author":"dellis23","text":"If my understanding of the patch is correct, it shouldn't matter whether Kibana is including the highlighting field.  Elasticsearch should only be trying to highlight string fields, even if a wildcard is being used.\n"},{"date":"2016-05-19T16:23:44Z","author":"brwe","text":"Ok, I managed to reproduce it on 2.3.3. It happens with `\"geohash\": true` in the mapping. \n\nSteps are:\n\n```\nDELETE test\nPUT test \n{\n  \"mappings\": {\n    \"doc\": {\n      \"properties\": {\n        \"point\": {\n          \"type\": \"geo_point\",\n          \"geohash\": true\n        }\n      }\n    }\n  }\n}\n\nPUT test\/doc\/1\n{\n  \"point\": \"60.12,100.34\"\n}\n\nPOST test\/_search\n{\n  \"query\": {\n    \"geo_bounding_box\": {\n      \"point\": {\n        \"top_left\": {\n          \"lat\": 61.10078883158897,\n          \"lon\": -170.15625\n        },\n        \"bottom_right\": {\n          \"lat\": -64.92354174306496,\n          \"lon\": 118.47656249999999\n        }\n      }\n    }\n  },\n  \"highlight\": {\n    \"fields\": {\n      \"*\": {}\n    }\n  }\n}\n```\n\nSorry, I did not think of that. I work on another fix.\n"}],"reopen_on":"2016-09-12T12:37:23Z","opened_by":"jaksmid","closed_on":"2016-09-19T10:27:35Z","description":"Hi guys,\n\nwe have upgraded ElasticSearch from 2.3.0 and reindexed our geolocations so the latitude and longitude are stored separately. We have noticed that some of our visualisation started to fail after we add a filter based on geolocation rectangle. However, map visualisation are working just fine. The problem occurs when we include actual documents. In this case, we get some failed shards (usually 1 out of 5) and error: Invalid shift value (xx) in prefixCoded bytes (is encoded value really a geo point?).\n\nDetails:\nOur geolocation index is based on:\n\n```\n\"dynamic_templates\": [{\n....\n{\n        \"ner_geo\": {\n          \"mapping\": {\n            \"type\": \"geo_point\",\n            \"lat_lon\": true\n          },\n          \"path_match\": \"*.coordinates\"\n        }\n      }],\n```\n\nThe ok query with the error is as follows. If we change the query size to 0 (map visualizations example), the query completes without problem.\n\n```\n{\n  \"size\": 100,\n  \"aggs\": {\n    \"2\": {\n      \"geohash_grid\": {\n        \"field\": \"authors.affiliation.coordinates\",\n        \"precision\": 2\n      }\n    }\n  },\n  \"query\": {\n    \"filtered\": {\n      \"query\": {\n        \"query_string\": {\n          \"analyze_wildcard\": true,\n          \"query\": \"*\"\n        }\n      },\n      \"filter\": {\n        \"bool\": {\n          \"must\": [\n            {\n              \"geo_bounding_box\": {\n                \"authors.affiliation.coordinates\": {\n                  \"top_left\": {\n                    \"lat\": 61.10078883158897,\n                    \"lon\": -170.15625\n                  },\n                  \"bottom_right\": {\n                    \"lat\": -64.92354174306496,\n                    \"lon\": 118.47656249999999\n                  }\n                }\n              }\n            }\n          ],\n          \"must_not\": []\n        }\n      }\n    }\n  },\n  \"highlight\": {\n    \"pre_tags\": [\n      \"@kibana-highlighted-field@\"\n    ],\n    \"post_tags\": [\n      \"@\/kibana-highlighted-field@\"\n    ],\n    \"fields\": {\n      \"*\": {}\n    },\n    \"require_field_match\": false,\n    \"fragment_size\": 2147483647\n  }\n}\n```\n\nElasticsearch version**: 2.3.0\nOS version**: Elasticsearch docker image with head plugin, marvel and big desk installed\n\nThank you for your help,\nregards,\nJakub Smid\n","id":"145978723","title":"Invalid shift value (xx) in prefixCoded bytes (is encoded value really a geo point?)","reopen_by":"brwe","opened_on":"2016-04-05T12:52:21Z","closed_by":"brwe"},{"number":"17483","comments":[{"date":"2016-04-04T13:51:32Z","author":"pciccarese","text":"I am afraid I will need to reopen this.\n\n**Elasticsearch version**: 2.3.0\n**JVM version**: 1.8.0_31\n**OS version**: MAC OS X 10.10.5\n\n**Use of custom header does not seem to work reliably.**\n\nHere is the CORS section of my configuration (elasticsearch.yml) file:\n\n```\nhttp.cors.enabled: true\nhttp.cors.allow-origin: \"*\"\nhttp.cors.allow-headers: \"X-Requested-With, Content-Type, Content-Length, X-User\"\n```\n\nThe client code is performing the following call:\n\n```\njQuery.ajax({\n    url: requrl,\n    data: reqdata,\n    type: 'POST',\n    headers: {\"X-User\": user},\n    success: function (result, status, xhr) {\n        resolve(result);\n    },\n    error: function (xhr, status, error) {\n        reject(error);\n    }\n});\n\n```\n\nNow, that request always goes through with Firefox 45.0.1.\nHowever, it does not work for:\n- Safari Version 9.1 (10601.5.17.4)\n- Chrome Version 49.0.2623.110 (64-bit) \n\nWhen inspecting with Chrome I get:\n\n```\nXMLHttpRequest cannot load http:\/\/localhost:9200\/myindex\/mytype\/_search. \nRequest header field x-user is not allowed by Access-Control-Allow-Headers in preflight response.\n```\n\nThese are the headers details I see in Chrome:\n\n```\nGeneral\n-----------\nRequest URL:http:\/\/localhost:9200\/myindex\/mytype\/_search\nRequest Method:OPTIONS\nStatus Code:200 OK\nRemote Address:[::1]:9200\n\nResponse Headers\n--------------------------\nAccess-Control-Allow-Methods:\nAccess-Control-Allow-Origin:*\nAccess-Control-Max-Age:1728000\ncontent-length:0\ndate:Mon, 04 Apr 2016 13:49:00 GMT\n\nRequest Headers\n-----------------------\nAccept:*\/*\nAccept-Encoding:gzip, deflate, sdch\nAccept-Language:en-US,en;q=0.8,it;q=0.6\nAccess-Control-Request-Headers:accept, content-type, x-user\nAccess-Control-Request-Method:POST\nCache-Control:no-cache\nConnection:keep-alive\nHost:localhost:9200\nOrigin:http:\/\/localhost:3333\nPragma:no-cache\nReferer:http:\/\/localhost:3333\/\nUser-Agent:Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/49.0.2623.110 Safari\/537.36\n```\n\n**The very same configuration, index and client code works reliably in 2.2.0**.\nAfter seeing in the release notes the entry: \u201CMore robust handling of CORS HTTP Access Control\u201D I am wondering if that has anything to do with it.\n"},{"date":"2016-04-04T17:40:17Z","author":"skundrik","text":"I have experienced same issue which now breaks Grafana UI.\n"},{"date":"2016-04-04T19:02:28Z","author":"skundrik","text":"- The problem appears to be in `NettyHttpServerTransport.buildCorsConfig()` where first parameter to the `getAsArray()` function should be the setting name and not the value of the setting. \n- It affects `cors-allow-headers` as well.\n- It's also specific to `2.3 branch` as the whole settings system seems to have been reworked on `master`.\n\nThe following code\n\n``` java\n        String[] strMethods = settings.getAsArray(settings.get(SETTING_CORS_ALLOW_METHODS, DEFAULT_CORS_METHODS), new String[0]);\n        HttpMethod[] methods = new HttpMethod[strMethods.length];\n        for (int i = 0; i < methods.length; i++) {\n            methods[i] = HttpMethod.valueOf(strMethods[i]);\n        }\n        return builder.allowedRequestMethods(methods)\n                      .maxAge(settings.getAsInt(SETTING_CORS_MAX_AGE, DEFAULT_CORS_MAX_AGE))\n                      .allowedRequestHeaders(settings.getAsArray(settings.get(SETTING_CORS_ALLOW_HEADERS, DEFAULT_CORS_HEADERS), new String[0]))\n                      .shortCircuit()\n                      .build();\n```\n\nshould say\n\n``` java\n        String[] strMethods = settings.getAsArray(SETTING_CORS_ALLOW_METHODS, DEFAULT_CORS_METHODS);\n        HttpMethod[] methods = new HttpMethod[strMethods.length];\n        for (int i = 0; i < methods.length; i++) {\n            methods[i] = HttpMethod.valueOf(strMethods[i]);\n        }\n        return builder.allowedRequestMethods(methods)\n                      .maxAge(settings.getAsInt(SETTING_CORS_MAX_AGE, DEFAULT_CORS_MAX_AGE))\n                      .allowedRequestHeaders(settings.getAsArray(SETTING_CORS_ALLOW_HEADERS, DEFAULT_CORS_HEADERS))\n                      .shortCircuit()\n                      .build();\n\n```\n"},{"date":"2016-04-05T13:32:37Z","author":"abeyad","text":"Thank you for reporting the issue.  We have PRs open to fix them for different versions.\n\n#17523 #17524 #17525 \n"}],"reopen_on":"2016-04-04T13:51:41Z","opened_by":"pciccarese","closed_on":"2016-04-07T19:35:20Z","description":"**Elasticsearch version**: 2.3.0\n**JVM version**: 1.8.0_31\n**OS version**: MAC OS X 10.10.5\n**Description of the problem including expected versus actual behavior**:\nDefinition of an accepted a custom header does not seem to work.\n\n**Steps to reproduce**:\n1. Add the following lines to the configuration file\n\n```\nhttp.cors.enabled: true\nhttp.cors.allow-origin: \"*\"\nhttp.cors.allow-methods: OPTIONS, HEAD, GET, POST, PUT, DELETE\nhttp.cors.allow-headers: \"X-Requested-With, Content-Type, Content-Length, X-User\"\n```\n1. Ran an ajax POST request with X-User header.\n2. Getting response \"Request header field x-user is not allowed by Access-Control-Allow-Headers in preflight response.\"\n","id":"145246162","title":"Problems with http.cors.allow-methods","reopen_by":"pciccarese","opened_on":"2016-04-01T18:07:55Z","closed_by":"abeyad"},{"number":"17097","comments":[{"date":"2016-03-14T16:34:04Z","author":"dadoonet","text":"@costin Is it the same as #15148 closed by #15447 in 2.3.0?\n"},{"date":"2016-03-14T16:36:51Z","author":"jasontedor","text":"Duplicates #15148, closed by #15447\n"},{"date":"2016-03-14T17:20:02Z","author":"costin","text":"looks about right - thanks for jumping on this guys. I just assumed 2.2 is recent enough to not check for duplicates :) Lazy, I know.\n"}],"reopen_on":"2016-03-14T16:36:51Z","opened_by":"costin","closed_on":"2016-03-14T16:36:53Z","description":"**Elasticsearch version**: 2.2\n\n**JVM version**: 1.8u66\n\n**OS version**: Win 8.1 \n\n**Description of the problem including expected versus actual behavior**:\n\nWhen specifying a bogus analyzer (maybe a typo), ES throws an internal NPE in the logs and the request blocks. Instead the exception should be thrown to the user.\n\n**Steps to reproduce**:\n1. \n\n```\ncurl -XPOST http:\/\/localhost:9200\/_analyze -d '{\n  \"analyzer\" : \"bogus\",\n  \"text\" : \"this is a test\"\n}'\n```\n\n**Provide logs (if relevant)**:\n\n```\n[2016-03-14 18:17:44,393][ERROR][transport                ] [Caleb Alexander] failed to handle exception for action [indices:admin\/analyze[s]], handler [org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$1@a566c0]\njava.lang.NullPointerException\n    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:195)\n    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.access$700(TransportSingleShardAction.java:115)\n    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$1.handleException(TransportSingleShardAction.java:174)\n    at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:821)\n    at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:799)\n    at org.elasticsearch.transport.TransportService$4.onFailure(TransportService.java:361)\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:39)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n```\n","id":"140723076","title":"Bogus analyzer fails with a NPE in the _analyzer API","reopen_by":"jasontedor","opened_on":"2016-03-14T16:21:49Z","closed_by":"jasontedor"},{"number":"17025","comments":[{"date":"2016-03-09T08:33:48Z","author":"s1monw","text":"I am looking into this - thanks for reporting\n"},{"date":"2016-03-09T09:12:51Z","author":"s1monw","text":"this will be in the upcoming 2.2.1 release - thanks for reporting\n"},{"date":"2016-03-09T11:52:21Z","author":"samcday","text":":+1: thanks for the über fast response! :)\n"},{"date":"2016-04-06T04:47:04Z","author":"samcday","text":"@s1monw we need to re-open this one, 2.2.1 still has no index name logged in _query_ slowlogs. By the looks of things your commit fixed indexing slowlogs, but I see no mention of query slowlogs.\n"},{"date":"2016-04-06T11:45:16Z","author":"clintongormley","text":"Need to make the same change for query slowlogs.\n"}],"reopen_on":"2016-04-06T11:44:43Z","opened_by":"samcday","closed_on":"2016-04-18T10:40:53Z","description":"**Elasticsearch version**: 2.2.0\n\n**JVM version**: 1.8.0_40\n\n**OS version**: OSX and Ubuntu 15.04\n\n**Description of the problem including expected versus actual behavior**:\nUnless I'm completely and utterly blind (which is a distinct possibility), it seems that both indexing and query slowlogs no longer include the name of the index that the operation was running against. This makes them almost entirely useless for us (since we have several hundred indices in our cluster).\n\n**Steps to reproduce**:\n1. Generate some queries \/ indexing requests that triggers a slowlog.\n2. Look at the slowlog\n3. Scratch your head and wonder why the `type` is present, but not the `index`.\n\n**Provide logs (if relevant)**:\n\nAn example slowlog from 2.2.0:\n\n```\n[2016-03-09 16:26:15,225][TRACE][index.indexing.slowlog.index] took[4.4ms], took_millis[4], type[test], id[AVNaDew0JKS5u4ProrcX], routing[] , source[{\"name\":\"vlucas\/phpdotenv\",\"version\":\"1.0.3\",\"type\":\"library\",\"description\":\"Loads environment variables from `.env` to `getenv()`, `$_ENV` and `$_SERVER` automagically.\",\"keywords\":[\"env\",\"dotenv\",\"environment\"],\"homepage\":\"http:\/\/github.com\/vlucas\/phpdotenv\",\"license\":\"BSD\",\"authors\":[{\"name\":\"Vance Lucas\",\"email\":\"vance@vancelucas.com\",\"homepage\":\"http:\/\/www.vancelucas.com\"}],\"require\":{\"php\":\">=5.3.2\"},\"require-dev\":{\"phpunit\/phpunit\":\"*\"},\"autoload\":{\"psr-0\":{\"Dotenv\":\"src\/\"}}}]\n```\n\nAn example slowlog from 1.7.5:\n\n```\n[2016-03-09 16:31:42,346][WARN ][index.indexing.slowlog.index] [Prowler] [slowtest][1] took[95.7ms], took_millis[95], type[test], id[AVNaEumoVIxyp_lAGADK], routing[], source[{\"name\":\"vlucas\/phpdotenv\",\"version\":\"1.0.3\",\"type\":\"library\",\"description\":\"Loads environment variables from `.env` to `getenv()`, `$_ENV` and `$_SERVER` automagically.\",\"keywords\":[\"env\",\"dotenv\",\"environment\"],\"homepage\":\"http:\/\/github.com\/vlucas\/phpdotenv\",\"license\":\"BSD\",\"authors\":[{\"name\":\"Vance Lucas\",\"email\":\"vance@vancelucas.com\",\"homepage\":\"http:\/\/www.vancelucas.com\"}],\"require\":{\"php\":\">=5.3.2\"},\"require-dev\":{\"phpunit\/phpunit\":\"*\"},\"autoload\":{\"psr-0\":{\"Dotenv\":\"src\/\"}}}]\n```\n","id":"139489767","title":"Slowlogs no longer include index name","reopen_by":"clintongormley","opened_on":"2016-03-09T06:33:21Z","closed_by":"jimczi"},{"number":"16219","comments":[{"date":"2016-01-26T12:36:33Z","author":"clintongormley","text":"Hi @medcl \n\nI tested this in 2.1.0 and 2.2.0 and it works as expected.  Try this:\n\n```\nPUT t\/t\/1\n{\n  \"name\": \"Foo Bar\"\n}\n\nGET _search\n{\n  \"fielddata_fields\": [\"name\"]\n}\n```\n\nIt also works for the `_all` field.\n"},{"date":"2016-01-26T14:09:35Z","author":"medcl","text":"@clintongormley  here is the method what i tried \n\n```\ncurl http:\/\/localhost:9200\/_analyze\\?text\\=medcl+test1234\n{\"tokens\":[{\"token\":\"medcl\",\"start_offset\":0,\"end_offset\":5,\"type\":\"<ALPHANUM>\",\"position\":0},{\"token\":\"test1234\",\"start_offset\":6,\"end_offset\":14,\"type\":\"<ALPHANUM>\",\"position\":1}]}\n```\n"},{"date":"2016-01-26T14:20:28Z","author":"clintongormley","text":"@medcl ah ok - that's a different issue, it's a bug in the analyze API and has never worked..  I'll change the title and reopen\n"},{"date":"2016-01-28T09:35:12Z","author":"johtani","text":"Note: The `standard` is specified the request without index.\nhttps:\/\/github.com\/elastic\/elasticsearch\/blob\/master\/core\/src\/main\/java\/org\/elasticsearch\/action\/admin\/indices\/analyze\/TransportAnalyzeAction.java#L234\n"},{"date":"2016-01-28T11:17:48Z","author":"clintongormley","text":"@johtani perhaps it should look for that setting first, before defaulting to standard.\n"},{"date":"2016-01-28T13:06:18Z","author":"johtani","text":"@clintongormley Agreed\n"},{"date":"2016-04-26T05:58:51Z","author":"johtani","text":"Noted: In 5.0, we can't set node level default analyzer. See: https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/master\/breaking_50_settings_changes.html#_index_level_settings\n"},{"date":"2016-04-26T14:17:28Z","author":"clintongormley","text":"agreed - closing\n"}],"reopen_on":"2016-01-26T14:20:28Z","opened_by":"medcl","closed_on":"2016-04-26T14:17:28Z","description":"Hi,\nI tried to set the default analyzer at node level by config the `elasticsearch.yml`, but without success, here is the config i used to test, under elasticsearch version 2.0 and 2.1.*\n\n```\nindex :\n  analysis :\n    analyzer :\n      default :\n        tokenizer : keyword\n```\n\nand \n\n```\nindex.analysis.analyzer.default.type: keyword\n```\n","id":"128744473","title":"Analysis API doesn't respect node level default analyzer setting","reopen_by":"clintongormley","opened_on":"2016-01-26T07:21:02Z","closed_by":"clintongormley"},{"number":"15860","comments":[{"date":"2016-01-08T18:33:44Z","author":"nik9000","text":"Fun times. Reproduces in master. I'll work on fixing it there and backporting it after that.\n"},{"date":"2016-01-08T20:01:37Z","author":"jreinke","text":"FYI, I have reported a similar bug with the same symptoms (1 numeric field included is ok but 2+ numeric fields give `number_format_exception`). Don't know if it could be related.\n\nhttps:\/\/github.com\/elastic\/elasticsearch\/issues\/3975#issuecomment-167577538\n"},{"date":"2016-01-08T20:22:56Z","author":"nik9000","text":"Good timing! I've figure out what is up and I've started on a solution. I've only got about two more hours left to work on it so I might not have anything before the weekend but, yeah, I'll have something soon.\n\nThe `lenient: true` issue is similar so I'll work on it while I'm in there.\n"},{"date":"2016-01-11T17:27:25Z","author":"nik9000","text":"Had to revert the change. I'll get it in there though.\n"}],"reopen_on":"2016-01-11T17:27:07Z","opened_by":"jreinke","closed_on":"2016-01-15T14:54:59Z","description":"Hi team,\n\nI get an error using a `multi_match` query with `cross_fields` type and a numeric query.\nI'm using v2.1.1 on OSX, installed via Homebrew.\n\n**Index basic data**\n\n```\ncurl -XPUT http:\/\/localhost:9200\/blog\/post\/1?pretty=1 -d '{\"foo\":123, \"bar\":\"xyzzy\", \"baz\":456}'\n```\n\n**Use a `multi_match` query with `cross_fields` type and a numeric query**\n\n```\ncurl -XGET http:\/\/localhost:9200\/blog\/post\/_search?pretty=1 -d '{\"query\": {\"multi_match\": {\"type\": \"cross_fields\", \"query\": \"100\", \"lenient\": true, \"fields\": [\"foo\", \"bar\", \"baz\"]}}}'\n```\n\n**Error**\n\n```\n{\n  \"error\" : {\n    \"root_cause\" : [ {\n      \"type\" : \"illegal_argument_exception\",\n      \"reason\" : \"Illegal shift value, must be 0..63; got shift=2147483647\"\n    } ],\n    \"type\" : \"search_phase_execution_exception\",\n    \"reason\" : \"all shards failed\",\n    \"phase\" : \"query\",\n    \"grouped\" : true,\n    \"failed_shards\" : [ {\n      \"shard\" : 0,\n      \"index\" : \"blog\",\n      \"node\" : \"0TxGVVWsSu2qX63hZdOv2w\",\n      \"reason\" : {\n        \"type\" : \"illegal_argument_exception\",\n        \"reason\" : \"Illegal shift value, must be 0..63; got shift=2147483647\"\n      }\n    } ]\n  },\n  \"status\" : 400\n}\n```\n\n**Note that the error does not appear if I specify only 1 numeric field in search.**\n\n**Stack trace**\n\n```\nCaused by: java.lang.IllegalArgumentException: Illegal shift value, must be 0..63; got shift=2147483647\n    at org.apache.lucene.util.NumericUtils.longToPrefixCodedBytes(NumericUtils.java:147)\n    at org.apache.lucene.util.NumericUtils.longToPrefixCoded(NumericUtils.java:121)\n    at org.apache.lucene.analysis.NumericTokenStream$NumericTermAttributeImpl.getBytesRef(NumericTokenStream.java:163)\n    at org.apache.lucene.analysis.NumericTokenStream$NumericTermAttributeImpl.clone(NumericTokenStream.java:217)\n    at org.apache.lucene.analysis.NumericTokenStream$NumericTermAttributeImpl.clone(NumericTokenStream.java:148)\n    at org.apache.lucene.util.AttributeSource$State.clone(AttributeSource.java:54)\n    at org.apache.lucene.util.AttributeSource.captureState(AttributeSource.java:281)\n    at org.apache.lucene.analysis.CachingTokenFilter.fillCache(CachingTokenFilter.java:96)\n    at org.apache.lucene.analysis.CachingTokenFilter.incrementToken(CachingTokenFilter.java:70)\n    at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:223)\n    at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)\n    at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:178)\n    at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:55)\n    at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:42)\n    at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:118)\n    at org.elasticsearch.index.search.MultiMatchQuery$CrossFieldsQueryBuilder.buildGroupedQueries(MultiMatchQuery.java:198)\n    at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:86)\n    at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:163)\n    at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:257)\n    at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:303)\n    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:206)\n    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:201)\n    at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)\n    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:831)\n    at org.elasticsearch.search.SearchService.createContext(SearchService.java:651)\n    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:617)\n    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:368)\n    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)\n    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)\n    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)\n    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\n```\n","id":"125648180","title":"multi_match query gives java.lang.IllegalArgumentException: Illegal shift value, must be 0..63; got shift=2147483647","reopen_by":"nik9000","opened_on":"2016-01-08T16:52:27Z","closed_by":"nik9000"},{"number":"15762","comments":[{"date":"2016-01-05T01:09:43Z","author":"mikemccand","text":"LGTM\n"},{"date":"2016-01-05T04:04:23Z","author":"jasontedor","text":"LGTM.\n"}],"reopen_on":"2016-01-05T07:23:03Z","opened_by":"s1monw","closed_on":"2016-01-05T07:23:12Z","description":"If we fail to create a writer all recovered translog readers are not\nclosed today which causes all open files to leak.\n\nCloses #15754\n\n@jasontedor @mikemccand can you take a look\n","id":"124856442","title":"Close recovered translog readers if createWriter fails","reopen_by":"s1monw","opened_on":"2016-01-04T22:43:43Z","closed_by":"s1monw"},{"number":"15264","comments":[{"date":"2015-12-07T08:45:24Z","author":"colings86","text":"@martinstuga thanks for the PR, could you sign the Contributer Licence Agreeement (https:\/\/www.elastic.co\/contributor-agreement\/) so we can merge this?\n"},{"date":"2015-12-07T08:49:31Z","author":"martinstuga","text":"@colings86 I've already signed CLA twice. I don't know why, but it's still appearing as not signed. May you help me please?\n"},{"date":"2015-12-07T09:55:04Z","author":"colings86","text":"@martinstuga it looks like the email you used to sign the CLA is different from the email on your commit. You could add the email address you use in your commits to your Github profile (you can make it private so it doesn't show up on your public profile) and this will allow our CLA checker tool to link your Github commits with your CLA signature in the future.\n"},{"date":"2015-12-07T12:07:20Z","author":"martinstuga","text":"Thanks for the help about the CLA. I'm seeing that you're going ahead with this issue.\n"},{"date":"2015-12-07T14:41:27Z","author":"colings86","text":"@martinstuga unfortunately at the moment the build fails with your change applied. Could you update this PR to fix the error, rebase your change onto the latest master and ensure `gradle assemble` runs successfully?\n\nThe error I get when running `gradle assemble` is:\n\n```\n$ES_REPO\/core\/src\/main\/java\/org\/elasticsearch\/search\/aggregations\/metrics\/stats\/StatsParser.java:37: error: package StatsAggegator does not exist\n        return new StatsAggegator.Factory(aggregationName, config);\n```\n"}],"reopen_on":"2015-12-07T10:09:08Z","opened_by":"martinstuga","closed_on":"2015-12-07T15:46:21Z","description":"Closes #14730\n","id":"120604931","title":"Correct typo in class name of StatsAggregator","reopen_by":"martinstuga","opened_on":"2015-12-06T03:31:43Z","closed_by":"martinstuga"},{"number":"15264","comments":[{"date":"2015-12-07T08:45:24Z","author":"colings86","text":"@martinstuga thanks for the PR, could you sign the Contributer Licence Agreeement (https:\/\/www.elastic.co\/contributor-agreement\/) so we can merge this?\n"},{"date":"2015-12-07T08:49:31Z","author":"martinstuga","text":"@colings86 I've already signed CLA twice. I don't know why, but it's still appearing as not signed. May you help me please?\n"},{"date":"2015-12-07T09:55:04Z","author":"colings86","text":"@martinstuga it looks like the email you used to sign the CLA is different from the email on your commit. You could add the email address you use in your commits to your Github profile (you can make it private so it doesn't show up on your public profile) and this will allow our CLA checker tool to link your Github commits with your CLA signature in the future.\n"},{"date":"2015-12-07T12:07:20Z","author":"martinstuga","text":"Thanks for the help about the CLA. I'm seeing that you're going ahead with this issue.\n"},{"date":"2015-12-07T14:41:27Z","author":"colings86","text":"@martinstuga unfortunately at the moment the build fails with your change applied. Could you update this PR to fix the error, rebase your change onto the latest master and ensure `gradle assemble` runs successfully?\n\nThe error I get when running `gradle assemble` is:\n\n```\n$ES_REPO\/core\/src\/main\/java\/org\/elasticsearch\/search\/aggregations\/metrics\/stats\/StatsParser.java:37: error: package StatsAggegator does not exist\n        return new StatsAggegator.Factory(aggregationName, config);\n```\n"}],"reopen_on":"2015-12-07T10:24:58Z","opened_by":"martinstuga","closed_on":"2015-12-07T15:46:21Z","description":"Closes #14730\n","id":"120604931","title":"Correct typo in class name of StatsAggregator","reopen_by":"martinstuga","opened_on":"2015-12-06T03:31:43Z","closed_by":"martinstuga"},{"number":"15225","comments":[{"date":"2015-12-04T08:54:01Z","author":"clintongormley","text":"This is because your are benchmarking an abusive and unrealistic use of Elasticsearch by randomly generating field names.  This is hitting two changes in particular:\n\n## Mapping changes must be confirmed by the master\n\nPreviously it was possible for the same field to be added to an index on different shards with different mapping.  This could result in incorrect results and even data loss down the line.  Now, any mapping update must be confirmed by the master (which then publishes a new cluster state to all nodes and waits for confirmation that it has ben received) before continuing with indexing.\n\nYou are adding new random field names in every document, so every document triggers a cluster state update. Not surprising that this is slow!\n\n## Doc values on by default\n\nAggregations, sorting, and scripting need to be able to retrieve the value of a field for a particular document.  Previously we used in-memory fielddata to do this, but that had two downsides:\n- A speed bump when loading fielddata from a big segment\n- It filled up your heap and could cause OOMs\n\nNow these values are written to disk at index time as doc values, a format which makes random access very fast.  This format requires a value for every field in every document (it is not sparse).  You are adding random field names each with one value, so your doc values matrix is growing exponentially.  This becomes even worse when small segments are merged into big segments.\n\n(Actually, in Lucene 5.4 there is an optimization for sparse doc values (LUCENE-6863)[https:\/\/issues.apache.org\/jira\/browse\/LUCENE-6863] which kicks in for fields that are present in less than 1% of documents).\n\n## Realistic benchmarks\n\nWe optimize for the way people actually use our software. Benchmarks are useless unless they test real world use cases.\n"},{"date":"2015-12-04T10:57:28Z","author":"ksundeepsatya","text":"@clintongormley I am not generating random field names each time, I am generating random values for the same object. There are 30 different fields and that is constant through out the test. In the attached BulkProcessor.txt you may notice that. I think that is a realistic use case. \n\nBulkProccessor is not slow in all cases. If I am adding 200K documents, it takes 40 seconds. But, if I add some documents, let's say 1000 and delete that index, and add 200K records to the same index, this time it takes 600 seconds. \n\nI repeated the same test on 2.0.1 and 2.0. In both the cases it takes only ~40 seconds. \n"},{"date":"2015-12-04T12:14:56Z","author":"clintongormley","text":">  In the attached BulkProcessor.txt you may notice that.\n\nOK - that wasn't evident in BulkProcessor - you just refer to a RandomValueGenerator but the code isn't there.\n\n@jasontedor has managed to replicate a slow down here.  Reopening\n"},{"date":"2015-12-04T12:29:11Z","author":"jasontedor","text":"@ksundeepsatya I've managed to reproduce a slow down here, and we have an understanding what is causing the slowdown. However, I was only observing a 2x slowdown, not a 10x slowdown. Given our understanding of what is causing the slowdown, are you by chance on spinning disks instead of SSD? That could explain the difference.\n"},{"date":"2015-12-04T12:36:02Z","author":"ksundeepsatya","text":"@jasontedor I am using SSD. I was able to see a 10X slow down on multiple machines all having SSD. \n"},{"date":"2015-12-04T12:38:46Z","author":"jasontedor","text":"> I am using SSD. I was able to see a 10X slow down on multiple machines all having SSD.\n\n@ksundeepsatya Okay, thanks. I don't have a good explanation for observing a 2x difference locally vs your reported 10x, but either way there is a performance regression here that we will address.\n"},{"date":"2015-12-04T12:44:32Z","author":"ksundeepsatya","text":"@jasontedor just curious to know if if this could be reproducible in any other flow other the way I mentioned. \nI am seeing it in 2.1, but not in 2.0.1. If this is a isolated case, I can upgrade to 2.1 instead of 2.0.1. \n"},{"date":"2015-12-04T14:44:22Z","author":"sarwarbhuiyan","text":"> high disk watermark [90%] exceeded on [ZkeRJaTUR4iu3h8DmIiV2w][Thing]... free: 11gb[4.7%], shards will be relocated away from this node.\n\nHow many nodes are you running and how much free disk are you having before runs? It looks like your'e running out of disk too.\n"},{"date":"2015-12-04T15:06:03Z","author":"ksundeepsatya","text":"@sarwarbhuiyan I am running Embedded Elasticsearch, single node. I have 328G free space on my disk. \nHere is the code snippet\nSettings.Builder elasticsearchSettings = Settings.settingsBuilder()\n                .put(\"http.enabled\", \"false\").put(\"path.data\", dataDirectory)\n                .put(\"path.home\", dataDirectory).put(\"script.indexed\", \"on\");\n\n```\n    client = nodeBuilder().local(true)\n            .settings(elasticsearchSettings.build()).node().client();\n```\n"},{"date":"2015-12-04T15:08:29Z","author":"jasontedor","text":"> just curious to know if if this could be reproducible in any other flow other the way I mentioned. \n\n@ksundeepsatya We are still investigating the scenarios under which this can occur. The reason that I asked about SSD vs. spinning disk is because the underlying cause appears to be due to how frequently merges are occurring.\n"},{"date":"2015-12-04T15:09:16Z","author":"jasontedor","text":"> How many nodes are you running and how much free disk are you having before runs? It looks like your'e running out of disk too.\n\n@sarwarbhuiyan It's a single embedded node in @ksundeepsatya's test code. The disk space is a red herring and not related to the underlying issue.\n"},{"date":"2015-12-04T20:13:20Z","author":"jasontedor","text":"> just curious to know if if this could be reproducible in any other flow other the way I mentioned. \n\n@ksundeepsatya Coming back to this question, we have a pretty good understanding now of the underlying cause of the performance regression, why that regression happened, and the scenarios under which it occurred.\n\nThe performance regression was caused by a substantial increase in the number of merges. The merges were caused by a certain indexing buffer (the version map) not being increased in size after bulk indexing started. The reason that the version map was not resized is because the controller that manages the indexing buffers did not detect the change in state for the underlying shards. Finally, the reason that it did not detect the state change is because of the create -> index -> delete -> create -> index sequence that you went through. The second time that the index was created, the shards were regenerated with the same ShardIds that they previously had. The state from the previous shards had not cleared and so the controller did not resize the buffer.\n\nSo, it appears the scenarios under which this regression can occur are rare and special. That said, it led us to rework the controller code so that it is not stateful and we are less likely to run into scenarios like this one.\n\nI've opened #15251 to address this and we will be working to have it released in version 2.1.1.\n\nThank you for discovering and reporting this issue.\n"},{"date":"2015-12-05T07:43:46Z","author":"javanna","text":"Let me clarify that this issue was not a problem in the BulkProcessor (java api code) as the title suggested, but in the indexing code on the server side. I updated the title to reflect that.\n"},{"date":"2015-12-10T12:13:06Z","author":"lwolters","text":"Hi javanna \/ jason.\n\nExperiencing the same over here. Just upgraded our ES server to 2.1.0 and suddenly test execution times are running up 10 times. (Only after deleting the index and reinserting test data)\n\nBut luckily you found the issue and hopefully it is fixed. Can I verify the fix by running a snapshot? if so, where can I find the latest snapshot? (sonatype?)\n\nThanks in advance\n"},{"date":"2015-12-10T12:48:31Z","author":"jasontedor","text":"> Experiencing the same over here. Just upgraded our ES server to 2.1.0 and suddenly test execution times are running up 10 times. (Only after deleting the index and reinserting test data)\n\nThat sounds like it could be the same issue.\n\n> Can I verify the fix by running a snapshot? if so, where can I find the latest snapshot? (sonatype?)\n\nYes. Clone the 2.1 branch of the source repository and package it using `mvn package -DskipTests`. This will produce a `tar.gz` that you can deploy.\n\n> Thanks in advance\n\nThank you! The sooner that you're able to let us know if you're still seeing performance issues after running against a build with the fix, the more likely it is that we'll be able to address in advance of the next release.\n"},{"date":"2015-12-10T13:27:46Z","author":"lwolters","text":"Hi Jason,\n\nVerified. When running with the 2.1.1-SNAPSHOT release from sonatype it simply runs as expected. Rerunning the tests (i.e. deleting \/ recreating indices) show similar performances when inserting data.\n"},{"date":"2015-12-10T13:46:52Z","author":"jasontedor","text":"> Verified. When running with the 2.1.1-SNAPSHOT release from sonatype it simply runs as expected. Rerunning the tests (i.e. deleting \/ recreating indices) show similar performances when inserting data.\n\nThanks much for verifying!\n"},{"date":"2016-05-28T14:01:45Z","author":"tapit69","text":"\"Previously it was possible for the same field to be added to an index on different shards with different mapping. This could result in incorrect results and even data loss down the line. Now, any mapping update must be confirmed by the master (which then publishes a new cluster state to all nodes and waits for confirmation that it has ben received) before continuing with indexing.\"  \n\nYou should be able to turn this off when creating many types under an index. And then be able to turn it back on when finish.  For example if I want to create an index with 200  types  all with the same mapping but have different type names. Like aa,ab,ac etc. In 1.7,2 I could just run a script and it would take about 10 Minutes now with 2.3. it takes HOURS. I never could finish I had to go back to 1.7.2\n"},{"date":"2016-05-29T21:05:31Z","author":"bleskes","text":"> For example if I want to create an index with 200 types all with the same mapping but have different type names\n\nIf all mappings are the same, maybe you could use a field with a value mapped to what you currently store as type. It will make things much simpler and easier to manage, with the one downside of having to add a filter to each search. This downside can also be mitigated by setting aliases with a filter.\n\nAnother option is to create the mapping in advance, before you index.\n\n> On 2.3 it takes HOURS..\n\nThat's peculiar as mapping updates are batched and processed together. Any idea why? Do you have hot threads of the master?\n"},{"date":"2016-06-24T13:32:35Z","author":"ayco-at-naturalis","text":"We have one index with 3 fairly large and deeply nested types. On ES1 it tooks us 38 minutes to import a dataset of 4.4 million records. With ES 2 (2.1, 2.2, 2.3) it now takes us 2 hours and 39 minutes. Both metrics come from imports on my dev machine (1 shard, 0 replicas).\n\nThat's almost 5 times as slow.\n\nBelow is the mapping for one of the three types:\n\n<details>\n\n```\n{\n  \"properties\" : {\n    \"sourceSystem\" : {\n      \"properties\" : {\n        \"code\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\"\n        },\n        \"name\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\",\n              \"index\" : \"analyzed\"\n            },\n            \"ci\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        }\n      }\n    },\n    \"sourceSystemId\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\",\n          \"index\" : \"analyzed\"\n        },\n        \"ci\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"recordURI\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\",\n          \"index\" : \"analyzed\"\n        },\n        \"ci\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"sourceInstitutionID\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\",\n          \"index\" : \"analyzed\"\n        },\n        \"ci\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"sourceID\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\",\n          \"index\" : \"analyzed\"\n        },\n        \"ci\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"owner\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\",\n          \"index\" : \"analyzed\"\n        },\n        \"ci\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"licenceType\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\",\n          \"index\" : \"analyzed\"\n        },\n        \"ci\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"licence\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\",\n          \"index\" : \"analyzed\"\n        },\n        \"ci\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"unitID\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\",\n          \"index\" : \"analyzed\"\n        },\n        \"ci\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"collectionType\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\",\n          \"index\" : \"analyzed\"\n        },\n        \"ci\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"title\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\",\n          \"index\" : \"analyzed\"\n        },\n        \"ci\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"caption\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\",\n          \"index\" : \"analyzed\"\n        },\n        \"ci\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"description\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\",\n          \"index\" : \"analyzed\"\n        },\n        \"ci\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"serviceAccessPoints\" : {\n      \"type\" : \"nested\",\n      \"properties\" : {\n        \"accessUri\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\",\n              \"index\" : \"analyzed\"\n            },\n            \"ci\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"format\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\",\n              \"index\" : \"analyzed\"\n            },\n            \"ci\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"variant\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\",\n              \"index\" : \"analyzed\"\n            },\n            \"ci\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        }\n      }\n    },\n    \"type\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\",\n          \"index\" : \"analyzed\"\n        },\n        \"ci\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"taxonCount\" : {\n      \"type\" : \"integer\"\n    },\n    \"creator\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\",\n          \"index\" : \"analyzed\"\n        },\n        \"ci\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"copyrightText\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\",\n          \"index\" : \"analyzed\"\n        },\n        \"ci\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"associatedSpecimenReference\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\",\n          \"index\" : \"analyzed\"\n        },\n        \"ci\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"associatedTaxonReference\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\",\n          \"index\" : \"analyzed\"\n        },\n        \"ci\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"specimenTypeStatus\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\",\n          \"index\" : \"analyzed\"\n        },\n        \"ci\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"multiMediaPublic\" : {\n      \"type\" : \"boolean\"\n    },\n    \"subjectParts\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\",\n          \"index\" : \"analyzed\"\n        },\n        \"ci\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"subjectOrientations\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\",\n          \"index\" : \"analyzed\"\n        },\n        \"ci\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"phasesOrStages\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\",\n          \"index\" : \"analyzed\"\n        },\n        \"ci\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"sexes\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\",\n          \"index\" : \"analyzed\"\n        },\n        \"ci\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"gatheringEvents\" : {\n      \"type\" : \"nested\",\n      \"properties\" : {\n        \"projectTitle\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\",\n              \"index\" : \"analyzed\"\n            },\n            \"ci\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"worldRegion\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\",\n              \"index\" : \"analyzed\"\n            },\n            \"ci\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"continent\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\",\n              \"index\" : \"analyzed\"\n            },\n            \"ci\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"country\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\",\n              \"index\" : \"analyzed\"\n            },\n            \"ci\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"iso3166Code\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\",\n              \"index\" : \"analyzed\"\n            },\n            \"ci\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"provinceState\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\",\n              \"index\" : \"analyzed\"\n            },\n            \"ci\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"island\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\",\n              \"index\" : \"analyzed\"\n            },\n            \"ci\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"locality\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\",\n              \"index\" : \"analyzed\"\n            },\n            \"ci\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"city\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\",\n              \"index\" : \"analyzed\"\n            },\n            \"ci\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"sublocality\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\",\n              \"index\" : \"analyzed\"\n            },\n            \"ci\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"localityText\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\",\n              \"index\" : \"analyzed\"\n            },\n            \"ci\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            },\n            \"like\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"like_analyzer\"\n            }\n          }\n        },\n        \"dateTimeBegin\" : {\n          \"type\" : \"date\"\n        },\n        \"dateTimeEnd\" : {\n          \"type\" : \"date\"\n        },\n        \"method\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\",\n              \"index\" : \"analyzed\"\n            },\n            \"ci\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"altitude\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\",\n              \"index\" : \"analyzed\"\n            },\n            \"ci\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"altitudeUnifOfMeasurement\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\",\n              \"index\" : \"analyzed\"\n            },\n            \"ci\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"depth\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\",\n              \"index\" : \"analyzed\"\n            },\n            \"ci\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"depthUnitOfMeasurement\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\",\n              \"index\" : \"analyzed\"\n            },\n            \"ci\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"gatheringPersons\" : {\n          \"type\" : \"nested\",\n          \"properties\" : {\n            \"agentText\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"fullName\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                },\n                \"like\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"like_analyzer\"\n                }\n              }\n            },\n            \"organization\" : {\n              \"properties\" : {\n                \"agentText\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"not_analyzed\",\n                  \"fields\" : {\n                    \"analyzed\" : {\n                      \"type\" : \"string\",\n                      \"index\" : \"analyzed\"\n                    },\n                    \"ci\" : {\n                      \"type\" : \"string\",\n                      \"analyzer\" : \"case_insensitive_analyzer\"\n                    }\n                  }\n                },\n                \"name\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"not_analyzed\",\n                  \"fields\" : {\n                    \"analyzed\" : {\n                      \"type\" : \"string\",\n                      \"index\" : \"analyzed\"\n                    },\n                    \"ci\" : {\n                      \"type\" : \"string\",\n                      \"analyzer\" : \"case_insensitive_analyzer\"\n                    }\n                  }\n                }\n              }\n            }\n          }\n        },\n        \"gatheringOrganizations\" : {\n          \"type\" : \"nested\",\n          \"properties\" : {\n            \"agentText\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"name\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            }\n          }\n        },\n        \"siteCoordinates\" : {\n          \"type\" : \"nested\",\n          \"properties\" : {\n            \"longitudeDecimal\" : {\n              \"type\" : \"double\"\n            },\n            \"latitudeDecimal\" : {\n              \"type\" : \"double\"\n            },\n            \"gridCellSystem\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"gridLatitudeDecimal\" : {\n              \"type\" : \"double\"\n            },\n            \"gridLongitudeDecimal\" : {\n              \"type\" : \"double\"\n            },\n            \"gridCellCode\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"gridQualifier\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"point\" : {\n              \"type\" : \"geo_shape\"\n            }\n          }\n        },\n        \"bioStratigraphy\" : {\n          \"type\" : \"nested\",\n          \"properties\" : {\n            \"youngBioDatingQualifier\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngBioName\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngFossilZone\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngFossilSubZone\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngBioCertainty\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngStratType\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"bioDatingQualifier\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"bioPreferredFlag\" : {\n              \"type\" : \"boolean\"\n            },\n            \"rangePosition\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldBioName\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"bioIdentifier\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldFossilzone\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldFossilSubzone\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldBioCertainty\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldBioStratType\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            }\n          }\n        },\n        \"chronoStratigraphy\" : {\n          \"type\" : \"nested\",\n          \"properties\" : {\n            \"youngRegionalSubstage\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngRegionalStage\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngRegionalSeries\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngDatingQualifier\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngInternSystem\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngInternSubstage\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngInternStage\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngInternSeries\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngInternErathem\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngInternEonothem\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngChronoName\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngCertainty\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldDatingQualifier\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"chronoPreferredFlag\" : {\n              \"type\" : \"boolean\"\n            },\n            \"oldRegionalSubstage\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldRegionalStage\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldRegionalSeries\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldInternSystem\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldInternSubstage\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldInternStage\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldInternSeries\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldInternErathem\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldInternEonothem\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldChronoName\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"chronoIdentifier\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldCertainty\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            }\n          }\n        },\n        \"lithoStratigraphy\" : {\n          \"type\" : \"nested\",\n          \"properties\" : {\n            \"qualifier\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"preferredFlag\" : {\n              \"type\" : \"boolean\"\n            },\n            \"member2\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"member\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"informalName2\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"informalName\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"importedName2\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"importedName1\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"lithoIdentifier\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"formation2\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"formationGroup2\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"formationGroup\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"formation\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"certainty2\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"certainty\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"bed2\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"bed\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            }\n          }\n        }\n      }\n    },\n    \"identifications\" : {\n      \"type\" : \"nested\",\n      \"properties\" : {\n        \"taxonRank\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\",\n              \"index\" : \"analyzed\"\n            },\n            \"ci\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"scientificName\" : {\n          \"properties\" : {\n            \"fullScientificName\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"taxonomicStatus\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"genusOrMonomial\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                },\n                \"like\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"like_analyzer\"\n                }\n              }\n            },\n            \"subgenus\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"specificEpithet\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                },\n                \"like\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"like_analyzer\"\n                }\n              }\n            },\n            \"infraspecificEpithet\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"infraspecificMarker\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"nameAddendum\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"authorshipVerbatim\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"author\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"year\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"references\" : {\n              \"type\" : \"nested\",\n              \"properties\" : {\n                \"titleCitation\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"not_analyzed\",\n                  \"fields\" : {\n                    \"analyzed\" : {\n                      \"type\" : \"string\",\n                      \"index\" : \"analyzed\"\n                    },\n                    \"ci\" : {\n                      \"type\" : \"string\",\n                      \"analyzer\" : \"case_insensitive_analyzer\"\n                    }\n                  }\n                },\n                \"citationDetail\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"not_analyzed\",\n                  \"fields\" : {\n                    \"analyzed\" : {\n                      \"type\" : \"string\",\n                      \"index\" : \"analyzed\"\n                    },\n                    \"ci\" : {\n                      \"type\" : \"string\",\n                      \"analyzer\" : \"case_insensitive_analyzer\"\n                    }\n                  }\n                },\n                \"uri\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"not_analyzed\",\n                  \"fields\" : {\n                    \"analyzed\" : {\n                      \"type\" : \"string\",\n                      \"index\" : \"analyzed\"\n                    },\n                    \"ci\" : {\n                      \"type\" : \"string\",\n                      \"analyzer\" : \"case_insensitive_analyzer\"\n                    }\n                  }\n                },\n                \"author\" : {\n                  \"properties\" : {\n                    \"agentText\" : {\n                      \"type\" : \"string\",\n                      \"index\" : \"not_analyzed\",\n                      \"fields\" : {\n                        \"analyzed\" : {\n                          \"type\" : \"string\",\n                          \"index\" : \"analyzed\"\n                        },\n                        \"ci\" : {\n                          \"type\" : \"string\",\n                          \"analyzer\" : \"case_insensitive_analyzer\"\n                        }\n                      }\n                    },\n                    \"fullName\" : {\n                      \"type\" : \"string\",\n                      \"index\" : \"not_analyzed\",\n                      \"fields\" : {\n                        \"analyzed\" : {\n                          \"type\" : \"string\",\n                          \"index\" : \"analyzed\"\n                        },\n                        \"ci\" : {\n                          \"type\" : \"string\",\n                          \"analyzer\" : \"case_insensitive_analyzer\"\n                        },\n                        \"like\" : {\n                          \"type\" : \"string\",\n                          \"analyzer\" : \"like_analyzer\"\n                        }\n                      }\n                    },\n                    \"organization\" : {\n                      \"properties\" : {\n                        \"agentText\" : {\n                          \"type\" : \"string\",\n                          \"index\" : \"not_analyzed\",\n                          \"fields\" : {\n                            \"analyzed\" : {\n                              \"type\" : \"string\",\n                              \"index\" : \"analyzed\"\n                            },\n                            \"ci\" : {\n                              \"type\" : \"string\",\n                              \"analyzer\" : \"case_insensitive_analyzer\"\n                            }\n                          }\n                        },\n                        \"name\" : {\n                          \"type\" : \"string\",\n                          \"index\" : \"not_analyzed\",\n                          \"fields\" : {\n                            \"analyzed\" : {\n                              \"type\" : \"string\",\n                              \"index\" : \"analyzed\"\n                            },\n                            \"ci\" : {\n                              \"type\" : \"string\",\n                              \"analyzer\" : \"case_insensitive_analyzer\"\n                            }\n                          }\n                        }\n                      }\n                    }\n                  }\n                },\n                \"publicationDate\" : {\n                  \"type\" : \"date\"\n                }\n              }\n            },\n            \"experts\" : {\n              \"type\" : \"nested\",\n              \"properties\" : {\n                \"agentText\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"not_analyzed\",\n                  \"fields\" : {\n                    \"analyzed\" : {\n                      \"type\" : \"string\",\n                      \"index\" : \"analyzed\"\n                    },\n                    \"ci\" : {\n                      \"type\" : \"string\",\n                      \"analyzer\" : \"case_insensitive_analyzer\"\n                    }\n                  }\n                },\n                \"fullName\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"not_analyzed\",\n                  \"fields\" : {\n                    \"analyzed\" : {\n                      \"type\" : \"string\",\n                      \"index\" : \"analyzed\"\n                    },\n                    \"ci\" : {\n                      \"type\" : \"string\",\n                      \"analyzer\" : \"case_insensitive_analyzer\"\n                    },\n                    \"like\" : {\n                      \"type\" : \"string\",\n                      \"analyzer\" : \"like_analyzer\"\n                    }\n                  }\n                },\n                \"organization\" : {\n                  \"properties\" : {\n                    \"agentText\" : {\n                      \"type\" : \"string\",\n                      \"index\" : \"not_analyzed\",\n                      \"fields\" : {\n                        \"analyzed\" : {\n                          \"type\" : \"string\",\n                          \"index\" : \"analyzed\"\n                        },\n                        \"ci\" : {\n                          \"type\" : \"string\",\n                          \"analyzer\" : \"case_insensitive_analyzer\"\n                        }\n                      }\n                    },\n                    \"name\" : {\n                      \"type\" : \"string\",\n                      \"index\" : \"not_analyzed\",\n                      \"fields\" : {\n                        \"analyzed\" : {\n                          \"type\" : \"string\",\n                          \"index\" : \"analyzed\"\n                        },\n                        \"ci\" : {\n                          \"type\" : \"string\",\n                          \"analyzer\" : \"case_insensitive_analyzer\"\n                        }\n                      }\n                    }\n                  }\n                }\n              }\n            }\n          }\n        },\n        \"defaultClassification\" : {\n          \"properties\" : {\n            \"kingdom\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"phylum\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"className\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"order\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"superFamily\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"family\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"genus\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"subgenus\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"specificEpithet\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"infraspecificEpithet\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"infraspecificRank\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            }\n          }\n        },\n        \"systemClassification\" : {\n          \"type\" : \"nested\",\n          \"properties\" : {\n            \"rank\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"name\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            }\n          }\n        },\n        \"vernacularNames\" : {\n          \"type\" : \"nested\",\n          \"properties\" : {\n            \"name\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"language\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"preferred\" : {\n              \"type\" : \"boolean\"\n            },\n            \"references\" : {\n              \"type\" : \"nested\",\n              \"properties\" : {\n                \"titleCitation\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"not_analyzed\",\n                  \"fields\" : {\n                    \"analyzed\" : {\n                      \"type\" : \"string\",\n                      \"index\" : \"analyzed\"\n                    },\n                    \"ci\" : {\n                      \"type\" : \"string\",\n                      \"analyzer\" : \"case_insensitive_analyzer\"\n                    }\n                  }\n                },\n                \"citationDetail\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"not_analyzed\",\n                  \"fields\" : {\n                    \"analyzed\" : {\n                      \"type\" : \"string\",\n                      \"index\" : \"analyzed\"\n                    },\n                    \"ci\" : {\n                      \"type\" : \"string\",\n                      \"analyzer\" : \"case_insensitive_analyzer\"\n                    }\n                  }\n                },\n                \"uri\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"not_analyzed\",\n                  \"fields\" : {\n                    \"analyzed\" : {\n                      \"type\" : \"string\",\n                      \"index\" : \"analyzed\"\n                    },\n                    \"ci\" : {\n                      \"type\" : \"string\",\n                      \"analyzer\" : \"case_insensitive_analyzer\"\n                    }\n                  }\n                },\n                \"author\" : {\n                  \"properties\" : {\n                    \"agentText\" : {\n                      \"type\" : \"string\",\n                      \"index\" : \"not_analyzed\",\n                      \"fields\" : {\n                        \"analyzed\" : {\n                          \"type\" : \"string\",\n                          \"index\" : \"analyzed\"\n                        },\n                        \"ci\" : {\n                          \"type\" : \"string\",\n                          \"analyzer\" : \"case_insensitive_analyzer\"\n                        }\n                      }\n                    },\n                    \"fullName\" : {\n                      \"type\" : \"string\",\n                      \"index\" : \"not_analyzed\",\n                      \"fields\" : {\n                        \"analyzed\" : {\n                          \"type\" : \"string\",\n                          \"index\" : \"analyzed\"\n                        },\n                        \"ci\" : {\n                          \"type\" : \"string\",\n                          \"analyzer\" : \"case_insensitive_analyzer\"\n                        },\n                        \"like\" : {\n                          \"type\" : \"string\",\n                          \"analyzer\" : \"like_analyzer\"\n                        }\n                      }\n                    },\n                    \"organization\" : {\n                      \"properties\" : {\n                        \"agentText\" : {\n                          \"type\" : \"string\",\n                          \"index\" : \"not_analyzed\",\n                          \"fields\" : {\n                            \"analyzed\" : {\n                              \"type\" : \"string\",\n                              \"index\" : \"analyzed\"\n                            },\n                            \"ci\" : {\n                              \"type\" : \"string\",\n                              \"analyzer\" : \"case_insensitive_analyzer\"\n                            }\n                          }\n                        },\n                        \"name\" : {\n                          \"type\" : \"string\",\n                          \"index\" : \"not_analyzed\",\n                          \"fields\" : {\n                            \"analyzed\" : {\n                              \"type\" : \"string\",\n                              \"index\" : \"analyzed\"\n                            },\n                            \"ci\" : {\n                              \"type\" : \"string\",\n                              \"analyzer\" : \"case_insensitive_analyzer\"\n                            }\n                          }\n                        }\n                      }\n                    }\n                  }\n                },\n                \"publicationDate\" : {\n                  \"type\" : \"date\"\n                }\n              }\n            },\n            \"experts\" : {\n              \"type\" : \"nested\",\n              \"properties\" : {\n                \"agentText\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"not_analyzed\",\n                  \"fields\" : {\n                    \"analyzed\" : {\n                      \"type\" : \"string\",\n                      \"index\" : \"analyzed\"\n                    },\n                    \"ci\" : {\n                      \"type\" : \"string\",\n                      \"analyzer\" : \"case_insensitive_analyzer\"\n                    }\n                  }\n                },\n                \"fullName\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"not_analyzed\",\n                  \"fields\" : {\n                    \"analyzed\" : {\n                      \"type\" : \"string\",\n                      \"index\" : \"analyzed\"\n                    },\n                    \"ci\" : {\n                      \"type\" : \"string\",\n                      \"analyzer\" : \"case_insensitive_analyzer\"\n                    },\n                    \"like\" : {\n                      \"type\" : \"string\",\n                      \"analyzer\" : \"like_analyzer\"\n                    }\n                  }\n                },\n                \"organization\" : {\n                  \"properties\" : {\n                    \"agentText\" : {\n                      \"type\" : \"string\",\n                      \"index\" : \"not_analyzed\",\n                      \"fields\" : {\n                        \"analyzed\" : {\n                          \"type\" : \"string\",\n                          \"index\" : \"analyzed\"\n                        },\n                        \"ci\" : {\n                          \"type\" : \"string\",\n                          \"analyzer\" : \"case_insensitive_analyzer\"\n                        }\n                      }\n                    },\n                    \"name\" : {\n                      \"type\" : \"string\",\n                      \"index\" : \"not_analyzed\",\n                      \"fields\" : {\n                        \"analyzed\" : {\n                          \"type\" : \"string\",\n                          \"index\" : \"analyzed\"\n                        },\n                        \"ci\" : {\n                          \"type\" : \"string\",\n                          \"analyzer\" : \"case_insensitive_analyzer\"\n                        }\n                      }\n                    }\n                  }\n                }\n              }\n            }\n          }\n        },\n        \"identificationQualifiers\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\",\n              \"index\" : \"analyzed\"\n            },\n            \"ci\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"dateIdentified\" : {\n          \"type\" : \"date\"\n        },\n        \"identifiers\" : {\n          \"type\" : \"nested\",\n          \"properties\" : {\n            \"agentText\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"analyzed\"\n                },\n                \"ci\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            }\n          }\n        }\n      }\n    },\n    \"identifyingEpithets\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\",\n          \"index\" : \"analyzed\"\n        },\n        \"ci\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"theme\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\",\n          \"index\" : \"analyzed\"\n        },\n        \"ci\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    }\n  },\n  \"dynamic\" : \"strict\"\n}\nLog file: \/home\/ayco\/projects\/nba\/v2\/import\/log\/print-mapping.2016_06_24_15_06.log\n{\n  \"properties\" : {\n    \"sourceSystem\" : {\n      \"properties\" : {\n        \"code\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\"\n        },\n        \"name\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\"\n            },\n            \"ignoreCase\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        }\n      }\n    },\n    \"sourceSystemId\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\"\n        },\n        \"ignoreCase\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"recordURI\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\"\n        },\n        \"ignoreCase\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"sourceInstitutionID\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\"\n        },\n        \"ignoreCase\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"sourceID\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\"\n        },\n        \"ignoreCase\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"owner\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\"\n        },\n        \"ignoreCase\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"licenceType\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\"\n        },\n        \"ignoreCase\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"licence\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\"\n        },\n        \"ignoreCase\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"unitID\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\"\n        },\n        \"ignoreCase\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"collectionType\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\"\n        },\n        \"ignoreCase\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"title\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\"\n        },\n        \"ignoreCase\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"caption\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\"\n        },\n        \"ignoreCase\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"description\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\"\n        },\n        \"ignoreCase\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"serviceAccessPoints\" : {\n      \"type\" : \"nested\",\n      \"properties\" : {\n        \"accessUri\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\"\n            },\n            \"ignoreCase\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"format\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\"\n            },\n            \"ignoreCase\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"variant\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\"\n            },\n            \"ignoreCase\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        }\n      }\n    },\n    \"type\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\"\n        },\n        \"ignoreCase\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"taxonCount\" : {\n      \"type\" : \"integer\"\n    },\n    \"creator\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\"\n        },\n        \"ignoreCase\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"copyrightText\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\"\n        },\n        \"ignoreCase\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"associatedSpecimenReference\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\"\n        },\n        \"ignoreCase\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"associatedTaxonReference\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\"\n        },\n        \"ignoreCase\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"specimenTypeStatus\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\"\n        },\n        \"ignoreCase\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"multiMediaPublic\" : {\n      \"type\" : \"boolean\"\n    },\n    \"subjectParts\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\"\n        },\n        \"ignoreCase\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"subjectOrientations\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\"\n        },\n        \"ignoreCase\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"phasesOrStages\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\"\n        },\n        \"ignoreCase\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"sexes\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\"\n        },\n        \"ignoreCase\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"gatheringEvents\" : {\n      \"type\" : \"nested\",\n      \"properties\" : {\n        \"projectTitle\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\"\n            },\n            \"ignoreCase\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"worldRegion\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\"\n            },\n            \"ignoreCase\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"continent\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\"\n            },\n            \"ignoreCase\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"country\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\"\n            },\n            \"ignoreCase\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"iso3166Code\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\"\n            },\n            \"ignoreCase\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"provinceState\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\"\n            },\n            \"ignoreCase\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"island\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\"\n            },\n            \"ignoreCase\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"locality\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\"\n            },\n            \"ignoreCase\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"city\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\"\n            },\n            \"ignoreCase\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"sublocality\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\"\n            },\n            \"ignoreCase\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"localityText\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\"\n            },\n            \"ignoreCase\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            },\n            \"like\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"like_analyzer\"\n            }\n          }\n        },\n        \"dateTimeBegin\" : {\n          \"type\" : \"date\"\n        },\n        \"dateTimeEnd\" : {\n          \"type\" : \"date\"\n        },\n        \"method\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\"\n            },\n            \"ignoreCase\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"altitude\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\"\n            },\n            \"ignoreCase\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"altitudeUnifOfMeasurement\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\"\n            },\n            \"ignoreCase\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"depth\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\"\n            },\n            \"ignoreCase\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"depthUnitOfMeasurement\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\"\n            },\n            \"ignoreCase\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"gatheringPersons\" : {\n          \"type\" : \"nested\",\n          \"properties\" : {\n            \"agentText\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"fullName\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                },\n                \"like\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"like_analyzer\"\n                }\n              }\n            },\n            \"organization\" : {\n              \"properties\" : {\n                \"agentText\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"not_analyzed\",\n                  \"fields\" : {\n                    \"analyzed\" : {\n                      \"type\" : \"string\"\n                    },\n                    \"ignoreCase\" : {\n                      \"type\" : \"string\",\n                      \"analyzer\" : \"case_insensitive_analyzer\"\n                    }\n                  }\n                },\n                \"name\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"not_analyzed\",\n                  \"fields\" : {\n                    \"analyzed\" : {\n                      \"type\" : \"string\"\n                    },\n                    \"ignoreCase\" : {\n                      \"type\" : \"string\",\n                      \"analyzer\" : \"case_insensitive_analyzer\"\n                    }\n                  }\n                }\n              }\n            }\n          }\n        },\n        \"gatheringOrganizations\" : {\n          \"type\" : \"nested\",\n          \"properties\" : {\n            \"agentText\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"name\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            }\n          }\n        },\n        \"siteCoordinates\" : {\n          \"type\" : \"nested\",\n          \"properties\" : {\n            \"longitudeDecimal\" : {\n              \"type\" : \"double\"\n            },\n            \"latitudeDecimal\" : {\n              \"type\" : \"double\"\n            },\n            \"gridCellSystem\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"gridLatitudeDecimal\" : {\n              \"type\" : \"double\"\n            },\n            \"gridLongitudeDecimal\" : {\n              \"type\" : \"double\"\n            },\n            \"gridCellCode\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"gridQualifier\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"point\" : {\n              \"type\" : \"geo_shape\"\n            }\n          }\n        },\n        \"bioStratigraphy\" : {\n          \"type\" : \"nested\",\n          \"properties\" : {\n            \"youngBioDatingQualifier\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngBioName\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngFossilZone\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngFossilSubZone\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngBioCertainty\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngStratType\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"bioDatingQualifier\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"bioPreferredFlag\" : {\n              \"type\" : \"boolean\"\n            },\n            \"rangePosition\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldBioName\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"bioIdentifier\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldFossilzone\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldFossilSubzone\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldBioCertainty\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldBioStratType\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            }\n          }\n        },\n        \"chronoStratigraphy\" : {\n          \"type\" : \"nested\",\n          \"properties\" : {\n            \"youngRegionalSubstage\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngRegionalStage\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngRegionalSeries\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngDatingQualifier\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngInternSystem\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngInternSubstage\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngInternStage\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngInternSeries\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngInternErathem\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngInternEonothem\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngChronoName\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"youngCertainty\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldDatingQualifier\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"chronoPreferredFlag\" : {\n              \"type\" : \"boolean\"\n            },\n            \"oldRegionalSubstage\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldRegionalStage\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldRegionalSeries\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldInternSystem\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldInternSubstage\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldInternStage\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldInternSeries\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldInternErathem\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldInternEonothem\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldChronoName\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"chronoIdentifier\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"oldCertainty\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            }\n          }\n        },\n        \"lithoStratigraphy\" : {\n          \"type\" : \"nested\",\n          \"properties\" : {\n            \"qualifier\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"preferredFlag\" : {\n              \"type\" : \"boolean\"\n            },\n            \"member2\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"member\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"informalName2\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"informalName\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"importedName2\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"importedName1\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"lithoIdentifier\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"formation2\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"formationGroup2\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"formationGroup\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"formation\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"certainty2\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"certainty\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"bed2\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"bed\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            }\n          }\n        }\n      }\n    },\n    \"identifications\" : {\n      \"type\" : \"nested\",\n      \"properties\" : {\n        \"taxonRank\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\"\n            },\n            \"ignoreCase\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"scientificName\" : {\n          \"properties\" : {\n            \"fullScientificName\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"taxonomicStatus\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"genusOrMonomial\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                },\n                \"like\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"like_analyzer\"\n                }\n              }\n            },\n            \"subgenus\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"specificEpithet\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                },\n                \"like\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"like_analyzer\"\n                }\n              }\n            },\n            \"infraspecificEpithet\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"infraspecificMarker\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"nameAddendum\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"authorshipVerbatim\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"author\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"year\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"references\" : {\n              \"type\" : \"nested\",\n              \"properties\" : {\n                \"titleCitation\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"not_analyzed\",\n                  \"fields\" : {\n                    \"analyzed\" : {\n                      \"type\" : \"string\"\n                    },\n                    \"ignoreCase\" : {\n                      \"type\" : \"string\",\n                      \"analyzer\" : \"case_insensitive_analyzer\"\n                    }\n                  }\n                },\n                \"citationDetail\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"not_analyzed\",\n                  \"fields\" : {\n                    \"analyzed\" : {\n                      \"type\" : \"string\"\n                    },\n                    \"ignoreCase\" : {\n                      \"type\" : \"string\",\n                      \"analyzer\" : \"case_insensitive_analyzer\"\n                    }\n                  }\n                },\n                \"uri\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"not_analyzed\",\n                  \"fields\" : {\n                    \"analyzed\" : {\n                      \"type\" : \"string\"\n                    },\n                    \"ignoreCase\" : {\n                      \"type\" : \"string\",\n                      \"analyzer\" : \"case_insensitive_analyzer\"\n                    }\n                  }\n                },\n                \"author\" : {\n                  \"properties\" : {\n                    \"agentText\" : {\n                      \"type\" : \"string\",\n                      \"index\" : \"not_analyzed\",\n                      \"fields\" : {\n                        \"analyzed\" : {\n                          \"type\" : \"string\"\n                        },\n                        \"ignoreCase\" : {\n                          \"type\" : \"string\",\n                          \"analyzer\" : \"case_insensitive_analyzer\"\n                        }\n                      }\n                    },\n                    \"fullName\" : {\n                      \"type\" : \"string\",\n                      \"index\" : \"not_analyzed\",\n                      \"fields\" : {\n                        \"analyzed\" : {\n                          \"type\" : \"string\"\n                        },\n                        \"ignoreCase\" : {\n                          \"type\" : \"string\",\n                          \"analyzer\" : \"case_insensitive_analyzer\"\n                        },\n                        \"like\" : {\n                          \"type\" : \"string\",\n                          \"analyzer\" : \"like_analyzer\"\n                        }\n                      }\n                    },\n                    \"organization\" : {\n                      \"properties\" : {\n                        \"agentText\" : {\n                          \"type\" : \"string\",\n                          \"index\" : \"not_analyzed\",\n                          \"fields\" : {\n                            \"analyzed\" : {\n                              \"type\" : \"string\"\n                            },\n                            \"ignoreCase\" : {\n                              \"type\" : \"string\",\n                              \"analyzer\" : \"case_insensitive_analyzer\"\n                            }\n                          }\n                        },\n                        \"name\" : {\n                          \"type\" : \"string\",\n                          \"index\" : \"not_analyzed\",\n                          \"fields\" : {\n                            \"analyzed\" : {\n                              \"type\" : \"string\"\n                            },\n                            \"ignoreCase\" : {\n                              \"type\" : \"string\",\n                              \"analyzer\" : \"case_insensitive_analyzer\"\n                            }\n                          }\n                        }\n                      }\n                    }\n                  }\n                },\n                \"publicationDate\" : {\n                  \"type\" : \"date\"\n                }\n              }\n            },\n            \"experts\" : {\n              \"type\" : \"nested\",\n              \"properties\" : {\n                \"agentText\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"not_analyzed\",\n                  \"fields\" : {\n                    \"analyzed\" : {\n                      \"type\" : \"string\"\n                    },\n                    \"ignoreCase\" : {\n                      \"type\" : \"string\",\n                      \"analyzer\" : \"case_insensitive_analyzer\"\n                    }\n                  }\n                },\n                \"fullName\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"not_analyzed\",\n                  \"fields\" : {\n                    \"analyzed\" : {\n                      \"type\" : \"string\"\n                    },\n                    \"ignoreCase\" : {\n                      \"type\" : \"string\",\n                      \"analyzer\" : \"case_insensitive_analyzer\"\n                    },\n                    \"like\" : {\n                      \"type\" : \"string\",\n                      \"analyzer\" : \"like_analyzer\"\n                    }\n                  }\n                },\n                \"organization\" : {\n                  \"properties\" : {\n                    \"agentText\" : {\n                      \"type\" : \"string\",\n                      \"index\" : \"not_analyzed\",\n                      \"fields\" : {\n                        \"analyzed\" : {\n                          \"type\" : \"string\"\n                        },\n                        \"ignoreCase\" : {\n                          \"type\" : \"string\",\n                          \"analyzer\" : \"case_insensitive_analyzer\"\n                        }\n                      }\n                    },\n                    \"name\" : {\n                      \"type\" : \"string\",\n                      \"index\" : \"not_analyzed\",\n                      \"fields\" : {\n                        \"analyzed\" : {\n                          \"type\" : \"string\"\n                        },\n                        \"ignoreCase\" : {\n                          \"type\" : \"string\",\n                          \"analyzer\" : \"case_insensitive_analyzer\"\n                        }\n                      }\n                    }\n                  }\n                }\n              }\n            }\n          }\n        },\n        \"defaultClassification\" : {\n          \"properties\" : {\n            \"kingdom\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"phylum\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"className\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"order\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"superFamily\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"family\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"genus\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"subgenus\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"specificEpithet\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"infraspecificEpithet\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"infraspecificRank\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            }\n          }\n        },\n        \"systemClassification\" : {\n          \"type\" : \"nested\",\n          \"properties\" : {\n            \"rank\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"name\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            }\n          }\n        },\n        \"vernacularNames\" : {\n          \"type\" : \"nested\",\n          \"properties\" : {\n            \"name\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"language\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            },\n            \"preferred\" : {\n              \"type\" : \"boolean\"\n            },\n            \"references\" : {\n              \"type\" : \"nested\",\n              \"properties\" : {\n                \"titleCitation\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"not_analyzed\",\n                  \"fields\" : {\n                    \"analyzed\" : {\n                      \"type\" : \"string\"\n                    },\n                    \"ignoreCase\" : {\n                      \"type\" : \"string\",\n                      \"analyzer\" : \"case_insensitive_analyzer\"\n                    }\n                  }\n                },\n                \"citationDetail\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"not_analyzed\",\n                  \"fields\" : {\n                    \"analyzed\" : {\n                      \"type\" : \"string\"\n                    },\n                    \"ignoreCase\" : {\n                      \"type\" : \"string\",\n                      \"analyzer\" : \"case_insensitive_analyzer\"\n                    }\n                  }\n                },\n                \"uri\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"not_analyzed\",\n                  \"fields\" : {\n                    \"analyzed\" : {\n                      \"type\" : \"string\"\n                    },\n                    \"ignoreCase\" : {\n                      \"type\" : \"string\",\n                      \"analyzer\" : \"case_insensitive_analyzer\"\n                    }\n                  }\n                },\n                \"author\" : {\n                  \"properties\" : {\n                    \"agentText\" : {\n                      \"type\" : \"string\",\n                      \"index\" : \"not_analyzed\",\n                      \"fields\" : {\n                        \"analyzed\" : {\n                          \"type\" : \"string\"\n                        },\n                        \"ignoreCase\" : {\n                          \"type\" : \"string\",\n                          \"analyzer\" : \"case_insensitive_analyzer\"\n                        }\n                      }\n                    },\n                    \"fullName\" : {\n                      \"type\" : \"string\",\n                      \"index\" : \"not_analyzed\",\n                      \"fields\" : {\n                        \"analyzed\" : {\n                          \"type\" : \"string\"\n                        },\n                        \"ignoreCase\" : {\n                          \"type\" : \"string\",\n                          \"analyzer\" : \"case_insensitive_analyzer\"\n                        },\n                        \"like\" : {\n                          \"type\" : \"string\",\n                          \"analyzer\" : \"like_analyzer\"\n                        }\n                      }\n                    },\n                    \"organization\" : {\n                      \"properties\" : {\n                        \"agentText\" : {\n                          \"type\" : \"string\",\n                          \"index\" : \"not_analyzed\",\n                          \"fields\" : {\n                            \"analyzed\" : {\n                              \"type\" : \"string\"\n                            },\n                            \"ignoreCase\" : {\n                              \"type\" : \"string\",\n                              \"analyzer\" : \"case_insensitive_analyzer\"\n                            }\n                          }\n                        },\n                        \"name\" : {\n                          \"type\" : \"string\",\n                          \"index\" : \"not_analyzed\",\n                          \"fields\" : {\n                            \"analyzed\" : {\n                              \"type\" : \"string\"\n                            },\n                            \"ignoreCase\" : {\n                              \"type\" : \"string\",\n                              \"analyzer\" : \"case_insensitive_analyzer\"\n                            }\n                          }\n                        }\n                      }\n                    }\n                  }\n                },\n                \"publicationDate\" : {\n                  \"type\" : \"date\"\n                }\n              }\n            },\n            \"experts\" : {\n              \"type\" : \"nested\",\n              \"properties\" : {\n                \"agentText\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"not_analyzed\",\n                  \"fields\" : {\n                    \"analyzed\" : {\n                      \"type\" : \"string\"\n                    },\n                    \"ignoreCase\" : {\n                      \"type\" : \"string\",\n                      \"analyzer\" : \"case_insensitive_analyzer\"\n                    }\n                  }\n                },\n                \"fullName\" : {\n                  \"type\" : \"string\",\n                  \"index\" : \"not_analyzed\",\n                  \"fields\" : {\n                    \"analyzed\" : {\n                      \"type\" : \"string\"\n                    },\n                    \"ignoreCase\" : {\n                      \"type\" : \"string\",\n                      \"analyzer\" : \"case_insensitive_analyzer\"\n                    },\n                    \"like\" : {\n                      \"type\" : \"string\",\n                      \"analyzer\" : \"like_analyzer\"\n                    }\n                  }\n                },\n                \"organization\" : {\n                  \"properties\" : {\n                    \"agentText\" : {\n                      \"type\" : \"string\",\n                      \"index\" : \"not_analyzed\",\n                      \"fields\" : {\n                        \"analyzed\" : {\n                          \"type\" : \"string\"\n                        },\n                        \"ignoreCase\" : {\n                          \"type\" : \"string\",\n                          \"analyzer\" : \"case_insensitive_analyzer\"\n                        }\n                      }\n                    },\n                    \"name\" : {\n                      \"type\" : \"string\",\n                      \"index\" : \"not_analyzed\",\n                      \"fields\" : {\n                        \"analyzed\" : {\n                          \"type\" : \"string\"\n                        },\n                        \"ignoreCase\" : {\n                          \"type\" : \"string\",\n                          \"analyzer\" : \"case_insensitive_analyzer\"\n                        }\n                      }\n                    }\n                  }\n                }\n              }\n            }\n          }\n        },\n        \"identificationQualifiers\" : {\n          \"type\" : \"string\",\n          \"index\" : \"not_analyzed\",\n          \"fields\" : {\n            \"analyzed\" : {\n              \"type\" : \"string\"\n            },\n            \"ignoreCase\" : {\n              \"type\" : \"string\",\n              \"analyzer\" : \"case_insensitive_analyzer\"\n            }\n          }\n        },\n        \"dateIdentified\" : {\n          \"type\" : \"date\"\n        },\n        \"identifiers\" : {\n          \"type\" : \"nested\",\n          \"properties\" : {\n            \"agentText\" : {\n              \"type\" : \"string\",\n              \"index\" : \"not_analyzed\",\n              \"fields\" : {\n                \"analyzed\" : {\n                  \"type\" : \"string\"\n                },\n                \"ignoreCase\" : {\n                  \"type\" : \"string\",\n                  \"analyzer\" : \"case_insensitive_analyzer\"\n                }\n              }\n            }\n          }\n        }\n      }\n    },\n    \"identifyingEpithets\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\"\n        },\n        \"ignoreCase\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    },\n    \"theme\" : {\n      \"type\" : \"string\",\n      \"index\" : \"not_analyzed\",\n      \"fields\" : {\n        \"analyzed\" : {\n          \"type\" : \"string\"\n        },\n        \"ignoreCase\" : {\n          \"type\" : \"string\",\n          \"analyzer\" : \"case_insensitive_analyzer\"\n        }\n      }\n    }\n  },\n  \"dynamic\" : \"strict\"\n}\n```\n\n<\/details>\n"},{"date":"2016-07-06T09:47:39Z","author":"KarimJedda","text":"I was looking for an answer to the same issue, and the snippet you just pasted broke my finger.\n"},{"date":"2016-07-06T12:44:00Z","author":"clintongormley","text":"@ayco-at-naturalis i've added `<details>` tags to your incredibly long comment.  I think your issue is probably to do with a change to the default `distance_error_pct` (see https:\/\/github.com\/elastic\/elasticsearch\/issues\/17907)\n"},{"date":"2016-07-11T07:15:11Z","author":"ayco-at-naturalis","text":"@clintongormley  Thanks for your reply and sorry for the amount of text. It does seem though as if it is indeed the (deep) nesting that is causing the trouble - not de geo stuff that you mention. As soon as I stopped providing data (while indexing) voor \"nested\" objects (which themselves contain \"nested\" objects), perfomance went back to what it was in ES 1.3.4\n\nOf course, since that data is not optional, I would still need to load that data in a separate round.\n\nNote that it isn't the mapping itself that makes it slow. It's when you actually provide data for the nested structures defined in the mapping.\n"},{"date":"2016-07-11T13:51:23Z","author":"clintongormley","text":"> it is indeed the (deep) nesting that is causing the trouble - not de geo stuff that you mention\n\nmy mistake, the geo issue only kicks in if you specify a `precision`\n"}],"reopen_on":"2015-12-04T12:14:56Z","opened_by":"ksundeepsatya","closed_on":"2015-12-04T20:18:40Z","description":"I am upgrading from Elasticsearch 1.7.1 to 2.1. I have few test cases that I wanted to validate before upgrading. Attached [BulkProcessorTest.txt](https:\/\/github.com\/elastic\/elasticsearch\/files\/51364\/BulkProcessorTest.txt) is one among them.\n\nHere are the steps\n1) create object with random values and corresponding JSONs are 3K Bytes. \n2) Index 200 such records (could be any number) to index \"test\"\n3) Drop index \"test\"\n4) Index 200K Records to index \"test\"\n\nIn 1.7.1 step 4 used to take ~40 seconds.\nIn 2.1.0 step 4 is taking ~600 seconds\nI reran the same test on 2.0.1 it is taking ~40 seconds. \n\nIf step 3 is not executed Step 4 takes only ~40 seconds in 2.1.0.\n\nI saw lot of these warnings when the performance was slow: \nhigh disk watermark [90%] exceeded on [ZkeRJaTUR4iu3h8DmIiV2w][Thing]... free: 11gb[4.7%], shards will be relocated away from this node.\nAlso, disk and cpu utilization were significantly higher.\n\nThese warnings and high utilization  were not seen when step 3 was not executed 2.1.0.\n","id":"120284629","title":"Indexing rate is ~10X lower in Elasticsearch 2.1","reopen_by":"clintongormley","opened_on":"2015-12-03T22:24:39Z","closed_by":"jasontedor"},{"number":"15061","comments":[{"date":"2015-11-28T11:11:30Z","author":"clintongormley","text":"@bleskes could you take a look at this please\n"},{"date":"2015-11-30T15:41:11Z","author":"bleskes","text":"yeah, sadly this is how things work today. When we recover a primary, we reach out to all the nodes in the cluster and select the best copy we find (in order to make sure we don't select stale copies). In order to make this effective we require to see at least a quorum of the possible copies (i.e., number of replicas +1 ) with a special case for 2.  Since the number of replicas is increased and the cluster is shut down before ES is able to make more copies, a new primary is not elected. \n\nThis will be fixed once we complete the work on #14739 . I'm closing this for now, as a duplicate of #14739 where github nicely refers back to this. Feel free to reopen if there is anything else.\n"},{"date":"2015-11-30T15:49:11Z","author":"bleskes","text":"sorry, mis read the ticket and thought you had more then 1 replica. Re-opening for further investigation. \n"},{"date":"2015-12-01T12:14:56Z","author":"bleskes","text":"I tried to reproduce but I couldn't, @jakommo are you sure you used just one replica in your example in # 4 ? if it was 2 what I said above would explain it.\n"},{"date":"2015-12-01T13:46:45Z","author":"jakommo","text":"@bleskes you are right, sorry. I initially tried it with 4 nodes and num of replicas set to 3.\nJust tried it again with 3 nodes and it doesn't happen with replicas set to 1, but with replicas 2.\n"},{"date":"2015-12-01T13:49:13Z","author":"bleskes","text":"OK. Good.  See my explanation above.  Closing again :)\n"}],"reopen_on":"2015-11-30T15:49:11Z","opened_by":"jakommo","closed_on":"2015-12-01T13:49:13Z","description":"If a cluster is shutdown (all nodes at once), while there is a process running to increase the number of replicas, the index stays in a `UNASSIGNED` state with no messages in the logs.\nIf the number of replica is changed again the index comes back.\n\nSteps to reproduce:\n1. spin up a cluster with at least 3 nodes\n2. Index some data (should be enough data for the replicas assignment to take some time) \n3. Change replicas to 0 `curl -XPUT 'localhost:9200\/_settings' -d '{ \"index\" : {\"number_of_replicas\" : 0 }}'`\n4. Raise number of replicas `curl -XPUT 'localhost:9200\/_settings' -d '{ \"index\" : {\"number_of_replicas\" : 1 }}'` and shut down all nodes before the replicas have been fully assigned.\n5. Start all nodes again\n6.  The Index will stay in an `UNASSIGNED` state, but there is nothing in the logs on why this is the case.\n7. Changing the number of replicas to 0 again, will bring back the index into a `STARTED` state.\n\nReproduced on 2.0 and 2.1.\n","id":"119180054","title":"Index stays in UNASSIGNED state, if cluster was shutdown while a process of changing the replicas was running.","reopen_by":"bleskes","opened_on":"2015-11-27T11:21:56Z","closed_by":"bleskes"},{"number":"14837","comments":[{"date":"2015-11-18T18:38:59Z","author":"clintongormley","text":"See https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/breaking_20_mapping_changes.html#_field_names_may_not_contain_dots\n"},{"date":"2015-11-18T18:43:07Z","author":"asaf","text":"@clintongormley \n\nThis is not the case,\nThe doc contains no dots, it contains nested properties,\n\nI am talking about query templates,\n\nElastic probably performs \".\" field validation on the query template too and it restricts the query to be persisted, but from my understanding, the \"nested\" filter acts like the nested matcher, which as stated requires \".\" notation,\n\n```\nA nested filter behaves much like a nested query, except that it doesn\u2019t accept the score_mode \nparameter. It can be used only in filter context\u2014such as inside a filtered query\u2014and it behaves like any \nother filter: it includes or excludes, but it doesn\u2019t score.\n```\n\nPlease re-open :)\n\nThanks.\n"},{"date":"2015-11-18T18:58:44Z","author":"clintongormley","text":"Ah read way too fast, apologies!\n"},{"date":"2015-11-18T19:08:00Z","author":"clintongormley","text":"A simpler recreation:\n\n```\nPUT _search\/template\/foo\n{\n  \"query\": {\n    \"match\": {\n      \"foo.bar\": \"TEXT\"\n    }\n  }\n}\n```\n"},{"date":"2015-11-18T19:17:05Z","author":"asaf","text":"@clintongormley :+1: \n"},{"date":"2015-11-25T13:01:11Z","author":"MaineC","text":"From a first look at the code the problem seems to be that on indexing the template query we try to parse it as a regular document which includes trying to create a mapping for it. This fails for the reason @clintongormley mentioned earlier (code that complains: https:\/\/github.com\/elastic\/elasticsearch\/blob\/debfb84d38e8d24755996032fa2bdd2b81f7068f\/core\/src\/main\/java\/org\/elasticsearch\/index\/mapper\/object\/ObjectMapper.java#L262 ).\n\nNot yet sure how to fix this issue though.\n"},{"date":"2015-11-25T19:20:13Z","author":"rjernst","text":"Why do we index the query template? Is it just for storage (although then I'm confused why it isn't just part of the cluster state)? Can we store it as a blob in a single field instead of using the structure from the query as the document?\n"},{"date":"2015-11-26T09:33:50Z","author":"MaineC","text":"@rjernst AFAIK it is just for storage (did a bit of digging, the original issue #5921 also suggests that).\n\n> (although then I'm confused why it isn't just part of the cluster state)\n\nNo idea. Maybe @clintongormley or @s1monw can comment on that.\n\n> Can we store it as a blob in a single field instead of using the structure from the query as the \n> document?\n\nIMHO that would make more sense than the current implementation.\n"},{"date":"2015-11-28T10:17:14Z","author":"clintongormley","text":"OK the root cause of this appears to be because indexed search templates can be passed without the outer `template` element (which is undocumented).\n\nIf you use the documented syntax:\n\n```\nPUT _search\/template\/foo\n{\n  \"template\": {\n    \"query\": {\n      \"match_all\": {}\n    }\n  }\n}\n```\n\nThen the `.scripts` index contains the following mapping:\n\n```\n{\n  \".scripts\": {\n    \"mappings\": {\n      \"mustache\": {\n        \"properties\": {\n          \"script\": {\n            \"type\": \"object\",\n            \"enabled\": false\n          },\n          \"template\": {\n            \"type\": \"object\",\n            \"enabled\": false\n          }\n        }\n      }\n    }\n  }\n}\n```\n\nThe `query` field is correctly set to `enabled: false`, so nothing in the query is indexed or even parsed.\n\nHowever, without the outer `template` property:\n\n```\nPUT _search\/template\/foo\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n```\n\nThen every element in the query generates a field:\n\n```\n{\n  \".scripts\": {\n    \"mappings\": {\n      \"mustache\": {\n        \"properties\": {\n          \"query\": {\n            \"properties\": {\n              \"match_all\": {\n                \"type\": \"object\"\n              }\n            }\n          },\n          \"script\": {\n            \"type\": \"object\",\n            \"enabled\": false\n          },\n          \"template\": {\n            \"type\": \"object\",\n            \"enabled\": false\n          }\n        }\n      }\n    }\n  }\n}\n```\n\nSo the solution here is to only accept a `template` property for indexed templates, and a `script` property for indexed scripts.\n"},{"date":"2015-11-29T11:56:02Z","author":"clintongormley","text":"Related to https:\/\/github.com\/elastic\/elasticsearch\/issues\/13729\n"},{"date":"2015-12-09T17:31:19Z","author":"brwe","text":"There are two ways to index scripts:\n1. `PUT _search\/template\/foo`\n2. `PUT .scripts\/mustache\/foo`\n\nFor the first we could add extra parsing logic somewhere that ensures that we start with \"template\" only and nothing else is allowed. For the second we would need a dedicated rest endpoint because currently this is just the regular index API that performs no checks whatsoever except for adding the \"_default_\" mapping here: https:\/\/github.com\/elastic\/elasticsearch\/blob\/master\/core\/src\/main\/java\/org\/elasticsearch\/index\/mapper\/MapperService.java#L136.\n\nThe trouble is that people can still use 2. with the java API and I have no idea how to hack the index API without being yelled at on github when I make the pull request :-) \n"},{"date":"2015-12-10T13:09:19Z","author":"brwe","text":"Discussed with @clintongormley and we decided to fix this only for the _search\/template endpoint and open a separate issue for the index api and discuss there\n"},{"date":"2016-02-14T17:30:05Z","author":"clintongormley","text":"Related to #16651\n"},{"date":"2016-04-12T14:10:43Z","author":"avalonabecker","text":"I got the Field name [myChild.name] cannot contain '.' when I forgot to put the \"_search\" at the end of my URL, querying for a nested field using the dot syntax from the documentation [here](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/guide\/current\/nested-query.html)  - just posting so others might save time on a newbie mistake there.\n"},{"date":"2016-05-07T15:37:21Z","author":"clintongormley","text":"Closed by https:\/\/github.com\/elastic\/elasticsearch\/issues\/16651\n"}],"reopen_on":"2015-11-18T18:58:44Z","opened_by":"asaf","closed_on":"2016-05-07T15:37:21Z","description":"Hey,\n\nI have a query template that looks like:\n\n``` json\n{\n  \"query\": {\n    \"filtered\": {\n      \"filter\": {\n        \"nested\": {\n          \"path\": \"myChild\",\n          \"filter\": {\n            \"bool\": {\n              \"must\": [\n                {\n                  \"term\": {\n                    \"myChild.name\": \"{{name}}\"\n                  }\n                },\n                .......... more\n              ]\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\nSaving it causes:\n\n```\n{\"msg\":\"[mapper_parsing_exception] Field name [myChild.name] cannot contain '.'\n```\n\nBut running the exact same query not as template works as expected,\n\nWhat is wrong?\n\nbtw this happens with `Elastic Search v2` (works fine with 1.7)\n\nThanks,\n\nAsaf.\n","id":"117610589","title":"Query Template: \"Field name [myChild.name] cannot contain '.'","reopen_by":"clintongormley","opened_on":"2015-11-18T15:26:39Z","closed_by":"clintongormley"},{"number":"14573","comments":[{"date":"2015-11-09T11:36:07Z","author":"clintongormley","text":"You're specifying the custom config file location incorrectly.  \n\nSee https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/breaking_20_setting_changes.html#_custom_config_file\n"},{"date":"2015-11-09T19:05:07Z","author":"kt97679","text":"Hi @clintongormley , thanks for quick response. As you can see from the description I've provided I was using option -Ddefault.path.conf. I tried again same command with option --path.conf. There was no exception because of config access issue, but I had to specify also --path.data and --path.logs because for some reason those settings were ignored in the config I've provided. In my config I also specify nonstandard ports to use and those settings are also not used. Any advise what can be wrong?\n\nThanks,\nKirill.\n"},{"date":"2015-11-09T19:23:19Z","author":"kt97679","text":"Looks like config is ignored completely. If I specify all options via command line I still get exception like above:\n\n<pre>\n# sudo -u elasticsearch \/usr\/share\/elasticsearch\/bin\/elasticsearch --path.conf=\/etc\/tribe-elasticsearch\/ --path.logs=\/var\/log\/elasticsearch --path.data=\/var\/lib\/elasticsearch\/ --transport.tcp.port=9301 --http.port=9201 --network.host=0.0.0.0 --tribels.cluster.name=logstash-data --tribe.els.discovery.zen.ping.multicast.enabled=false --tribe.els.discovery.zen.ping.unicast.hosts=[\"10.128.69.48\",\"10.128.75.237\"]                                                                                                              \nlog4j:WARN No appenders could be found for logger (bootstrap).\nlog4j:WARN Please initialize the log4j system properly.\nlog4j:WARN See http:\/\/logging.apache.org\/log4j\/1.2\/faq.html#noconfig for more info.\nException in thread \"main\" java.security.AccessControlException: access denied (\"java.io.FilePermission\" \"\/usr\/share\/elasticsearch\/config\/elasticsearch.yml\" \"read\")\n        at java.security.AccessControlContext.checkPermission(AccessControlContext.java:457)\n        at java.security.AccessController.checkPermission(AccessController.java:884)\n        at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)\n        at java.lang.SecurityManager.checkRead(SecurityManager.java:888)\n        at sun.nio.fs.UnixPath.checkRead(UnixPath.java:795)\n        at sun.nio.fs.UnixFileSystemProvider.checkAccess(UnixFileSystemProvider.java:290)\n        at java.nio.file.Files.exists(Files.java:2385)\n        at org.elasticsearch.node.internal.InternalSettingsPreparer.prepareEnvironment(InternalSettingsPreparer.java:87)\n        at org.elasticsearch.node.Node.<init>(Node.java:128)\n        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)\n        at org.elasticsearch.tribe.TribeService.<init>(TribeService.java:136)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:422)\n        at <<<guice>>>\n        at org.elasticsearch.node.Node.<init>(Node.java:198)\n        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)\n        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:170)\n        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)\n        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)\n<\/pre>\n"},{"date":"2015-11-17T12:55:24Z","author":"clintongormley","text":"Thanks for persisting.  I've managed to replicate this and it is indeed a bug.\n\nWhen the tribe node attempts to instantiate a node for the tribe service, it checks for access to the config directory, but that setting is no longer available to it and so it defaults to checking for path.home.\n\nThis can be replicated with a simple config file, saved as `foo\/elasticsearch.yml`:\n\n```\nnode.name: foo\n\ntribe:\n    foo:\n        cluster.name: bar\n```\n\nStart elasticsearch as:\n\n```\n.\/elasticsearch-2.0.0\/bin\/elasticsearch --path.conf foo\/\n```\n\nAnd it fails with:\n\n```\n[2015-11-17 13:54:47,763][INFO ][node                     ] [foo] version[2.0.0], pid[5940], build[de54438\/2015-10-22T08:09:48Z]\n[2015-11-17 13:54:47,763][INFO ][node                     ] [foo] initializing ...\n[2015-11-17 13:54:47,836][INFO ][plugins                  ] [foo] loaded [], sites []\nException in thread \"main\" java.security.AccessControlException: access denied (\"java.io.FilePermission\" \"\/Users\/clinton\/workspace\/servers\/elasticsearch-2.0.0\/config\/elasticsearch.yml\" \"read\")\n  at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)\n  at java.security.AccessController.checkPermission(AccessController.java:884)\n  at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)\n  at java.lang.SecurityManager.checkRead(SecurityManager.java:888)\n  at sun.nio.fs.UnixPath.checkRead(UnixPath.java:795)\n  at sun.nio.fs.UnixFileSystemProvider.checkAccess(UnixFileSystemProvider.java:290)\n  at java.nio.file.Files.exists(Files.java:2385)\n  at org.elasticsearch.node.internal.InternalSettingsPreparer.prepareEnvironment(InternalSettingsPreparer.java:87)\n  at org.elasticsearch.node.Node.<init>(Node.java:128)\n  at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)\n  at org.elasticsearch.tribe.TribeService.<init>(TribeService.java:136)\n  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n  at java.lang.reflect.Constructor.newInstance(Constructor.java:422)\n  at <<<guice>>>\n  at org.elasticsearch.node.Node.<init>(Node.java:198)\n  at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)\n  at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:170)\n  at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)\n  at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)\n```\n"},{"date":"2015-11-17T13:02:57Z","author":"clintongormley","text":"@javanna could you take a look at this please?\n"},{"date":"2015-11-17T18:36:50Z","author":"javanna","text":"I had a look at this. Only selected settings are forwarded to the inner tribe clients from the tribe node. `path.home` is one of them but `path.conf` is not. That said, if I remember correctly the tribe clients shouldn't read from configuration file (and sysprops) but only inherit a few settings from the parent node (like it happens in `TribeService`), something that we had enforced with #9721. I think something got lost with #13383 where `loadConfigSettings` was removed, which was our way to prevent loading anything from the config file. With that set to `false` I believe we wouldn't even check for the existence of the file, thus we wouldn't need any permission for that. At this point it seems to me that we would have to forward `path.conf` to the tribe clients just because we are going to check for its existence at some point although we have nothing to load from it (otherwise we check for path.home that we have no permissions for)? I think I'd need @rjernst to verify if what I explained makes any sense, it might be that I overlooked something.\n"},{"date":"2015-11-17T21:21:02Z","author":"rjernst","text":"If I understand the tribe node correctly, it is no different than any other client node (well, creating multiple client nodes internally). So to me, it should be passing along any settings it needs to configure the node (including `path.conf`).  However, I'm not sure what this has to do with the transport client? The transport client by definition now does not use the config file settings (and the stack trace shown above indicates the exception was from building a node, not a transport client).\n"},{"date":"2015-11-18T17:50:54Z","author":"javanna","text":"> However, I'm not sure what this has to do with the transport client?\n\n@rjernst it doesn't have to do directly with the transport client, but the inner tribe nodes have a similar requirement when it comes to loading from config file. They should not be reading out of the config file but only inherit some selected settings from their \"parent\" node (the actual tribe node), and that is why we were previously setting `loadConfigSettings` to `false`, which is now removed though. If my analysis is correct security manager barfs because we check if the config file exists while creating inner client nodes as part of `TribeService`, but we shouldn't need to read from that file at that point anyway. I could forward  the `path.conf` setting to the client nodes too, but I feel it is not the right fix given that we should not be reading from that file nor check if it exists. Not sure what the right fix is though.\n"},{"date":"2015-11-19T14:28:21Z","author":"javanna","text":"I looked deeper, I can confirm this is not just a problem around passing in the right `path.conf` to the inner nodes. The inner client nodes must not read from the main configuration file, something that was fixed in #9721. The option to not load from config settings for a node was though removed with #13383. I had expected `TribeUnitTests` to fail after that change but it doesn't unfortunately. If you try setting for instance `transport.tcp.port` in the configuration file, the tribe node will get that port, but the inner nodes will try to get that one too and will fail. The inner nodes should only get some selected settings from their parent node but never read from config file or system properties.\n"},{"date":"2015-11-19T15:21:35Z","author":"ESamir","text":"+1\nRemoving the path.conf did not resolve the issue \n\nthe config used \n\n```\nbootstrap:\n  mlockall: true\ncluster:\n  name: tribe.elk.h2.com\ndiscovery:\n  zen:\n    minimum_master_nodes: 2\n    ping:\n      unicast:\n        hosts:\n             - h2-clt01\n             - h2-clt02\n             - h2-clt03\nnetwork:\n  host: h2-clt01\nnode:\n  data: false\n  master: true\n  name: h2-ct01-h2-ct01\npath:\n  data: \/data\/h2-ct01\ntribe:\n  h2:\n    cluster:\n      name: elk.h2.com\n    discovery:\n      zen:\n        ping:\n          unicast:\n            hosts:\n                 - h2-cm01\n                 - h2-cm02\n                 - h2-cm03\n  h3:\n    cluster:\n      name: elk.h3.com\n    discovery:\n      zen:\n        ping:\n          unicast:\n            hosts:\n                 - h3-cm01\n                 - h3-cm02\n                 - h3-cm03\n```\n"},{"date":"2015-11-19T15:42:04Z","author":"clintongormley","text":"There is a workaround for this bug.  Assuming your tribe config directory is `\/etc\/tribe\/`:\n\n```\ncd \/etc\ncp -a \/etc\/tribe \/etc\/tribe-client\necho \"\" > \/etc\/tribe-client\/elasticsearch.yml\nchown -R elasticsearch \/etc\/tribe-client\n```\n\nThen edit  `\/etc\/tribe\/elasticsearch.yml` and specify a `path.conf` for each tribe cluster, eg:\n\n```\n# arbitrary config\ntransport.tcp.port: 9301\nhttp.port: 9201\nnetwork.host: 0.0.0.0\npath.data: \/var\/lib\/elasticsearch\/\npath.logs: \/var\/log\/elasticsearch\/\n\ntribe:\n    kibana:\n        path.conf: \/etc\/tribe-client  ### ADD THIS LINE\n        cluster.name: logstash-kibana\n        discovery.zen.ping.multicast.enabled: false\n        discovery.zen.ping.unicast.hosts: [\"127.0.0.1\"]\n    els:\n        path.conf: \/etc\/tribe-client  ### ADD THIS LINE\n        cluster.name: logstash-data\n        discovery.zen.ping.multicast.enabled: false\n        discovery.zen.ping.unicast.hosts: [\"10.128.69.48\", \"10.128.75.237\"]\n```\n\nThen start elasticsearch as:\n\n```\n.\/bin\/elasticsearch --path.conf \/etc\/tribe\n```\n\nThe tribe node will use `\/etc\/tribe\/` as its config directory.  Then the tribe node starts a node client for each cluster, and will use `\/etc\/tribe-client` as its config directory, but `\/etc\/tribe-client\/elasticsearch.yml` is empty, so no settings will be loaded.\n"},{"date":"2015-11-19T16:24:43Z","author":"javanna","text":"Workaround above works, the only caveat is that depending on where the additional empty configuration file is located, we might not have the permissions to read from it. I think it should work if we simply add an empty configuration file under the tribe node config and point right to it, not only specifying its parent directory but the complete path that includes the filename:\n\n```\ntribe.t1.path.conf: \/path\/to\/config\/tribe.yml\ntribe.t2.path.conf: \/path\/to\/config\/tribe.yml\n```\n"},{"date":"2015-11-19T17:48:42Z","author":"ppf2","text":"Ran into this last night when attempting to set up a tribe node on 2.0.  This will also affect users who attempt to set a custom transport.tcp.port for the tribe node.  In this case, setting a custom transport.tcp.port for the tribe node causes a misleading `BindException[Address already in use];` exception when the port specified is not actually already in use.\n\n```\ncluster.name: elasticsearch_2_0_0_tribe_cluster\nnetwork.host: 127.0.0.1\ntransport.tcp.port: 11111\nnode.name: tribe_cluster_node1\ntribe:\n  t1:\n    cluster.name: elasticsearch_2_0_0_cluster1\n  t2:\n    cluster.name: elasticsearch_2_0_0_cluster2\n```\n\nSettings for the 2 clusters:\n\n```\ncluster.name: elasticsearch_2_0_0_cluster2\nnetwork.host: 127.0.0.1\ntransport.tcp.port: 9301\nhttp.port: 9201\nnode.name: cluster2_node1\n```\n\nand \n\n```\ncluster.name: elasticsearch_2_0_0_cluster1\nnetwork.host: 127.0.0.1\ntransport.tcp.port: 9300\nhttp.port: 9200\nnode.name: cluster1_node1\n```\n\nThe problem is that the tribe node will not start up as long as I have the transport.tcp.port: 11111 in place.  If I don't set a custom transport port for the tribe node, it starts up fine and can connect with the 2 clusters.\n\nThe following is the error that shows up when I attempt to set transport.tcp.port for the tribe node.  Note that prior to starting the tribe node, I used lsof to confirm that there's no process on the machine using port 11111 (and it doesn't matter what port I set it to, as long as transport.tcp.port is set for the tribe node, it will throw the same bind exception).\n\n```\n[2015-11-19 01:09:12,816][DEBUG][discovery.zen.elect      ] [tribe_cluster_node1\/t1] using minimum_master_nodes [-1]\n\n[2015-11-19 01:09:12,816][DEBUG][discovery.zen.ping.unicast] [tribe_cluster_node1\/t1] using initial hosts [127.0.0.1, [::1]], with concurrent_connects [10]\n\n[2015-11-19 01:09:12,817][DEBUG][discovery.zen            ] [tribe_cluster_node1\/t1] using ping.timeout [3s], join.timeout [1m], master_election.filter_client [true], master_election.filter_data [false]\n\n[2015-11-19 01:09:12,817][DEBUG][discovery.zen.fd         ] [tribe_cluster_node1\/t1] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]\n\n[2015-11-19 01:09:12,817][DEBUG][discovery.zen.fd         ] [tribe_cluster_node1\/t1] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]\n\n[2015-11-19 01:09:12,820][DEBUG][script                   ] [tribe_cluster_node1\/t1] using script cache with max_size [100], expire [null]\n\n[2015-11-19 01:09:12,853][DEBUG][cluster.routing.allocation.decider] [tribe_cluster_node1\/t1] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]\n\n[2015-11-19 01:09:12,853][DEBUG][cluster.routing.allocation.decider] [tribe_cluster_node1\/t1] using [cluster_concurrent_rebalance] with [2]\n\n[2015-11-19 01:09:12,854][DEBUG][cluster.routing.allocation.decider] [tribe_cluster_node1\/t1] using node_concurrent_recoveries [2], node_initial_primaries_recoveries [4]\n\n[2015-11-19 01:09:12,855][DEBUG][gateway                  ] [tribe_cluster_node1\/t1] using initial_shards [quorum]\n\n[2015-11-19 01:09:12,885][DEBUG][indices.recovery         ] [tribe_cluster_node1\/t1] using max_bytes_per_sec[40mb], concurrent_streams [3], file_chunk_size [512kb], translog_size [512kb], translog_ops [1000], and compress [true]\n\n[2015-11-19 01:09:12,886][DEBUG][indices.store            ] [tribe_cluster_node1\/t1] using indices.store.throttle.type [NONE], with index.store.throttle.max_bytes_per_sec [10gb]\n\n[2015-11-19 01:09:12,886][DEBUG][indices.memory           ] [tribe_cluster_node1\/t1] using indexing buffer size [99mb], with indices.memory.min_shard_index_buffer_size [4mb], indices.memory.max_shard_index_buffer_size [512mb], indices.memory.shard_inactive_time [5m], indices.memory.interval [30s]\n\n[2015-11-19 01:09:12,887][DEBUG][indices.cache.query      ] [tribe_cluster_node1\/t1] using [node] query cache with size [10%], actual_size [99mb], max filter count [1000]\n\n[2015-11-19 01:09:12,887][DEBUG][indices.fielddata.cache  ] [tribe_cluster_node1\/t1] using size [-1] [-1b], expire [null]\n\n[2015-11-19 01:09:12,897][INFO ][node                     ] [tribe_cluster_node1\/t1] initialized\n\n[2015-11-19 01:09:12,906][INFO ][node                     ] [tribe_cluster_node1] initialized\n\n[2015-11-19 01:09:12,907][INFO ][node                     ] [tribe_cluster_node1] starting ...\n\n[2015-11-19 01:09:12,924][DEBUG][netty.channel.socket.nio.SelectorUtil] Using select timeout of 500\n\n[2015-11-19 01:09:12,924][DEBUG][netty.channel.socket.nio.SelectorUtil] Epoll-bug workaround enabled = false\n\n[2015-11-19 01:09:12,947][DEBUG][transport.netty          ] [tribe_cluster_node1] using profile[default], worker_count[8], port[11111], bind_host[null], publish_host[null], compress[false], connect_timeout[30s], connections_per_node[2\/3\/6\/1\/1], receive_predictor[512kb->512kb]\n\n[2015-11-19 01:09:12,957][DEBUG][transport.netty          ] [tribe_cluster_node1] binding server bootstrap to: 127.0.0.1\n\n[2015-11-19 01:09:12,985][DEBUG][transport.netty          ] [tribe_cluster_node1] Bound profile [default] to address {127.0.0.1:11111}\n\n[2015-11-19 01:09:12,986][INFO ][transport                ] [tribe_cluster_node1] publish_address {127.0.0.1:11111}, bound_addresses {127.0.0.1:11111}\n\n[2015-11-19 01:09:12,993][DEBUG][discovery.local          ] [tribe_cluster_node1] Connected to cluster [Cluster [elasticsearch_2_0_0_tribe_cluster]]\n\n[2015-11-19 01:09:12,996][INFO ][discovery                ] [tribe_cluster_node1] elasticsearch_2_0_0_tribe_cluster\/baK4hDMwRiaKGS5D8ivYng\n\n[2015-11-19 01:09:12,996][WARN ][discovery                ] [tribe_cluster_node1] waited for 0s and no initial state was set by the discovery\n\n[2015-11-19 01:09:12,996][DEBUG][gateway                  ] [tribe_cluster_node1] can't wait on start for (possibly) reading state from gateway, will do it asynchronously\n\n[2015-11-19 01:09:13,010][DEBUG][http.netty               ] [tribe_cluster_node1] Bound http to address {127.0.0.1:22222}\n\n[2015-11-19 01:09:13,011][INFO ][http                     ] [tribe_cluster_node1] publish_address {127.0.0.1:22222}, bound_addresses {127.0.0.1:22222}\n\n[2015-11-19 01:09:13,011][INFO ][node                     ] [tribe_cluster_node1\/t2] starting ...\n\n[2015-11-19 01:09:13,016][DEBUG][transport.netty          ] [tribe_cluster_node1\/t2] using profile[default], worker_count[8], port[11111], bind_host[null], publish_host[null], compress[false], connect_timeout[30s], connections_per_node[2\/3\/6\/1\/1], receive_predictor[512kb->512kb]\n\n[2015-11-19 01:09:13,022][DEBUG][transport.netty          ] [tribe_cluster_node1\/t2] binding server bootstrap to: 127.0.0.1\n\n[2015-11-19 01:09:13,039][INFO ][node                     ] [tribe_cluster_node1\/t2] stopping ...\n\n[2015-11-19 01:09:13,041][INFO ][node                     ] [tribe_cluster_node1\/t2] stopped\n\n[2015-11-19 01:09:13,042][INFO ][node                     ] [tribe_cluster_node1\/t2] closing ...\n\n[2015-11-19 01:09:13,048][INFO ][node                     ] [tribe_cluster_node1\/t2] closed\n\n[2015-11-19 01:09:13,048][INFO ][node                     ] [tribe_cluster_node1\/t1] closing ...\n\n[2015-11-19 01:09:13,052][INFO ][node                     ] [tribe_cluster_node1\/t1] closed\n\nException in thread \"main\" BindTransportException[Failed to bind to [11111]]; nested: ChannelException[Failed to bind to: \/127.0.0.1:11111]; nested: BindException[Address already in use];\n\nLikely root cause: java.net.BindException: Address already in use\n\nat sun.nio.ch.Net.bind0(Native Method)\n\nat sun.nio.ch.Net.bind(Net.java:444)\n\nat sun.nio.ch.Net.bind(Net.java:436)\n\nat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)\n\nat sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)\n\nat org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193)\n\nat org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:391)\n\nat org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:315)\n\nat org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)\n\nat org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n\nat org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\nat java.lang.Thread.run(Thread.java:745)\n\nRefer to the log for complete error details.\n\n[2015-11-19 01:09:13,058][INFO ][node                     ] [tribe_cluster_node1] stopping ...\n\n[2015-11-19 01:09:13,064][INFO ][node                     ] [tribe_cluster_node1] stopped\n\n[2015-11-19 01:09:13,064][INFO ][node                     ] [tribe_cluster_node1] closing ...\n\n[2015-11-19 01:09:13,066][INFO ][node                     ] [tribe_cluster_node1] closed\n```\n\nNote that I cannot reproduce this on 1.7.2.  On 1.7.2, I can set up a custom transport.tcp.port for the tribe node and it will start up fine.  \n"},{"date":"2015-11-19T23:21:47Z","author":"javanna","text":"@ppf2 this happens because the tribe node process will start three nodes, the first one will get the configured port, and the second will try to get the same one as it reads from the same configuration file. The workaround provided by Clint above should work till we fix this properly.\n"},{"date":"2015-11-19T23:44:40Z","author":"rjernst","text":"@javanna I am going to explore having the tribe node have its own subclass of Node which can customize this single behavior (how to get the node's settings). I don't think we should add back this general purpose flag as we need to keep the tons of ways Nodes can be configured to a minimum.\n"},{"date":"2015-11-20T00:03:33Z","author":"javanna","text":"@rjernst thanks that sounds good to me. \n"},{"date":"2015-11-20T00:03:35Z","author":"ppf2","text":"Confirmed that the workaround works to prevent the BindTransportException error, thx!\n"},{"date":"2015-11-20T00:27:10Z","author":"ppf2","text":"@rjernst Do we have a sense of whether the fix will make it to the upcoming 2.1 release? Or will it likely be after 2.1 (i.e. use the workaround until a later 2.x release)?\n"},{"date":"2015-11-20T01:10:11Z","author":"rjernst","text":"@ppf2 Definitely after 2.1. I would not want to destabilize 2.1 with a refactoring like this. \n"},{"date":"2015-11-20T01:12:15Z","author":"ppf2","text":"@rjernst sounds good, thx!\n"},{"date":"2015-12-02T09:12:57Z","author":"clintongormley","text":"This requires some fairly extensive changes, so we will target this for 2.2.  In the meantime, we should document the workaround in the 2.1 docs.\n"},{"date":"2015-12-08T04:22:34Z","author":"rjernst","text":"I opened a PR to fix this here: #15300.\n\nNote that I was able to do the fix simply enough that I think it will be ok to backport to 2.1.x\n"},{"date":"2015-12-09T12:15:45Z","author":"clintongormley","text":"thanks @rjernst \n"},{"date":"2015-12-10T02:26:47Z","author":"ppf2","text":"Thanks @rjernst !\n"},{"date":"2015-12-12T23:02:06Z","author":"lb425","text":"I'm late to the party but thought this might be useful for anyone coming across this. I found that the dummy config file isn't needed to work around the issue. Instead for creating a new directory (\/etc\/tribe-client in the example) path.conf can reference the current configuration directory.\n\nUsing the above example where the config directory was \/etc\/tribe\n\n# arbitrary config\n\ntransport.tcp.port: 9301\nhttp.port: 9201\nnetwork.host: 0.0.0.0\npath.data: \/var\/lib\/elasticsearch\/\npath.logs: \/var\/log\/elasticsearch\/\n\ntribe:\n    kibana:\n        path.conf: \/etc\/tribe  #\n        cluster.name: logstash-kibana\n        discovery.zen.ping.multicast.enabled: false\n        discovery.zen.ping.unicast.hosts: [\"127.0.0.1\"]\n    els:\n        path.conf: \/etc\/tribe  #\n        cluster.name: logstash-data\n        discovery.zen.ping.multicast.enabled: false\n        discovery.zen.ping.unicast.hosts: [\"10.128.69.48\", \"10.128.75.237\"]\n"},{"date":"2015-12-22T00:25:55Z","author":"kt97679","text":"Is this fixed in 2.1.1?\n"},{"date":"2015-12-30T10:50:28Z","author":"thn-dev","text":"With v2.1.1, I still have to specify path.conf and I used the valid path as mentioned above by lb425. In my case, I also had to specify path.plugins for similar reason. Otherwise, I kept getting AccessControlException error.\n\nI did not have to specify both path.conf and path.plugins when I was using v1.7.3\n"},{"date":"2015-12-31T14:20:17Z","author":"thn-dev","text":"WRT ES v2.1.1, I have to do the following to get the tribe node talking to two different clusters: cluster A and cluster B\n\n**# tribe node's configuration (elasticsearch.yml)**\n**network.host:** 0.0.0.0\n**transport.tcp.port:** 9300\n**http.port:** 9200\n**http.enabled:** true\n\ntribe.**t1**.cluster.name: **<cluster A>**\ntribe.**t1**.discovery.zen.ping.unicast.hosts: **<cluster A's master node>**\ntribe.**t1**.discovery.zen.ping.multicast.enabled: false\ntribe.**t1**.path.conf: **<valid path\/to\/conf>**\ntribe.**t1**.path.plugins: **<valid path\/to\/plugin>**\ntribe.**t1**.network.bind_host: **0.0.0.0**\ntribe.**t1**.network.publish_host: **<tribe node's IP>**\ntribe.**t1**.transport.tcp.port: **<optional but different from tribe node port above>**\n\n_repeat the same block but replace \"t1\" to \"t2\" for cluster B and fill in proper info related to cluster B but keep the tribe.t2.network.\\* the same with different tribe.t2.transport.tcp.port value from t1 if specified_\n"},{"date":"2015-12-31T19:12:18Z","author":"rjernst","text":"@thn-dev Setting network and path settings for tribe nodes (the t1, t2 here) should not be necessary. Can you share your full elasticsearch.yml for both the tribe node, as well as cluster A and cluster B?\n"},{"date":"2016-01-01T00:56:30Z","author":"thn-dev","text":"@rjernst I did not have to do network and path settings when I was using v1.7.3. It was a surprise to me when v2.1.1 kept giving me AccessControlException error message. Initially, it pointed to the \"plugins\" location, after I set it, it complained about the \"config\" location. If I did not do the network settings for t1 and t2, it was not able to connect to cluster A and\/or B. This part is weird too. Again, I did not have to do this in v1.7.3.\n\nAll ES instances are installed using .rpm file, not .zip file.\n\nMy settings for tribe node is above with additional parameters\n- cluster.name\n- discovery.zen.ping.multicast.enabled: false\n\nCluster A and B, each has 1 master node, 3 data nodes with the following parameters' settings (I don't have all information with me at the moment)\n- cluster.name: <cluster A or B>\n- network.host: 0.0.0.0\n- transport.tcp.port: 9300\n- http.port: 9200 (master)\n- http.enabled: true (master)\n- discovery.zen.ping.multicast.enabled: false\n- discovery.zen.ping.unicast.hosts: <master node's IP>\n- path.conf: \/data\/es\/config\n- path.plugins: \/data\/es\/plugins\n- path.data: \/data\/es\n"}],"reopen_on":"2015-11-17T12:49:22Z","opened_by":"kt97679","closed_on":"2015-12-08T16:07:26Z","description":"Hi folks,\n\nI'm trying to start tribe node using following config:\n\n<pre>\ntransport.tcp.port: 9301\nhttp.port: 9201\nnetwork.host: 0.0.0.0\npath.data: \/var\/lib\/elasticsearch\/\npath.logs: \/var\/log\/elasticsearch\/\n\ntribe:\n    kibana:\n        cluster.name: logstash-kibana\n        discovery.zen.ping.multicast.enabled: false\n        discovery.zen.ping.unicast.hosts: [\"127.0.0.1\"]\n    els:\n        cluster.name: logstash-data\n        discovery.zen.ping.multicast.enabled: false\n        discovery.zen.ping.unicast.hosts: [\"10.128.69.48\", \"10.128.75.237\"]\n<\/pre>\n\nThis config resides in the file \/etc\/tribe-elasticseach\/elasticsearch.yml. I'm starting it using following command:\n\n<pre>\nsudo -u elasticsearch \/usr\/share\/elasticsearch\/bin\/elasticsearch -Ddefault.path.conf=\/etc\/tribe-elasticsearch\/\n<\/pre>\n\nElasticsearch fails with following output:\n\n<pre>\n[2015-11-05 17:07:42,433][INFO ][node                     ] [Bucky] version[2.0.0], pid[25943], build[de54438\/2015-10-22T08:09:48Z]\n[2015-11-05 17:07:42,434][INFO ][node                     ] [Bucky] initializing ...\n[2015-11-05 17:07:42,596][INFO ][plugins                  ] [Bucky] loaded [], sites []\nException in thread \"main\" java.security.AccessControlException: access denied (\"java.io.FilePermission\" \"\/usr\/share\/elasticsearch\/config\/elasticsearch.yml\" \"read\")\n        at java.security.AccessControlContext.checkPermission(AccessControlContext.java:457)\n        at java.security.AccessController.checkPermission(AccessController.java:884)\n        at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)\n        at java.lang.SecurityManager.checkRead(SecurityManager.java:888)\n        at sun.nio.fs.UnixPath.checkRead(UnixPath.java:795)\n        at sun.nio.fs.UnixFileSystemProvider.checkAccess(UnixFileSystemProvider.java:290)\n        at java.nio.file.Files.exists(Files.java:2385)\n        at org.elasticsearch.node.internal.InternalSettingsPreparer.prepareEnvironment(InternalSettingsPreparer.java:87)\n        at org.elasticsearch.node.Node.<init>(Node.java:128)\n        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)\n        at org.elasticsearch.tribe.TribeService.<init>(TribeService.java:136)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:422)\n        at <<<guice>>>\n        at org.elasticsearch.node.Node.<init>(Node.java:198)\n        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)\n        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:170)\n        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)\n        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)\n<\/pre>\n\nI'm not sure why it tries to access \/usr\/share\/elasticsearch\/config\/elasticsearch.yml. There is no such file in the elasticsearch deb package. I created this file, but command above still fails with same output. Please advise how this can be resolved.\n\nI'm running elasticsearch 2.0.0 installed from the debian package downloaded from the official site. I'm using ubuntu 14.\n\nThanks,\nKirill.\n","id":"115413338","title":"elasticsearch fails to start tribe node","reopen_by":"clintongormley","opened_on":"2015-11-06T01:17:02Z","closed_by":"rjernst"},{"number":"14315","comments":[{"date":"2015-10-27T16:59:25Z","author":"jasontedor","text":"Would you mind taking a look @jpountz?\n"},{"date":"2015-10-27T17:45:22Z","author":"jpountz","text":"Doesn't this change bring back the issue that you can run into a deadlock if you load dependent keys that end up in the same segment?\n"},{"date":"2015-10-27T18:48:30Z","author":"jasontedor","text":"@jpountz I've pushed df972d2c95b948fcca6d3299d7ad6656fa79d15f to avoid reintroducing the deadlock problem that was initially solved by not executing loads while holding a segment lock.\n"},{"date":"2015-10-27T19:48:22Z","author":"jpountz","text":"I _think_ this is good, but to be honest the logic is quite complex so I could easily miss something. Could you add tests for the two issues that you are trying to fix, maybe as well as the potential deadlock issue in case of dependent keys?\n"},{"date":"2015-10-27T21:05:24Z","author":"jasontedor","text":"@jpountz Adding a test caught an issue with the pollution handling in racing threads. I've added a fix in c1d8ace818ef819d39826f6678685d7a93c3d5c4 and a test in 1b2b71bcd93e124ab6a0456924e934d6a38babb3 that currently fails on master and does not fail after c1d8ace818ef819d39826f6678685d7a93c3d5c4.\n"},{"date":"2015-10-28T08:46:53Z","author":"jpountz","text":"Can you also add a test for the deadlock problem with dependent keys? I might be wrong but it should be easy to test with an object that generates bad hashcodes?\n"},{"date":"2015-10-28T12:33:01Z","author":"jasontedor","text":"@jpountz I opened #14334 to add a unit test for the deadlock issue. This test fails prior to but passes after commit 1d0b93f76667e4af1bad7b0e522f1a4e6c8b3fbc (the commit to address the deadlock issue) and also passes against this current pull request.\n"},{"date":"2015-10-28T17:12:15Z","author":"jpountz","text":"LGTM\n"},{"date":"2015-10-28T18:29:47Z","author":"jasontedor","text":"Thanks again for another careful review @jpountz. Squashed and integrated into master.\n"}],"reopen_on":"2015-10-27T16:59:28Z","opened_by":"jasontedor","closed_on":"2015-10-28T18:29:26Z","description":"This commit fixes two issues that could arise when a loader throws an\nexception during a load in Cache#computeIfAbsent.\n\nThe underlying issue is that if the loader throws an exception,\nCache#computeIfAbsent would attempt to remove the polluted entry from\nthe cache. However, this cleanup was performed outside of the segment\nlock. This means another thread could race and expire the polluted\nentry (leading to NPEs) or get a polluted entry out of the cache before\nthe loading thread had a chance to cleanup (leading to ISEs).\n\nThe solution to the initial problem of correctly handling failed cached\nloads is to check for failed loads in all places where entries are\nretrieved from the map backing the segment. In such cases, we treat it\nas if there was no entry in the cache, and we clean up the cache on a\nbest-effort basis. All of this is done outside of the segment lock to\navoid reintroducing the deadlock that was initially a problem when\nloads were executed under a segment lock.\n","id":"113631338","title":"Fix issues with failed cache loads","reopen_by":"jasontedor","opened_on":"2015-10-27T16:58:16Z","closed_by":"jasontedor"},{"number":"13884","comments":[{"date":"2015-10-02T15:53:05Z","author":"clintongormley","text":"This does look like a bug.  The min_should_match is being applied at the wrong level:\n\n```\nGET \/test\/test\/_validate\/query?explain\n{\n  \"query\" : {\n    \"simple_query_string\" : {\n      \"fields\" : [ \"title\", \"f1\", \"f2\" ],\n      \"query\" : \"test\",\n      \"minimum_should_match\" : \"-50%\"\n    }\n  }\n}\n```\n\nReturns an explanation of:\n\n```\n+((f1:test title:test f2:test)~2)\n```\n\nWhile the `query_string` and `multi_match` equivalents return:\n\n```\n+(title:test | f1:test | f2:test)\n```\n"},{"date":"2015-10-14T10:09:12Z","author":"cbuescher","text":"I had a look and saw that `simple_query_string` iterates over all fields for each token in the query string and combines them all in boolean query `should` clauses, and we apply the `minimum_should_match` on the whole result. \n\n@clintongormley As far as I understand you, we should parse the query string for each field separately, apply the `minimum_should_match` there and then combine the result in an overall Boolean query. This however raised another question for me. Suppose we have two terms like `\"query\" : \"test document\"` instead, then currently we we get:\n\n```\n((f1:test title:test f2:test) (f1:document title:document f2:document))~1\n```\n\nIf we would instead create the query per field individually we would get something like\n\n```\n((title:test title:document)~1 (f1:test f1:document)~1 (f2:test f2:document)~1)\n```\n\nWhile treating the query string for each field individually looks like the right behaviour in this case, I wonder if this will break other cases. wdyt?\n"},{"date":"2015-10-14T11:31:18Z","author":"clintongormley","text":"@cbuescher the `query_string` query takes the same approach as your first output, ie:\n\n```\n((f1:test title:test f2:test) (f1:document title:document f2:document))~1\n```\n\nI think the bug is maybe a bit more subtle.  A query across 3 fields for two terms with min should match 80% results in:\n\n```\nbool:\n    min_should_match: 80% (==1)\n    should:\n      bool:\n        should: [ f1:term1, f2:term1, f3:term1]\n      bool:\n        should: [ f1:term2, f2:term2, f3:term2]\n```\n\nhowever with only one term it is producing:\n\n```\nbool:\n  min_should_match: 80% (==2) \n  should: [ f1:term1, f2:term1, f3:term1]\n```\n\nIn other words, min should match is being applied to the wrong `bool` query.  Instead, even the one term case should be wrapped in another `bool` query, and the min should match should be applied at that level.\n"},{"date":"2015-10-14T12:03:12Z","author":"cbuescher","text":"@clintongormley Yes, I think thats what I meant. I'm working on a PR that applies the `minimum_should_match` to sub-queries that only target one field. That way your examples above would change to something like\n\n```\nbool:    \n    should:\n      bool:\n        min_should_match: 80% (==1)\n        should: [ f1:term1, f1:term2]\n      bool:\n        min_should_match: 80% (==1)\n        should: [ f2:term1, f2:term2]\n      bool:\n        min_should_match: 80% (==1)\n        should: [ f3:term1, f3:term2]\n```\n\nand for one term\n\n```\nbool:    \n    should:\n      bool:\n        min_should_match: 80% (==0)\n        should: [ f1:term1]\n      bool:\n        min_should_match: 80% (==0)\n        should: [ f2:term1]\n      bool:\n        min_should_match: 80% (==0)\n        should: [ f3:term1]\n```\n\nIn the later case we already additionally simplify one-term bool queries to TermQueries.\n"},{"date":"2015-10-15T11:22:45Z","author":"clintongormley","text":"@cbuescher I think that is incorrect.  The simple query string query (like the query string query) is term-centric rather than field-centric.  In other words, min should match should be applied to the number of terms (regardless of which field the term is in).\n\nI'm guessing that there is an \"optimization\" for the one term case where the field-level bool clause is not wrapped in an outer bool clause.  Then the min should match is applied at the field level instead of at the term level, resulting in the wrong calculation.\n"},{"date":"2015-10-15T12:49:18Z","author":"cbuescher","text":"That guess seems right, there is an optimization in lucenes\nSimpleQueryParser for boolean queries with 0 or 1 clauses that seems to be\nthe problem. I think we can overwrite that.\n\nOn Thu, Oct 15, 2015 at 1:23 PM, Clinton Gormley notifications@github.com\nwrote:\n\n> @cbuescher https:\/\/github.com\/cbuescher I think that is incorrect. The\n> simple query string query (like the query string query) is term-centric\n> rather than field-centric. In other words, min should match should be\n> applied to the number of terms (regardless of which field the term is in).\n> \n> I'm guessing that there is an \"optimization\" for the one term case where\n> the field-level bool clause is not wrapped in an outer bool clause. Then\n> the min should match is applied at the field level instead of at the term\n> level, resulting in the wrong calculation.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https:\/\/github.com\/elastic\/elasticsearch\/issues\/13884#issuecomment-148358711\n> .\n\n## \n\nChristoph Büscher\n"},{"date":"2015-10-15T15:04:19Z","author":"clintongormley","text":"If this is in Lucene, perhaps it should be fixed there?\n\n@jdconrad what do you think?\n"},{"date":"2015-10-15T15:15:15Z","author":"cbuescher","text":"@clintongormley I don't think SimpleQueryParser#simplify() is at the root of this anymore. The problem seems to be that SimpleQueryParser parses term by term-centric, but only starts wrapping the resulting queries when combining more than two of them. For one search term and two fields I get a Boolean query with two TermQuery clauses (without enclosing Boolean query), for two terms and one field I get an enclosing Boolean query with two Boolean query subclauses. I'm not sure yet how this can be distiguished from outside of the Lucene parser without inspecting the query, and if a solution like that holds for more complicated cases.\n"},{"date":"2015-10-19T08:58:58Z","author":"cbuescher","text":"Althought it would be nice if Lucene SimpleQueryParse would output a Boolquery with one should-clause and three nested Boolqueries for the 1-term\/multi-field case, I think we can detect this case and do the wrapping in the additional Boolquery in the SimpleQueryStringBuilder. I just opened a PR.\n"},{"date":"2015-10-19T17:15:49Z","author":"jdconrad","text":"The SQP wasn't really designed around multi-field terms, but needed to have it added afterwards for use as a default field which is why the min-should-match never gets applied down at that the level.  I don't know if the correct behavior is to make it work on multi-fields.  I'll have to give that some thought given that it really is as @cbuescher described as term-centric, and it sort of supposed to be disguised from the user.  One thing that will make this easier to fix, though, I believe is #4707, since it will flatten the parse tree a bit.\n"},{"date":"2015-10-20T09:10:12Z","author":"cbuescher","text":"@jdconrad thanks for explaining, in the meantime I opened #14186 which basically tries to distinguish the one vs. multi-field cases and tries wraps the resulting query one more time to get a correct min-should-match. Please leave comment there if my current approach will colide the plans regarding #4707.\n"},{"date":"2015-11-03T11:01:39Z","author":"cbuescher","text":"Reopening this issue since the fix proposed in #14186 was too fragile. Discussed with @javanna and @jpountz, at this point we think the options are either fixing this in lucenes SimpleQueryParser so that we can apply minimum_should_match correctly on the ES side or remove this option from `simple_query_string` entirely because it cannot properly supported.\n"},{"date":"2015-11-03T11:57:40Z","author":"cbuescher","text":"Trying to sum up this issue so far: \n- the number of should-clauses returned by `SimpleQueryParser` is not 1 for one search term and multiple fields, so we cannot apply `minimum_should_match` correctly in `SimpleQueryStringBuilder`. e.g. for `\"query\" : \"term1\", \"fields\" : [ \"f1\", \"f2\", \"f3\" ]` SimpleQueryParser returns a BooleanQuery with three should-clauses. As soon  as we add more search terms, the number of should-clauses is the same as the number of search terms, e.g. `\"query\" : \"term1 term2\", \"fields\" : [ \"f1\", \"f2\", \"f3\" ]` returns a BooleanQuery with two subclauses, one per term.\n- it is difficult to determine the number of terms from the query string upfront, because the tokenization depends on the analyzer used, so we really need `SimpleQueryParser#parse()` for this.\n- it is hard to determine the correct number of terms from the returned lucene query without making assumptions about the inner structure of the query (which is subject to change, reason for #14186 beeing reverted). e.g. currently `\"query\" : \"term1\", \"fields\" : [ \"f1\", \"f2\", \"f3\" ]` and `\"query\" : \"term1 term2 term3\", \"fields\" : [ \"f1\" ]` will return a BooleanQuery with same structure (three should-clauses, each containing a TermQuery). \n"},{"date":"2016-02-04T18:11:51Z","author":"jimczi","text":"@cbuescher this issue is fixed by https:\/\/github.com\/elastic\/elasticsearch\/pull\/16155. \n@rmuir has pointed out a nice way to distinguish between a single word query with multiple fields against a multi word query with a single field: we just have to check if the coord are disabled on the top level BooleanQuery, the simple query parser disables the coord when the boolean query for multiple fields is built.\nThough I tested the single word with multiple fields case only, if you think of other issues please reopen this ticket or open a new one ;).\n"},{"date":"2016-02-04T21:08:13Z","author":"cbuescher","text":"@jimferenczi thats great, I just checked this with a test which is close to the problem desciption here. I'm not sure if this adds anything to your tests, but just in case I justed opened  #16465 which adds this as an integration test for SimpleQueryStringQuery. Maybe you can take a look and tell me if it makes sense to add those as well.\n"},{"date":"2016-02-05T08:55:03Z","author":"jimczi","text":"@cbuescher thanks, a unit test in SimpleQueryStringBuilderTest could be useful as well. The integ test does not check the minimum should match that is applied (or not) to the boolean query. \n"}],"reopen_on":"2015-11-03T11:01:39Z","opened_by":"davidlbowen","closed_on":"2016-02-04T18:11:51Z","description":"```\n# With a one-word query and minimum_should_match=-50%, adding extra non-matching fields should not matter.\n# Tested on v1.7.2.\n\n# delete and re-create the index\ncurl -XDELETE localhost:9200\/test\ncurl -XPUT localhost:9200\/test\n\necho \n\n # insert a document\ncurl -XPUT 'http:\/\/localhost:9200\/test\/test\/1' -d '\n { \"title\": \"test document\"}\n '\ncurl -XPOST 'http:\/\/localhost:9200\/test\/_refresh'\n\necho \n\n # this correctly finds the document (f1 is a non-existent field)\n curl -XGET 'http:\/\/localhost:9200\/test\/test\/_search' -d '{\n  \"query\" : {\n    \"simple_query_string\" : {\n      \"fields\" : [ \"title\", \"f1\" ],\n      \"query\" : \"test\",\n      \"minimum_should_match\" : \"-50%\"\n    }\n  }\n }\n'\n\necho \n\n# this incorrectly does not find the document (f1 and f2 are non-existent fields)\ncurl -XGET 'http:\/\/localhost:9200\/test\/test\/_search' -d '{\n  \"query\" : {\n    \"simple_query_string\" : {\n      \"fields\" : [ \"title\", \"f1\", \"f2\" ],\n      \"query\" : \"test\",\n      \"minimum_should_match\" : \"-50%\"\n    }\n  }\n }\n'\n\necho\n```\n","id":"109217921","title":"Bug with simple_query_string, minimum_should_match, and multiple fields.","reopen_by":"cbuescher","opened_on":"2015-10-01T02:34:48Z","closed_by":"jimczi"},{"number":"13492","comments":[{"date":"2015-09-10T21:18:31Z","author":"rjernst","text":"This should be simple to fix. The `onModule(DiscoveryModule)` methods in the aws and azure plugins just need to add the provider there (the discovery module has this method now). This is what I intended, but missed it since all tests pass without this! We really need some tests here...\n"},{"date":"2015-09-10T21:22:10Z","author":"rmuir","text":"I still think some mocks for integs that listen on a socket is the way to go. means something on a socket in pre-integration-test and the code really treats it like AWS X service or azure Y service. \n"},{"date":"2015-09-10T21:27:23Z","author":"dadoonet","text":"Definitely. I started playing with [this lib](https:\/\/github.com\/treelogic-swe\/aws-mock) (running that in a Jetty container to simulate AWS calls). Was working fine at the beginning but at the end I was missing some important methods.\nI started to contribute a bit but then started something else... \n"},{"date":"2015-09-11T08:27:46Z","author":"dadoonet","text":"Reopening as the fix for Azure was wrong. \nReverted with https:\/\/github.com\/elastic\/elasticsearch\/commit\/163c34127f877c70a0cb5bf95c64c6efada484eb\nTests are now failing.\n"}],"reopen_on":"2015-09-11T08:27:46Z","opened_by":"dadoonet","closed_on":"2015-09-11T11:21:53Z","description":"Discovery does not work anymore in azure and ec2.\n\nIt's caused by the following commit which does not call anymore `addUnicastHostProvider`.\n- AWS: https:\/\/github.com\/elastic\/elasticsearch\/commit\/40f119d85a4eaa39d0a6e594e46f70de725557f0#diff-767ee396aa86de938ebb72b4c2f359a0L35\n- Azure: https:\/\/github.com\/elastic\/elasticsearch\/commit\/40f119d85a4eaa39d0a6e594e46f70de725557f0#diff-56a7239bf69dfcd365858d333edca510L43\n\n@rjernst Could you have a look at it please?\n\ncc @drewr \n","id":"105898077","title":"[ec2\/azure] discovery does not work anymore from 2.0.0-beta1","reopen_by":"dadoonet","opened_on":"2015-09-10T21:08:43Z","closed_by":"s1monw"},{"number":"13240","comments":[{"date":"2015-09-01T13:14:39Z","author":"clintongormley","text":"Hi @kiryam \n\nHow is this related to #13203?  There is nothing in the PR description. Is this just the backport? (normally we take care of the backport)\n"},{"date":"2015-09-01T13:21:04Z","author":"kiryam","text":"@clintongormley Hello, yes! it is backport only. Tests pending\n"},{"date":"2015-09-01T13:28:57Z","author":"s1monw","text":"@kiryam don't worry about backporting I will take care of this if we decide it should go in that branch!\n"},{"date":"2015-09-01T13:29:18Z","author":"kiryam","text":"Thx!\n"},{"date":"2016-01-09T19:29:29Z","author":"s1monw","text":"this can be closed it has been backported afaik\n"}],"reopen_on":"2015-09-01T14:00:33Z","opened_by":"kiryam","closed_on":"2016-01-09T19:29:29Z","description":"Backport of #13203.\nRelated to #13202\n","id":"104231565","title":"Add listeners for postIndex, postCreate, and postDelete, backport to 1.7","reopen_by":"kiryam","opened_on":"2015-09-01T10:00:34Z","closed_by":"s1monw"},{"number":"13142","comments":[{"date":"2015-09-01T07:38:33Z","author":"jpountz","text":"Left one minor comment, but other than that it looks good to me. Good catch!\n"},{"date":"2015-09-01T07:38:54Z","author":"jpountz","text":"Sorry, closed by mistake.\n"}],"reopen_on":"2015-09-01T07:38:34Z","opened_by":"javanna","closed_on":"2015-09-03T18:12:55Z","description":"The match_phrase_prefix query properly parses the boost etc. but it loses it in its rewrite method. Fixed that by setting the orginal boost to the rewritten query before returning it. Also cleaned up some warning in MultiPhrasePrefixQuery.\n\nCloses #13129\n","id":"103510869","title":"Query DSL: match_phrase_prefix to take boost into account","reopen_by":"jpountz","opened_on":"2015-08-27T13:37:11Z","closed_by":"javanna"},{"number":"13017","comments":[{"date":"2015-08-20T16:29:23Z","author":"rjernst","text":"This seems like a bug maybe? why are we not consuming the token there?\n"},{"date":"2015-08-20T16:39:42Z","author":"tlrx","text":"Looks like it has been introduced by #11414\n"},{"date":"2015-08-20T17:16:58Z","author":"rjernst","text":"I think you misunderstood. The assertion code is correct, it means there was leftover stuff at the end of parsing. But _why_ is there leftover stuff in this case. what is the document you are sending, I only see the mapping in the original description? My hunch is, we need to fully consume the parser for mappings that are disabled.\n"},{"date":"2015-08-20T17:51:03Z","author":"tlrx","text":"What I don't get is why I can create this mapping and index documents in elasticsearch but cannot do exactly the same thing in integration tests.\n\nHere is the document I index: https:\/\/github.com\/tlrx\/elasticsearch\/commit\/94712121dd524c48f2dc40c787efd1eb6375d799#diff-82784da2187574da5398ae215c8a6795R276\n\nThis test fails on my computer and I don't get why.\n"},{"date":"2015-08-20T17:57:41Z","author":"rjernst","text":"Assertions are enabled in tests, but not when running with bin\/elasticsearch.\n"},{"date":"2015-08-20T18:00:50Z","author":"rmuir","text":"Why isn't this a real check\n"},{"date":"2015-08-20T18:03:11Z","author":"tlrx","text":"@rjernst Yes I know, but the test I linked to seems good to me and I don't see why this assertion throws up. Can you please have a quick look to the code and\/or try to reproduce?\n"},{"date":"2015-08-24T12:27:35Z","author":"clintongormley","text":"this is a bug.  The following fails with assertions enabled:\n\n```\nPUT my_index\n{\n  \"mappings\": {\n    \"my_type\": {\n      \"enabled\": false\n    }\n  }\n}\n\nPUT my_index\/my_type\/1\n{\n  \"foo\": \"bar\"\n}\n```\n\nThe whole type is disabled, which means the _source should be stored but nothing should be indexed.  It looks like we're just skipping parsing completely instead of consuming the tokens.\n"},{"date":"2015-08-24T16:25:16Z","author":"rjernst","text":"I have a test reproducing the issue and am investigating.\n"},{"date":"2015-08-26T08:11:25Z","author":"tlrx","text":"@rjernst Thanks for the work you have done :) Unfortunately I reopen this bug because indexing a document (like @clinton suggested) now fails with a `NullPointerException` (I tested on latest snapshot - build e8834cc78c13a507e2851908f8d51489e7888570).\n\nAs far as I understand the code it seems that your fix skips the document parsing when the type is disabled. So field mappers like `UidFieldMapper` are not used, resulting in a null `uid` thrown [here](https:\/\/github.com\/elastic\/elasticsearch\/blob\/master\/core\/src\/main\/java\/org\/elasticsearch\/index\/shard\/IndexShard.java#L515). \n\nI'm not familiar with this part of the code but if you don't have time to look further please let me know and I'll try to have a look.\n"}],"reopen_on":"2015-08-26T08:11:25Z","opened_by":"tlrx","closed_on":"2015-08-30T19:14:18Z","description":"In integration tests, using `enabled: false` on a root document type makes document indexing fails.\n\nHere is the mapping I use:\n\n```\n\"my_doc_type\": {\n  \"enabled\": false\n}\n```\n\nBut then indexing a document throws a MapperParsingException with the following stack:\n\n``` java\nMapperParsingException[failed to parse]; nested: AssertionError;\n    at __randomizedtesting.SeedInfo.seed([1B10B6999E316CD7:4AED1D00C8EC9444]:0)\n    at org.elasticsearch.index.mapper.DocumentParser.innerParseDocument(DocumentParser.java:155)\n    at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:79)\n    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:317)\n    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:313)\n    ...\n```\n\nThe assertion is located in the `DocumentParser` class:\n\n``` java\n            \/\/ try to parse the next token, this should be null if the object is ended properly\n            \/\/ but will throw a JSON exception if the extra tokens is not valid JSON (this will be handled by the catch)\n            if (Version.indexCreated(indexSettings).onOrAfter(Version.V_2_0_0_beta1)\n                && source.parser() == null && parser != null) {\n                \/\/ only check for end of tokens if we created the parser here\n                token = parser.nextToken();\n                assert token == null; \/\/ double check, in tests, that we didn't end parsing early\n            }\n```\n\nWhen assertion are disabled, everything is OK. I did a simple test to reproduce the issue here:\nhttps:\/\/github.com\/tlrx\/elasticsearch\/commit\/94712121dd524c48f2dc40c787efd1eb6375d799\n\nThis assertion has been added recently and I don't know much about this part of code, maybe @rjernst can help here?\n","id":"102171560","title":"Using enabled:false on document type throws exception in tests","reopen_by":"tlrx","opened_on":"2015-08-20T15:54:58Z","closed_by":"rjernst"},{"number":"12945","comments":[{"date":"2016-01-28T12:23:20Z","author":"clintongormley","text":"Multicast support has been removed. Closing\n"},{"date":"2016-01-28T12:25:44Z","author":"rmuir","text":"Multicast has been removed? Why do i still see the plugin in master?\n"},{"date":"2016-01-29T09:15:21Z","author":"clintongormley","text":"I was under the mistaken impression that it was deprecated when it was moved to a plugin.\n"},{"date":"2016-01-29T14:22:19Z","author":"clintongormley","text":"See #16310 \n"},{"date":"2016-02-03T13:08:15Z","author":"clintongormley","text":"Multicast has been removed by #16326\n"}],"reopen_on":"2016-01-29T09:15:01Z","opened_by":"rmuir","closed_on":"2016-02-03T13:08:15Z","description":"followup to #12942\n\nCurrently this is only going to work over ipv4, due to the address used. We should support a pure v6 environment: maybe it works today if you use a -D to change the address, but it sorta defeats the purpose of not requiring configuration:)\n\nHowever, I think its best to get #12914 straightened out on the OS X first, to eliminate any confusion.\n","id":"101492196","title":"multicast ping should work over ipv6","reopen_by":"clintongormley","opened_on":"2015-08-17T19:23:17Z","closed_by":"clintongormley"},{"number":"12791","comments":[{"date":"2015-08-11T10:52:55Z","author":"dadoonet","text":"I can confirm and reproduce the issue. \n\nFunny thing: when you do that again from root dir but \"resume from\" smoke-test-shaded module, you don't hit the issue...\n\n`mvn clean verify -rf :smoke-test-shaded` gives `BUILD SUCCESS`\n"},{"date":"2015-08-11T11:29:16Z","author":"dadoonet","text":"I found a way to quickly reproduce the issue.\n\n```\nmvn verify -pl org.elasticsearch.distribution.shaded:elasticsearch,:smoke-test-shaded\n```\n\nIt makes obvious that when shaded is added to the reactor, it fails then smoke tests.\nWorking on it...\n"},{"date":"2015-08-11T12:07:33Z","author":"dadoonet","text":"So the difference between both execution plan is obviously the `projectArtifactMap`:\n\nThe wrong one when you run `org.elasticsearch.distribution.shaded:elasticsearch,:smoke-test-shaded`\n\n```\n[DEBUG]   (f) projectArtifactMap = \n* org.elasticsearch.distribution.shaded:elasticsearch=org.elasticsearch.distribution.shaded:elasticsearch:jar:2.0.0-beta1-SNAPSHOT:compile\n* org.elasticsearch:elasticsearch=org.elasticsearch:elasticsearch:jar:2.0.0-beta1-SNAPSHOT:compile\n* org.apache.lucene:lucene-backward-codecs=org.apache.lucene:lucene-backward-codecs:jar:5.2.1:compile\n* org.apache.lucene:lucene-analyzers-common=org.apache.lucene:lucene-analyzers-common:jar:5.2.1:compile\n* org.apache.lucene:lucene-queries=org.apache.lucene:lucene-queries:jar:5.2.1:compile\n* org.apache.lucene:lucene-memory=org.apache.lucene:lucene-memory:jar:5.2.1:compile\n* org.apache.lucene:lucene-highlighter=org.apache.lucene:lucene-highlighter:jar:5.2.1:compile\n* org.apache.lucene:lucene-queryparser=org.apache.lucene:lucene-queryparser:jar:5.2.1:compile\n* org.apache.lucene:lucene-sandbox=org.apache.lucene:lucene-sandbox:jar:5.2.1:compile\n* org.apache.lucene:lucene-suggest=org.apache.lucene:lucene-suggest:jar:5.2.1:compile\n* org.apache.lucene:lucene-misc=org.apache.lucene:lucene-misc:jar:5.2.1:compile\n* org.apache.lucene:lucene-join=org.apache.lucene:lucene-join:jar:5.2.1:compile\n* org.apache.lucene:lucene-grouping=org.apache.lucene:lucene-grouping:jar:5.2.1:compile\n* org.apache.lucene:lucene-spatial=org.apache.lucene:lucene-spatial:jar:5.2.1:compile\n* com.spatial4j:spatial4j=com.spatial4j:spatial4j:jar:0.4.1:compile\n* com.google.guava:guava=com.google.guava:guava:jar:18.0:compile\n* com.carrotsearch:hppc=com.carrotsearch:hppc:jar:0.7.1:compile\n* joda-time:joda-time=joda-time:joda-time:jar:2.8:compile\n* org.joda:joda-convert=org.joda:joda-convert:jar:1.2:compile\n* com.fasterxml.jackson.core:jackson-core=com.fasterxml.jackson.core:jackson-core:jar:2.5.3:compile\n* com.fasterxml.jackson.dataformat:jackson-dataformat-smile=com.fasterxml.jackson.dataformat:jackson-dataformat-smile:jar:2.5.3:compile\n* com.fasterxml.jackson.dataformat:jackson-dataformat-yaml=com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:jar:2.5.3:compile\n* org.yaml:snakeyaml=org.yaml:snakeyaml:jar:1.12:compile\n* com.fasterxml.jackson.dataformat:jackson-dataformat-cbor=com.fasterxml.jackson.dataformat:jackson-dataformat-cbor:jar:2.5.3:compile\n* io.netty:netty=io.netty:netty:jar:3.10.3.Final:compile\n* com.ning:compress-lzf=com.ning:compress-lzf:jar:1.0.2:compile\n* com.tdunning:t-digest=com.tdunning:t-digest:jar:3.0:compile\n* org.hdrhistogram:HdrHistogram=org.hdrhistogram:HdrHistogram:jar:2.1.6:compile\n* org.apache.commons:commons-lang3=org.apache.commons:commons-lang3:jar:3.3.2:compile\n* commons-cli:commons-cli=commons-cli:commons-cli:jar:1.3.1:compile\n* com.twitter:jsr166e=com.twitter:jsr166e:jar:1.1.0:compile\n* org.hamcrest:hamcrest-all=org.hamcrest:hamcrest-all:jar:1.3:test\n* org.apache.lucene:lucene-test-framework=org.apache.lucene:lucene-test-framework:jar:5.2.1:test\n* org.apache.lucene:lucene-codecs=org.apache.lucene:lucene-codecs:jar:5.2.1:test\n* org.apache.lucene:lucene-core=org.apache.lucene:lucene-core:jar:5.2.1:compile\n* com.carrotsearch.randomizedtesting:randomizedtesting-runner=com.carrotsearch.randomizedtesting:randomizedtesting-runner:jar:2.1.16:test\n* junit:junit=junit:junit:jar:4.11:test\n* org.apache.ant:ant=org.apache.ant:ant:jar:1.8.2:test\n```\n\nThe right one:\n\n```\n[DEBUG]   (f) projectArtifactMap = \n* org.elasticsearch.distribution.shaded:elasticsearch=org.elasticsearch.distribution.shaded:elasticsearch:jar:2.0.0-beta1-SNAPSHOT:compile\n* org.apache.lucene:lucene-core=org.apache.lucene:lucene-core:jar:5.2.1:compile\n* org.apache.lucene:lucene-backward-codecs=org.apache.lucene:lucene-backward-codecs:jar:5.2.1:compile\n* org.apache.lucene:lucene-analyzers-common=org.apache.lucene:lucene-analyzers-common:jar:5.2.1:compile\n* org.apache.lucene:lucene-queries=org.apache.lucene:lucene-queries:jar:5.2.1:compile\n* org.apache.lucene:lucene-memory=org.apache.lucene:lucene-memory:jar:5.2.1:compile\n* org.apache.lucene:lucene-highlighter=org.apache.lucene:lucene-highlighter:jar:5.2.1:compile\n* org.apache.lucene:lucene-queryparser=org.apache.lucene:lucene-queryparser:jar:5.2.1:compile\n* org.apache.lucene:lucene-sandbox=org.apache.lucene:lucene-sandbox:jar:5.2.1:compile\n* org.apache.lucene:lucene-suggest=org.apache.lucene:lucene-suggest:jar:5.2.1:compile\n* org.apache.lucene:lucene-misc=org.apache.lucene:lucene-misc:jar:5.2.1:compile\n* org.apache.lucene:lucene-join=org.apache.lucene:lucene-join:jar:5.2.1:compile\n* org.apache.lucene:lucene-grouping=org.apache.lucene:lucene-grouping:jar:5.2.1:compile\n* org.apache.lucene:lucene-spatial=org.apache.lucene:lucene-spatial:jar:5.2.1:compile\n* com.spatial4j:spatial4j=com.spatial4j:spatial4j:jar:0.4.1:compile\n* org.hamcrest:hamcrest-all=org.hamcrest:hamcrest-all:jar:1.3:test\n* org.apache.lucene:lucene-test-framework=org.apache.lucene:lucene-test-framework:jar:5.2.1:test\n* org.apache.lucene:lucene-codecs=org.apache.lucene:lucene-codecs:jar:5.2.1:test\n* com.carrotsearch.randomizedtesting:randomizedtesting-runner=com.carrotsearch.randomizedtesting:randomizedtesting-runner:jar:2.1.16:test\n* junit:junit=junit:junit:jar:4.11:test\n* org.apache.ant:ant=org.apache.ant:ant:jar:1.8.2:test\n```\n\nTrying to find a fix now...\n"},{"date":"2015-08-11T12:15:14Z","author":"dadoonet","text":"Adding here some notes on how to quickly debug that (might help for the future).\n\nJust run:\n\n``` sh\nmvn dependency:tree -pl :smoke-test-shaded\n```\n\n```\n[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ smoke-test-shaded ---\n[INFO] org.elasticsearch.qa:smoke-test-shaded:jar:2.0.0-beta1-SNAPSHOT\n[INFO] +- org.elasticsearch.distribution.shaded:elasticsearch:jar:2.0.0-beta1-SNAPSHOT:compile\n[INFO] |  +- org.apache.lucene:lucene-core:jar:5.2.1:compile\n[INFO] |  +- org.apache.lucene:lucene-backward-codecs:jar:5.2.1:compile\n[INFO] |  +- org.apache.lucene:lucene-analyzers-common:jar:5.2.1:compile\n[INFO] |  +- org.apache.lucene:lucene-queries:jar:5.2.1:compile\n[INFO] |  +- org.apache.lucene:lucene-memory:jar:5.2.1:compile\n[INFO] |  +- org.apache.lucene:lucene-highlighter:jar:5.2.1:compile\n[INFO] |  +- org.apache.lucene:lucene-queryparser:jar:5.2.1:compile\n[INFO] |  +- org.apache.lucene:lucene-sandbox:jar:5.2.1:compile\n[INFO] |  +- org.apache.lucene:lucene-suggest:jar:5.2.1:compile\n[INFO] |  +- org.apache.lucene:lucene-misc:jar:5.2.1:compile\n[INFO] |  +- org.apache.lucene:lucene-join:jar:5.2.1:compile\n[INFO] |  +- org.apache.lucene:lucene-grouping:jar:5.2.1:compile\n[INFO] |  +- org.apache.lucene:lucene-spatial:jar:5.2.1:compile\n[INFO] |  \\- com.spatial4j:spatial4j:jar:0.4.1:compile\n[INFO] +- org.hamcrest:hamcrest-all:jar:1.3:test\n[INFO] \\- org.apache.lucene:lucene-test-framework:jar:5.2.1:test\n[INFO]    +- org.apache.lucene:lucene-codecs:jar:5.2.1:test\n[INFO]    +- com.carrotsearch.randomizedtesting:randomizedtesting-runner:jar:2.1.16:test\n[INFO]    +- junit:junit:jar:4.11:test\n[INFO]    \\- org.apache.ant:ant:jar:1.8.2:test\n```\n\nand \n\n``` sh\nmvn dependency:tree -pl org.elasticsearch.distribution.shaded:elasticsearch,:smoke-test-shaded\n```\n\n```\n[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ smoke-test-shaded ---\n[INFO] org.elasticsearch.qa:smoke-test-shaded:jar:2.0.0-beta1-SNAPSHOT\n[INFO] +- org.elasticsearch.distribution.shaded:elasticsearch:jar:2.0.0-beta1-SNAPSHOT:compile\n[INFO] |  \\- org.elasticsearch:elasticsearch:jar:2.0.0-beta1-SNAPSHOT:compile\n[INFO] |     +- org.apache.lucene:lucene-backward-codecs:jar:5.2.1:compile\n[INFO] |     +- org.apache.lucene:lucene-analyzers-common:jar:5.2.1:compile\n[INFO] |     +- org.apache.lucene:lucene-queries:jar:5.2.1:compile\n[INFO] |     +- org.apache.lucene:lucene-memory:jar:5.2.1:compile\n[INFO] |     +- org.apache.lucene:lucene-highlighter:jar:5.2.1:compile\n[INFO] |     +- org.apache.lucene:lucene-queryparser:jar:5.2.1:compile\n[INFO] |     |  \\- org.apache.lucene:lucene-sandbox:jar:5.2.1:compile\n[INFO] |     +- org.apache.lucene:lucene-suggest:jar:5.2.1:compile\n[INFO] |     |  \\- org.apache.lucene:lucene-misc:jar:5.2.1:compile\n[INFO] |     +- org.apache.lucene:lucene-join:jar:5.2.1:compile\n[INFO] |     |  \\- org.apache.lucene:lucene-grouping:jar:5.2.1:compile\n[INFO] |     +- org.apache.lucene:lucene-spatial:jar:5.2.1:compile\n[INFO] |     |  \\- com.spatial4j:spatial4j:jar:0.4.1:compile\n[INFO] |     +- com.google.guava:guava:jar:18.0:compile\n[INFO] |     +- com.carrotsearch:hppc:jar:0.7.1:compile\n[INFO] |     +- joda-time:joda-time:jar:2.8:compile\n[INFO] |     +- org.joda:joda-convert:jar:1.2:compile\n[INFO] |     +- com.fasterxml.jackson.core:jackson-core:jar:2.5.3:compile\n[INFO] |     +- com.fasterxml.jackson.dataformat:jackson-dataformat-smile:jar:2.5.3:compile\n[INFO] |     +- com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:jar:2.5.3:compile\n[INFO] |     |  \\- org.yaml:snakeyaml:jar:1.12:compile\n[INFO] |     +- com.fasterxml.jackson.dataformat:jackson-dataformat-cbor:jar:2.5.3:compile\n[INFO] |     +- io.netty:netty:jar:3.10.3.Final:compile\n[INFO] |     +- com.ning:compress-lzf:jar:1.0.2:compile\n[INFO] |     +- com.tdunning:t-digest:jar:3.0:compile\n[INFO] |     +- org.hdrhistogram:HdrHistogram:jar:2.1.6:compile\n[INFO] |     +- org.apache.commons:commons-lang3:jar:3.3.2:compile\n[INFO] |     +- commons-cli:commons-cli:jar:1.3.1:compile\n[INFO] |     \\- com.twitter:jsr166e:jar:1.1.0:compile\n[INFO] +- org.hamcrest:hamcrest-all:jar:1.3:test\n[INFO] \\- org.apache.lucene:lucene-test-framework:jar:5.2.1:test\n[INFO]    +- org.apache.lucene:lucene-codecs:jar:5.2.1:test\n[INFO]    +- org.apache.lucene:lucene-core:jar:5.2.1:compile\n[INFO]    +- com.carrotsearch.randomizedtesting:randomizedtesting-runner:jar:2.1.16:test\n[INFO]    +- junit:junit:jar:4.11:test\n[INFO]    \\- org.apache.ant:ant:jar:1.8.2:test\n```\n"},{"date":"2015-08-13T16:52:43Z","author":"dadoonet","text":"Reopening as the change has been reverted\n"},{"date":"2015-09-08T19:28:43Z","author":"dadoonet","text":"We removed the shaded module so it's not an issue anymore... :p \n\nClosing\n"}],"reopen_on":"2015-08-13T16:52:43Z","opened_by":"imotov","closed_on":"2015-09-08T19:28:43Z","description":"Build fails with maven 3.3.1 and 3.3.3. To reproduce, install one of the 3.3.x versions of maven and run `mvn clean verify` in the root directory of the project. The build will fail in the QA: Smoke Test Shaded Jar module with the following error:\n\n```\nStarted J0 PID(99979@flea.local).\nSuite: org.elasticsearch.shaded.test.ShadedIT\n  2> NOTE: reproduce with: ant test  -Dtestcase=ShadedIT -Dtests.method=testJodaIsNotOnTheCP -Dtests.seed=2F4D23A7462CF921 -Dtests.locale= -Dtests.timezone=Asia\/Baku -Dtests.asserts=true -Dtests.file.encoding=UTF-8\nFAILURE 0.06s | ShadedIT.testJodaIsNotOnTheCP <<<\n  > Throwable #1: junit.framework.AssertionFailedError: Expected an exception but the test passed: java.lang.ClassNotFoundException\n  > at __randomizedtesting.SeedInfo.seed([2F4D23A7462CF921:3A9404F1F69FD80]:0)\n  > at junit.framework.Assert.fail(Assert.java:57)\n  > at java.lang.Thread.run(Thread.java:745)\n  2> NOTE: reproduce with: ant test  -Dtestcase=ShadedIT -Dtests.method=testGuavaIsNotOnTheCP -Dtests.seed=2F4D23A7462CF921 -Dtests.locale= -Dtests.timezone=Asia\/Baku -Dtests.asserts=true -Dtests.file.encoding=UTF-8\nFAILURE 0.01s | ShadedIT.testGuavaIsNotOnTheCP <<<\n  > Throwable #1: junit.framework.AssertionFailedError: Expected an exception but the test passed: java.lang.ClassNotFoundException\n  > at __randomizedtesting.SeedInfo.seed([2F4D23A7462CF921:C2502FD54D83433D]:0)\n  > at junit.framework.Assert.fail(Assert.java:57)\n  > at java.lang.Thread.run(Thread.java:745)\n  2> NOTE: reproduce with: ant test  -Dtestcase=ShadedIT -Dtests.method=testjsr166eIsNotOnTheCP -Dtests.seed=2F4D23A7462CF921 -Dtests.locale= -Dtests.timezone=Asia\/Baku -Dtests.asserts=true -Dtests.file.encoding=UTF-8\nFAILURE 0.01s | ShadedIT.testjsr166eIsNotOnTheCP <<<\n  > Throwable #1: junit.framework.AssertionFailedError: Expected an exception but the test passed: java.lang.ClassNotFoundException\n  > at __randomizedtesting.SeedInfo.seed([2F4D23A7462CF921:35593286F4269392]:0)\n  > at junit.framework.Assert.fail(Assert.java:57)\n  > at java.lang.Thread.run(Thread.java:745)\n  2> NOTE: leaving temporary files on disk at: \/Users\/Shared\/Jenkins\/Home\/workspace\/elasticsearch-master\/qa\/smoke-test-shaded\/target\/J0\/temp\/org.elasticsearch.shaded.test.ShadedIT_2F4D23A7462CF921-001\n  2> NOTE: test params are: codec=CheapBastard, sim=DefaultSimilarity, locale=, timezone=Asia\/Baku\n  2> NOTE: Mac OS X 10.10.4 x86_64\/Oracle Corporation 1.8.0_25 (64-bit)\/cpus=8,threads=1,free=482137936,total=514850816\n  2> NOTE: All tests run in this JVM: [ShadedIT]\nCompleted [1\/1] in 6.61s, 5 tests, 3 failures <<< FAILURES!\n\n\nTests with failures:\n  - org.elasticsearch.shaded.test.ShadedIT.testJodaIsNotOnTheCP\n  - org.elasticsearch.shaded.test.ShadedIT.testGuavaIsNotOnTheCP\n  - org.elasticsearch.shaded.test.ShadedIT.testjsr166eIsNotOnTheCP\n```\n\nPlease note that build doesn't fail with maven 3.2.x and it doesn't fail if mvn command is executed inside the qa\/smoke-test-sharded directory. Only when the build is started from the root directory the error above can be observed. \n","id":"100198732","title":"Build of QA: Smoke Test Shaded Jar fails under maven 3.3.x","reopen_by":"dadoonet","opened_on":"2015-08-11T00:38:50Z","closed_by":"dadoonet"},{"number":"12756","comments":[{"date":"2015-08-10T12:37:56Z","author":"jpountz","text":"Actually I was recently thinking about changing this query so that we always return a `TermsQuery`. It would break backward compatibility, but I'm not too happy with the current state where `terms`, when used as a `query`, is essentially a shortcut for creating a `bool` query. I would rather make the generated `query` and `filter` have more consistent behaviours and recommend to use `bool` instead of `terms` if `disable_coord` and\/or `min_shoul_match` are required?\n"},{"date":"2015-08-10T13:34:47Z","author":"clintongormley","text":"@jpountz i agree with your comment - this should be a simple query.  All the `terms` query buys you is a more compact syntax when looking for lots of terms.  I would guess that min_should_match and coord is more useful when dealing with only a few terms, so using a bool query instead would be ok\n"},{"date":"2015-08-11T08:34:03Z","author":"jpountz","text":"Ohhh I understand why you opened this PR now: this used to be supported in 1.x but support for this parameter was accidentally lost in 2.x. So we have a bw compat break. Then I'm inclined to merge the change and to later think about how we could deprecate min_should_match\/disable_coord.\n"}],"reopen_on":"2015-08-09T23:43:01Z","opened_by":"keety","closed_on":"2015-08-11T14:13:58Z","description":"Trivial change to enable `disable_coord` support for TermsQuery .\n\nAdded test case to verify support for options in  `TermsQuery` Parser.\n\nCloses #12755\n","id":"99940185","title":"Add support for `disable_coord` param to `terms` query","reopen_by":"keety","opened_on":"2015-08-09T23:34:01Z","closed_by":"jpountz"},{"number":"12716","comments":[{"date":"2015-08-07T11:31:07Z","author":"clintongormley","text":"Hi @rvrignaud \n\nAs a workaround, you can set this in your elasticsearch.yml:\n\n```\nnode.max_local_storage_nodes: 1\n```\n"},{"date":"2015-08-07T11:50:41Z","author":"rvrignaud","text":"Hey @clintongormley,\nI already have max_local_storage_nodes set to 1 but that doesn't prevent the second java process to start and to allocate memory (XMS and XMX setted to the same value).\n"},{"date":"2015-08-27T13:38:57Z","author":"electrical","text":"With the rpm init script we place a lockfile before starting ES and remove it afterwards.\nIf someone would try a second startup it would find the lockfile and abort directly.\nIt should be a fairly easy fix to implement this in the ubuntu\/debian init script as well.\nSince more things are moving to systemd this will not pose an issue later.\n"},{"date":"2016-06-09T04:55:13Z","author":"jordansissel","text":"I think the problem here is these two lines of the init script:\n- [start-stop-daemon is given `-b` flag](https:\/\/github.com\/elastic\/elasticsearch\/blob\/master\/distribution\/deb\/src\/main\/packaging\/init.d\/elasticsearch#L140)\n- [elasticsearch is given -d flag](https:\/\/github.com\/elastic\/elasticsearch\/blob\/master\/distribution\/deb\/src\/main\/packaging\/init.d\/elasticsearch#L82)\n\nGiven the above, I think Elasticsearch is backgrounding itself (`elasticsearch -d`) and `start-stop-daemon` is also forking, so I _think_ start-stop-daemon is tracking the pid of the parent Elasticsearch process which immediately dies after Elasticsearch daemonizes.\n\nMy suggestion is that one of the two things are changed:\n- Don't give `-b` to start-stop-daemon\n- or, don't give `-d` to elasticsearch.\n"},{"date":"2016-06-09T04:59:01Z","author":"jordansissel","text":"I think this is a dup of https:\/\/github.com\/elastic\/elasticsearch\/issues\/8796\n"},{"date":"2016-09-27T15:33:38Z","author":"dakrone","text":"We now default `node.max_local_storage_nodes` to 1, so closing this.\n"},{"date":"2016-09-27T16:03:06Z","author":"rvrignaud","text":"@dakrone as replied here https:\/\/github.com\/elastic\/elasticsearch\/issues\/12716#issuecomment-128681997 `node.max_local_storage_nodes` set to 1 does not fix the problem. IMHO this should be kept open.\n"},{"date":"2016-09-27T16:16:03Z","author":"dakrone","text":"@rvrignaud ahh I see, my apologies! I think this is related to the daemonization that we do and would have to be fixed there then, I'm re-opening this. Thanks for catching this!\n"}],"reopen_on":"2016-09-27T16:16:03Z","opened_by":"rvrignaud","closed_on":null,"description":"On ubuntu 14.04 for Elasticsearch 1.7.0 debian package,\nIf we start twice elasticsearch very shortly, the pidofproc function doesn't return the PID of the first elasticsearch.\nSo 2 elasticsearch can be started at the same time. In my case this causes an OOM trigger. \n\nHow to reproduce:\n\n```\n# service elasticsearch stop\n# service elasticsearch start; service elasticsearch start; \n```\n","id":"99608655","title":"Elasticsearch ubuntu init script doesn't prevent to start 2 elasticsearch","reopen_by":"dakrone","opened_on":"2015-08-07T08:53:46Z","closed_by":"dakrone"},{"number":"11577","comments":[{"date":"2015-06-10T16:30:22Z","author":"rjernst","text":"LGTM\n"},{"date":"2015-06-17T15:27:41Z","author":"alexksikes","text":"Not sure this entirely solves it. The items have fields that should be mapped to their respective index names as well before fetching. Perhaps we should add `just_name` functionality to TVs API?\n"},{"date":"2015-07-09T11:48:31Z","author":"s1monw","text":"@alexksikes the TV API is pretty much broken and I am not sure if it's fixable with that regard. I really don't know how to fix this since we might reanalyze etc.\n"},{"date":"2015-07-09T12:17:23Z","author":"alexksikes","text":"@s1monw yes so then this is a good enough fix.\n"},{"date":"2015-07-12T10:29:14Z","author":"clintongormley","text":"@s1monw this was autoclosed when we deleted 1.x - could you reopen against master?\n"}],"reopen_on":"2015-07-09T11:42:48Z","opened_by":"s1monw","closed_on":"2015-07-10T21:18:21Z","description":"Closes #11573\n","id":"86955740","title":"Rewrite fields with `just_name` option to their actual index names in MLT","reopen_by":"s1monw","opened_on":"2015-06-10T12:35:08Z","closed_by":"s1monw"},{"number":"11564","comments":[{"date":"2015-06-12T14:04:01Z","author":"clintongormley","text":"@jaymode please could you take a look\n"},{"date":"2015-12-17T23:47:41Z","author":"joshuar","text":"Looks to be still a problem on 2.1.  Regression here?  @GlenRSmith and I can confirm this occurs with the 2.1 release archive.\n"},{"date":"2015-12-17T23:49:53Z","author":"joshuar","text":"It seems to be independent of which config item you want to prompt for:\n\n```\nelasticsearch-2.1.0  bin\/elasticsearch\nEnter value for [cluster.name]: foo\nEnter value for [cluster.name]: bar\n[2015-12-18 10:48:50,219][INFO ][node                     ] [Wendell Vaughn] version[2.1.0], pid[21231], build[72cd1f1\/2015-11-18T22:40:03Z]\n[2015-12-18 10:48:50,220][INFO ][node                     ] [Wendell Vaughn] initializing ...\n[2015-12-18 10:48:50,280][INFO ][plugins                  ] [Wendell Vaughn] loaded [], sites []\n[2015-12-18 10:48:50,304][INFO ][env                      ] [Wendell Vaughn] using [1] data paths, mounts [[\/ (\/dev\/mapper\/fedora_josh--xps13-root)]], net usable_space [79.1gb], net total_space [233.9gb], spins? [no], types [ext4]\n[2015-12-18 10:48:51,761][INFO ][node                     ] [Wendell Vaughn] initialized\n[2015-12-18 10:48:51,762][INFO ][node                     ] [Wendell Vaughn] starting ...\n[2015-12-18 10:48:51,902][INFO ][transport                ] [Wendell Vaughn] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}, {[::1]:9300}\n[2015-12-18 10:48:51,912][INFO ][discovery                ] [Wendell Vaughn] bar\/azOFxYXMQ6muCpGOpqvxSw\n```\n"},{"date":"2015-12-18T00:00:40Z","author":"GlenRSmith","text":"Just confirmed on 2.1.1.\n"},{"date":"2015-12-18T06:52:08Z","author":"rjernst","text":"This is different than the original issue. I think what is happening is we first initialize the settings\/environment in bootstrap so that we can eg init logging, but then when bootstrap creates the node, it passes in a fresh Settings, and the Node constructor again initializes settings\/environment.\n"},{"date":"2016-01-18T20:16:45Z","author":"clintongormley","text":"@jaymode could you take a look at this please?\n"},{"date":"2016-01-19T15:07:27Z","author":"jaymode","text":"As @rjernst said, this is a different issue that the original. `BootstrapCLIParser` was added which extends CLITool. CLITool prepares the settings and environment which includes prompting since CLITools are usually run outside of the bootstrap process. This causes the first prompt that is ignored. The second prompt that is used, comes from `Bootstrap`, which is passed to the node.\n\nPart of the issue is that the `BootstrapCLIParser` sets properties that will change the value of settings. I think we can solve this a few different ways:\n1. Pass in empty settings\/null environment for this CLITool. If we try to create a valid environment here then we have to prepare the settings to ensure we parse the paths from the settings for our directories\n2. Do not use the `CLITool` infrastructure\n3. Prepare the environment in bootstrap, pass to `BootstrapCLIParser`. Re-prepare the settings\/environment, passing in the already prepared settings.\n\n@spinscale @rjernst any thoughts?\n"},{"date":"2016-01-19T19:46:38Z","author":"rjernst","text":"I would say preparing the environment once would be the best option, it will just take some refactoring to pass it through. Running prepare already creates Environment twice (the first time so it can try and load the config file, which might have other paths like plugin path or data path). Really I think we should simply not allow paths to be configured in elasticsearch.yml, instead it should only be through sysprops.  It might be something that could simplify the settings\/env prep, to make this a little easier.\n"},{"date":"2016-02-24T02:40:12Z","author":"jasontedor","text":"I have a fix for this that will come as part of #16579.\n"},{"date":"2016-03-14T00:07:24Z","author":"jasontedor","text":"Closed by #17024.\n"}],"reopen_on":"2015-12-17T23:47:41Z","opened_by":"djschny","closed_on":"2016-03-14T00:07:24Z","description":"With the following configuration\n\n```\nnode.name: ${prompt.text}\n```\n\nElasticsearch 1.6.0 prompts you twice. Once for \"node.name\" and \"name\". Ultimately it uses the second one for the value of the configuration item:\n\n```\ndjschny:elasticsearch-1.6.0 djschny$ ..\/startElastic.sh \nEnter value for [node.name]: foo\nEnter value for [name]: bar\n[2015-06-09 16:10:03,405][INFO ][node                     ] [bar] version[1.6.0], pid[4836], build[cdd3ac4\/2015-06-09T13:36:34Z]\n[2015-06-09 16:10:03,405][INFO ][node                     ] [bar] initializing ...\n```\n","id":"86720842","title":"${prompt.text} and ${prompt.secret} double prompting","reopen_by":"joshuar","opened_on":"2015-06-09T20:13:45Z","closed_by":"jasontedor"},{"number":"11429","comments":[{"date":"2015-05-29T18:48:55Z","author":"clintongormley","text":"Hi @rookie7799 \n\nPlease can you ask questions like this on the forum instead https:\/\/discuss.elastic.co\/c\/elasticsearch\n\nthanks\n"},{"date":"2015-05-29T20:32:13Z","author":"dadoonet","text":"@clintongormley Actually, it looks like a bug to me. \n\n``` java\nSearchRequestBuilder builder = client().prepareSearch(\"index\").setTypes(\"type\");\nSystem.out.println(\"builder = \" + builder);\n```\n\nGives\n\n```\nbuilder = { }\n```\n\nI think this change did that: https:\/\/github.com\/elastic\/elasticsearch\/pull\/9944\/files#diff-8e501ed49a549876267b54efd2347077R170\n\nI pinged @javanna about it. Let's see what he thinks about it but we might need to reopen this issue although it might be a duplicate of #9962.\n"},{"date":"2015-05-29T20:37:28Z","author":"clintongormley","text":"thanks for spotting @dadoonet - reopening\n"},{"date":"2015-05-30T06:49:27Z","author":"javanna","text":"The types do get set, but you don't see them as the output of the `toString`, simply because the toString only prints out the content of what would be the request body, the actual search request and not what would be in the url, like types, index etc. it is something that we'll have to get back to at some point, but kinda expected for now.\n"},{"date":"2015-05-30T07:22:21Z","author":"dadoonet","text":"Thanks @javanna. Should we close this issue? Is this going to be fixed with  https:\/\/github.com\/elastic\/elasticsearch\/issues\/9962?\n"},{"date":"2015-06-02T08:44:56Z","author":"javanna","text":"yes I'd close in favour of #9962, where we have to discuss what `toString` should do in general and follow the same convention everywhere.\n"}],"reopen_on":"2015-05-29T20:37:28Z","opened_by":"rookie7799","closed_on":"2015-06-02T08:44:56Z","description":"Hi,\nWhen I print SearchRequestBuilder to see json the \"types\" : [ ]\nIs this normal ?\n","id":"82526785","title":".prepareSearch(\"index\").setTypes(\"mytype\") does not set types","reopen_by":"clintongormley","opened_on":"2015-05-29T18:03:30Z","closed_by":"javanna"},{"number":"11226","comments":[{"date":"2015-05-19T15:53:57Z","author":"s1monw","text":"I had local failures... need to investigate\n"}],"reopen_on":"2015-05-19T15:53:58Z","opened_by":"kimchy","closed_on":"2015-05-21T07:53:32Z","description":"This test fails since we get into a cycle of trying to recover the primary from replica 1 and replica 2 which are corrupted, and never end up reaching the non corrupted previous primary shard\n","id":"78071171","title":"CorruptedFileTest#testReplicaCorruption fails","reopen_by":"s1monw","opened_on":"2015-05-19T12:39:19Z","closed_by":"s1monw"},{"number":"11170","comments":[{"date":"2015-05-15T18:43:15Z","author":"clintongormley","text":"@shikhar We haven't seen this issue reported before.  From your last comment:\n\n> It might have been due to using a custom Elasticsearch discovery plugin which is purely asynchronous that those 2 bits ended up happening in parallel, and caused the deadlock.\n\nDid you determine whether this happened when you weren't using the custom plugin?\n"},{"date":"2015-05-16T07:44:55Z","author":"shikhar","text":"@clintongormley Not yet, but I'll try to ascertain whether this is still happening by doing a ton of cluster restarts with and without the plugin. I'll reopen if I find this is still an issue. Thanks :)\n"},{"date":"2015-05-19T12:59:15Z","author":"shikhar","text":"I have been able to reproduce this with Zen. The only interesting setting is probably `discovery.zen.publish_timeout=0` which makes it similar to [eskka](http:\/\/github.com\/shikhar\/eskka) in that master does not block for acks before processing more updates. But I'm not sure if this is relevant to the problem at all, just thought I'd mention in case it is.\n\nTo describe what happens, the cluster is stuck in the RED state while starting up. On the master node, there are many errors like:\n\n```\n[2015-05-19 12:48:34,460][WARN ][gateway.local            ] [search45-es1] [mfg-1431690242][0]: failed to list shard stores on node [vrHV_EVZSuSFdCGEC7lUsg]\norg.elasticsearch.action.FailedNodeException: Failed node [vrHV_EVZSuSFdCGEC7lUsg]\n        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.onFailure(TransportNodesOperationAction.java:206)\n        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.access$1000(TransportNodesOperationAction.java:97)\n        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction$4.handleException(TransportNodesOperationAction.java:178)\n        at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:529)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: org.elasticsearch.transport.ReceiveTimeoutTransportException: [search44-es1][inet[\/172.31.240.55:8301]][internal:cluster\/nodes\/indices\/shard\/store[n]] request_id [4273] timed out after [30001ms]\n        ... 4 more\n```\n\nA thread dump from the `vrHV_EVZSuSFdCGEC7lUsg` node that it is complaining about: \nhttps:\/\/gist.github.com\/shikhar\/8ad5c166c6a20458c4d2 -- grepping for `Codec` or `<init>` should reveal the threads where the static initialization deadlock is happening. These threads are shown as RUNNABLE while in `Object.wait()`\n\nMy hunch is that this can happen while the cluster is starting up and an index create event lands (we have some crons that are creating new indexes in the background as a way of reindexing). You can see that one of the threads that is deadlocked landed in the Codec-ey bits via:\n\n```\norg.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewIndices(IndicesClusterStateService.java:311)\n```\n"},{"date":"2015-05-19T13:05:33Z","author":"shikhar","text":"I think the static initialization of the deadlock-prone bits that @uschindler mentioned should happen in a deterministic manner at ES startup, rather than e.g. when creating indexes.\n"},{"date":"2015-05-19T13:10:14Z","author":"clintongormley","text":"@shikhar thanks for coming back to us with the details\n"},{"date":"2015-05-19T13:13:58Z","author":"shikhar","text":"Forgot to mention that this was reproduced on ES 1.5.2\n"},{"date":"2015-05-19T13:24:06Z","author":"uschindler","text":"The discussion when and why this might happen is here:\nhttps:\/\/issues.apache.org\/jira\/browse\/LUCENE-6482\n\nThe proposal is to load the Codec, PostingsFormat and DocValuesFormat in the Elasticsearch Boostrap class. So something like calls to:\n\n``` java\nCodec.reloadCodecs();\nPostingsFormat.reloadPostingsFormats();\n\/\/ and others, see Solr's SolrResourceLoader startup\n```\n\nsomewhere at the beginning of Boostrap class. In Solr this is already done in the startup code. This prevents dependency problems if those index classes are started in a concurrent way.\n"},{"date":"2015-05-19T13:43:16Z","author":"rmuir","text":"Uwe, why should users of lucene have to do this? Tracking down a class loading deadlock like this is something i do not wish on anyone (thank you very much @shikhar for digging here). So I don't like that you need a \"magical sequence\" at the startup of your app, or you might get hangs.\n\nIs there no better way?\n"},{"date":"2015-05-19T14:10:55Z","author":"uschindler","text":"The problem here is that on Elasticsearch startup the big builder is initializing a lot of shit in a unforseeable order, and concurrent in addition. The main problem here is that the Codec\/Postingsformat\/... stuff has a large spaghetti of load-time dependencies, so if called in the wrong order concurrently, it hangs.\n\nTo solve the issue here, my suggestion is to simply call those static initializes at a defined place in Elasticsearch's bootstrap, so it cannot happen that index A is initialized at the same time while index B is also booting up and both are loading SimpleText codec and ElasticsearchFooBar codec in parallel.\n\nWe can then look into Lucene's code to cleanup the loading of codecs. The big problem is the dependency graph and static initializers. The main problem is the Codec.getDefault() value that is initialized with Codec.forName(). I think the problem would be much easier if we would just initialize the default codec with a simple new LuceneXYCodec() instead of SPI on the <clinit> phase. Currently we have code like \"static Codec defaultCodec = forName(\"Lucene50\");\"\n\nSo two steps:\n- prevent deadlocks in ES caused by Lucene for now by explicitely initializing Codec class in Bootstrap.java\n- change Lucene's Codec\/PostingsFormat to initialize the default codecs\/formats without forName(), because this can lead to deadlock. Just use a simple \"new\" call to constructor.\n\nSimilar propblems are regularirly happening in ICU4J: You remember the case where ICU4J caused NullPointerExceptions because it was trying to print a message using the default codec before the default codec was actually loaded and initialized... The problems in Lucene are the same, just happening in multithreaded code.\n"},{"date":"2015-05-19T14:16:36Z","author":"rmuir","text":"> I think the problem would be much easier if we would just initialize the default codec with a simple new LuceneXYCodec() instead of SPI on the phase. Currently we have code like \"static Codec defaultCodec = forName(\"Lucene50\");\"\n\nIs that really all thats needed to fix it in Lucene? If so, +1.\n\nBut for master branch, we shouldn't need a hack in bootstrap, lets just fix it in lucene and upgrade to a newer lucene snapshot jar.\n\nFor any backport fixes (elasticsearch 1.X) your solution is practical. But i would not be upset to see a 4.10.5 either.\n"},{"date":"2015-05-19T14:20:38Z","author":"uschindler","text":"In my opinion this should work fine. We should just not use SPI to initialize the defaults.\n\nThe problem with forName() is: You are loading a lot of classes by that, which initialize themselves and may reference to the class doing the forName(). If we initialize the default codec statically, we dont need to wait for forName() to complete, the default codec is there ASAP. If some index file needs another codec, it can load it with forName() but there is no risk in deadlocking.\n"},{"date":"2015-05-19T14:32:57Z","author":"rmuir","text":"I see. I think this is a lot simpler in 5.x, because forName is not \"abused\" to load \"impersonator\" codecs in tests. You remember, in that case we had a Lucene3x that was writeable and we reordered jar files in tests and all that... all gone now.\n\nSo I think its too tricky to fix as a bugfix in lucene 4.10.x, and we should apply your logic to bootstrap for ES 1.x. But lets fix lucene 5.x as proposed, then simply update our snapshot for ES master.\n"},{"date":"2015-05-19T14:46:14Z","author":"uschindler","text":"I would wish we could write a test to check this out... But as usual, this test would need to run isolated (new and fresh JVM trying to open several indexes in parallel without loading Codec class before).\n\nThank you for remembering me why we did this Codec.forName() for the static initialization! That makes sense. I think we should kill this in 5.x, its a one-line change (and maybe a change in smoketester that validates that the defaultCodec line was updated after release). PostingsFormat is not affected, all of that is caused by Codec's <clinit>.\n\nI analyzed the stack traces provided. What happens in the example is exactly as described: One of the threads is initializing Codec class (<clinit>), as side effect of opening an index. 3 other threads are opening other indexes at same time. Because the Codec class is not yet initialized in clinit, those threads are waiting before SegmentInfos's call to Codec.forName(), which is blocked because the SegmentInfos's call cannot access the Codec class yet. Because of concurrent class loading the newInstance() call in NamedSPILoader to the codec class actually loaded is blocked (too). This happens because JVM has internal locks on that (not 100% sure why and where this blocks, but seems to be the reason).\n"},{"date":"2015-05-19T14:52:06Z","author":"rmuir","text":"Related to that, there was some previous discussion about implementing a static detector here: https:\/\/issues.apache.org\/jira\/browse\/LUCENE-5573\n\nI am just hoping some policeman finds himself bored one day and implements the ASM logic so we could try to detect these, even if it wouldn't find this particular one.\n"},{"date":"2015-05-19T15:40:04Z","author":"uschindler","text":"I know this issue :-) I wonder that there is no Eclipse plugin that can detect this!\n"},{"date":"2015-05-19T16:10:40Z","author":"rmuir","text":"I did a quick survey, nothing exhaustive but I couldn't find anything. I haven't tried to play around with writing a detector. But IMO ideally it would be a policeman-tool like forbidden API, and we just scan for it in builds and fail the build.\n"},{"date":"2015-06-22T17:59:27Z","author":"shikhar","text":"hey @rmuir what are the plans for tackling this? It is fixed in Lucene 5.2.1 (https:\/\/issues.apache.org\/jira\/browse\/LUCENE-6482) -- if only ES 2.0 can use 5.x, there probably either needs to be that ES Bootstrap workaround or a backport to Lucene 4.x\n"},{"date":"2015-06-22T18:05:50Z","author":"uschindler","text":"I would go with the Bootstrap workaround for ES 1.5 and 1.6. This is very easy to implement and there is no need to release several 4.x versions of Lucene. Should I provide a PR?\n"},{"date":"2015-06-23T14:37:43Z","author":"clintongormley","text":"@uschindler yes please! :)\n"},{"date":"2015-06-23T14:37:50Z","author":"s1monw","text":"@uschindler can you open a PR for this against 1.x branch please\n"},{"date":"2015-06-23T17:00:24Z","author":"uschindler","text":"OK, will provide one later! I am currently in the U.S. and busy :-)\n"},{"date":"2015-06-23T17:00:33Z","author":"uschindler","text":"Ah which branch?\n"},{"date":"2015-06-23T17:57:50Z","author":"uschindler","text":"Found it out: 1.x\n"},{"date":"2015-06-23T19:08:18Z","author":"uschindler","text":"I created a PR: #11837\n\nThis one does a \"fake\" Codecs.availableCodecs() on InternalNode startup. I did not do it in the Boostrap, so also people embedding ES can make use of it. In any case, the method call is cheap and does not slowdown, it just returns an immutable list.\n\nIs there a problem that InternalNode directly depends on lucene-core.jar?\n"},{"date":"2015-06-23T19:24:36Z","author":"uschindler","text":"Thanks for merging!\n"}],"reopen_on":"2015-05-19T12:50:53Z","opened_by":"shikhar","closed_on":"2015-06-23T19:31:07Z","description":"From https:\/\/issues.apache.org\/jira\/browse\/LUCENE-6482\n\n> during startup of Elasticsearch nodes: From the last 2 stack traces, you see that there are 2 things happening in parallel: Loading of Codec.class (because an Index was opened), but in parallel, Elasticsearch seems to initialize the PostingsFormats.class in the class CodecModule (Elasticsearch). In my opinion, this should not happen in parallel, but a fix would maybe that CodecModule should also call Codecs.forName() so those classes are initialized sequentially at one single place. The problem with Codec.class and PostingsFormat.class clinit running in parallel in different threads may have similar effects like you have seen in the blog post (Codecs depend on PostingsFormat and some PostingsFormats depend on the Codec class, which then hangs if PostingsFormats and Codecs are initialized from 2 different threads at same time, waiting for each other). But we have no chance to prevent this (unfortunately).\n> \n> I cannot say for sure, but something seems to be fishy while initializing Elasticsearch, because there is too much happening at the same time. In my opinion, Codecs and Postingsformats and Docvalues classes should be initialized sequentially, but I have no idea how to enforce this.\n","id":"76465192","title":"Cluster startup deadlock involving Lucene static initialization bits","reopen_by":"shikhar","opened_on":"2015-05-14T19:30:10Z","closed_by":"mikemccand"},{"number":"10632","comments":[{"date":"2015-04-16T15:18:39Z","author":"dakrone","text":"I believe this may be related to #9866, which was fixed in Lucene 5.1 (see: https:\/\/issues.apache.org\/jira\/browse\/LUCENE-6298). I'll look and see whether it is the same issue.\n"},{"date":"2015-04-16T15:23:37Z","author":"pierrre","text":"My Lucene version is 4.10.4.\n\nWhat should happen if I search \"*\" with a simple_query_string?\nReturn all docs or nothing?\nThe query_string returns all docs.\n"},{"date":"2016-01-17T17:43:49Z","author":"clintongormley","text":"This has been fixed.  \\* will return all docs\n"},{"date":"2016-01-17T17:45:50Z","author":"pierrre","text":"Thank you!\n"},{"date":"2016-02-05T15:36:46Z","author":"pierrre","text":"Actually, this search:\n\n```\n{\n  \"fields\": [],\n  \"query\": {\n    \"simple_query_string\": {\n      \"fields\": [\n        \"shortDescription\"\n      ],\n      \"query\": \"*\"\n    }\n  }\n}\n```\n\nreturns nothing with Elasticsearch 2.2\n\n@clintongormley is the fix for this issue released?\n"},{"date":"2016-02-13T19:39:09Z","author":"clintongormley","text":"Initially I thought this might be related to https:\/\/github.com\/elastic\/elasticsearch\/issues\/13214 but no, a query string of `\"*\"` doesn't match any docs, with or without custom `fields`:\n\n```\nPUT t\/t\/1\n{}\n\nGET t\/_search\n{\n  \"query\": {\n    \"simple_query_string\": {\n      \"query\": \"*\"\n    }\n  }\n}\n\nGET _validate\/query?explain\n{\n  \"query\": {\n    \"simple_query_string\": {\n      \"query\": \"*\"\n    }\n  }\n}\n```\n\nThe explanation returned by the last request is simply `\"\"`\n"},{"date":"2016-09-16T12:34:39Z","author":"cbuescher","text":"I looked into this a bit by adding an IT test. It looks like Lucenes SimpleQueryParser only recognizes a terms with a closing '*' as prefix query. A single wildcard without leading charactes is treated as a regular term in SimpleQueryParser#consumeToken(). This might even be the intended behaviour. For `query_string`  the situation is different since it uses MapperQueryParser internally which has an `allowLeadingWildcard` option. Maybe @jimferenczi or @dakrone have an opinion on whether this can or should be fixed in SimpleQueryParser or is even intended. \n"},{"date":"2016-09-16T15:55:39Z","author":"dakrone","text":"@cbuescher personally I think a SQS query for `*` should be internally rewritten to a `MatchAllDocsQuery`, which I think means this would go into Lucene. If people agree I can work on a patch for the next Lucene release\n"},{"date":"2016-09-19T14:48:17Z","author":"clintongormley","text":"@dakrone not so sure - should it be match all or should it match all docs that have a value for the specified fields?  See #13214 for a similar discussion\n"},{"date":"2016-09-26T14:44:55Z","author":"dakrone","text":"@clintongormley ahh okay, in light of that discussion:\n\n>    `\"*\"` always means match all documents\n>   `\"field:*\"` means find all documents that contain a non-null value in the field (incl empty string)\n\nSince `simple_query_string` can either use the `default_field` or have `fields`\nspecified, I'm thinking we should do:\n- If query is `\"*\"` and no `fields` are specified, treat it as match all\n  documents\n- If query is `\"*\"` and `fields` are specified, query is essentially `field1:*\n  OR field2:* OR field3:*` which matches documents that have non-null values in\n  the field\n\nDo you think that'd be a better solution?\n"},{"date":"2016-10-06T16:05:44Z","author":"clintongormley","text":"@dakrone Given that the end user can't specify fields in the SQS like you can in QS (`user:kimchy`) I've come around to thinking that `*` should always mean match all docs.\n"},{"date":"2016-10-11T21:17:06Z","author":"dakrone","text":"I opened https:\/\/issues.apache.org\/jira\/browse\/LUCENE-7490 for this with a patch\n"},{"date":"2016-10-12T17:59:24Z","author":"dakrone","text":"Looks like this will be in Lucene 6.3, so I'm marking this as \"stalled\" until 6.3 is released and incorporated.\n"},{"date":"2017-03-30T09:38:10Z","author":"weberhofer","text":"I think this issue has been solved with ES 5.3"},{"date":"2017-03-30T10:11:13Z","author":"cbuescher","text":"@weberhofer thanks a lot, I just checked with 5.1 (which is based in Lucene 6.3) and 5.2 and it seems to be fixed. I will close this."}],"reopen_on":"2016-02-13T19:36:58Z","opened_by":"pierrre","closed_on":"2017-03-30T10:11:13Z","description":"My simple_query_string returns nothing with \"*\" search.\n\nIf I use a query_string, it works (it returns hits).\nThe fields and analyzer are the same.\n\nIf I wrap my simple_query_string with a bool+must (or should) query, it works. (???)\n\nIs it a bug?\n\nElasticsearch version: 1.5.1\n","id":"68946027","title":"simple_query_string returns nothing for \"*\" search","reopen_by":"clintongormley","opened_on":"2015-04-16T15:16:18Z","closed_by":"cbuescher"},{"number":"10500","comments":[{"date":"2015-04-12T14:39:06Z","author":"clintongormley","text":"Hi @amarandon \n\nI've just tried this on 1.5.0, and it works correctly. \n\n```\nDELETE myindex\n\nPUT \/myindex?pretty\n\n# Insert mapping with geo_shape type\nPUT \/myindex\/_mapping\/mytype?pretty\n{\n  \"mytype\": {\n    \"properties\": {\n      \"location\": {\n        \"type\": \"geo_shape\",\n        \"tree\": \"quadtree\",\n        \"precision\": \"1m\"\n      }\n    }\n  }\n}\n\n# Check that mapping is correct\nGET \/myindex\/_mapping\/?pretty\n\n\nPUT \/myindex\/mytype\/mydocument?pretty\n{\n  \"location\": {\n    \"type\": \"point\",\n    \"coordinates\": [\n      1.44207,\n      43.59959\n    ]\n  }\n}\n\n# Check that we can retrieve our document with a normal query\n\nGET \/myindex\/mytype\/_search?pretty\n{\n    \"query\": {\n        \"geo_shape\": {\n            \"location\": {\n                \"shape\": {\n                    \"type\": \"envelope\",\n                    \"coordinates\": [[0, 50],[2, 40]]\n                }\n            }\n        }\n    }\n}\n\n# Try to submit the same query to the percolator. Works\nPUT \/myindex\/.percolator\/myquery?pretty\n{\n    \"query\": {\n        \"geo_shape\": {\n            \"location\": {\n                \"shape\": {\n                    \"type\": \"envelope\",\n                    \"coordinates\": [[0, 50],[2, 40]]\n                }\n            }\n        }\n    }\n}\n\n# Percolate request works too\nPOST myindex\/mytype\/_percolate\n{\n  \"doc\": {\n    \"location\": {\n      \"type\": \"point\",\n      \"coordinates\": [\n        1.44207,\n        43.59959\n      ]\n    }\n  }\n}\n```\n"},{"date":"2015-04-12T15:07:44Z","author":"amarandon","text":"Hi @clintongormley Thanks for trying it out. I found out that the issue is triggered by having `index.percolator.map_unmapped_fields_as_string: true` in my config file. I'm in the process of migrating an app built against an earlier version of Elasticsearch and found out that I had to enable that option to keep it working because not all the percolator queries we record have corresponding mappings.\n"},{"date":"2015-04-14T06:26:19Z","author":"amarandon","text":"@clintongormley Does it still work for you with `index.percolator.map_unmapped_fields_as_string: true` in your config file?\n"},{"date":"2015-04-14T13:22:03Z","author":"clintongormley","text":"@amarandon if you're using that option, then I'm not surprised it fails... A geoshape query can't work on a string field.  You need to have the field specified in the mapping before you can create a percolator which uses it.  Otherwise (with that setting enabled) it will assume that the missing field is a string, and... fail\n"},{"date":"2015-04-14T13:49:57Z","author":"amarandon","text":"@clintongormley But I do have the field specified in the mapping before I create the percolator which uses it. In the test script I provided, we create that mapping explicitly and even check that it's been properly created with a GET request before trying to create a percolator query against it. In other words the geo_shape field is not unmapped and shouldn't be affected by that option.\n"},{"date":"2015-04-14T14:17:44Z","author":"clintongormley","text":"Gotcha!  And I can recreate, too.  Agreed, this is a bug.\n\n@martijnvg please could you take a look\n\nHere's the full recreation:\n\n```\nPUT \/myindex?pretty\n{\n  \"settings\": {\n    \"index.percolator.map_unmapped_fields_as_string\":true\n  }\n}\n\n# Insert mapping with geo_shape type\nPUT \/myindex\/_mapping\/mytype?pretty\n{\n  \"mytype\": {\n    \"properties\": {\n      \"location\": {\n        \"type\": \"geo_shape\",\n        \"tree\": \"quadtree\",\n        \"precision\": \"1m\"\n      }\n    }\n  }\n}\n\n# Check that mapping is correct\nGET \/myindex\/_mapping\/?pretty\n\n\nPUT \/myindex\/mytype\/mydocument?pretty\n{\n  \"location\": {\n    \"type\": \"point\",\n    \"coordinates\": [\n      1.44207,\n      43.59959\n    ]\n  }\n}\n\n# Check that we can retrieve our document with a normal query\n\nGET \/myindex\/mytype\/_search?pretty\n{\n    \"query\": {\n        \"geo_shape\": {\n            \"location\": {\n                \"shape\": {\n                    \"type\": \"envelope\",\n                    \"coordinates\": [[0, 50],[2, 40]]\n                }\n            }\n        }\n    }\n}\n\n# Try to submit the same query to the percolator. Works\nPUT \/myindex\/.percolator\/myquery?pretty\n{\n    \"query\": {\n        \"geo_shape\": {\n            \"location\": {\n                \"shape\": {\n                    \"type\": \"envelope\",\n                    \"coordinates\": [[0, 50],[2, 40]]\n                }\n            }\n        }\n    }\n}\n\n# Percolate request works too\nPOST myindex\/mytype\/_percolate\n{\n  \"doc\": {\n    \"location\": {\n      \"type\": \"point\",\n      \"coordinates\": [\n        1.44207,\n        43.59959\n      ]\n    }\n  }\n}\n```\n"},{"date":"2016-01-17T16:53:56Z","author":"clintongormley","text":"This still fails in 2.2 and master, but it fails when trying to PUT the percolator query with `location is not a geoshape`.\n\n@martijnvg are percolator queries still parse-once, or are they parsed on each execution? If the latter, then could we just remove the `index.percolator.map_unmapped_fields_as_string` setting?\n"},{"date":"2016-01-17T21:15:08Z","author":"martijnvg","text":"@clintongormley Percolator queries are still parsed once. \n\nThis issue was caused by a bug that if the 'map unmapped fields as strings' was enabled it would even substitute found fields with string fields! The `geo_shape` query has a hard check if what type of field is being returned from the mapping as therefor fails. Luckily this is easy to fix: #16043\n"}],"reopen_on":"2015-04-14T14:16:46Z","opened_by":"amarandon","closed_on":"2016-01-29T10:57:22Z","description":"Here is a script to reproduce the problem:\n\n``` shell\nES_SERVER=http:\/\/localhost:9200\ncurl -XPUT $ES_SERVER\/myindex?pretty\n\n# Insert mapping with geo_shape type\ncurl -XPUT $ES_SERVER\/myindex\/_mapping\/mytype?pretty -d '\n{\n    \"mytype\" : {\n        \"properties\": {\n            \"location\": {\n                \"type\": \"geo_shape\",\n                \"tree\": \"quadtree\",\n                \"precision\": \"1m\"\n            }\n        }\n    }\n}\n'\n\n# Check that mapping is correct\ncurl -XGET $ES_SERVER\/myindex\/_mapping\/?pretty\n\ncurl -XPUT $ES_SERVER\/myindex\/mytype\/mydocument?pretty -d'\n{\n    \"location\" : {\n        \"type\" : \"point\",\n        \"coordinates\" : [1.44207, 43.59959]\n    }\n}\n'\n\n# Check that we can retrieve our document with a normal query\ncurl -XGET $ES_SERVER\/myindex\/mytype\/_search?pretty -d '\n{\n    \"query\": {\n        \"geo_shape\": {\n            \"location\": {\n                \"shape\": {\n                    \"type\": \"envelope\",\n                    \"coordinates\": [[0, 50],[2, 40]]\n                }\n            }\n        }\n    }\n}\n'\n# Try to submit the same query to the percolator. FAIL!\ncurl -XPUT $ES_SERVER\/myindex\/.percolator\/myquery?pretty -d '\n{\n    \"query\": {\n        \"geo_shape\": {\n            \"location\": {\n                \"shape\": {\n                    \"type\": \"envelope\",\n                    \"coordinates\": [[0, 50],[2, 40]]\n                }\n            }\n        }\n    }\n}\n'\n\n```\n\nEverything goes as expected except the last request which returns:\n\n``` json\n{\n  \"error\" : \"PercolatorException[[myindex] failed to parse query [myquery]]; nested: QueryParsingException[[myindex] Field [location] is not a geo_shape]; \",\n  \"status\" : 500\n}\n```\n\nFollowing the documentation, it seems that it should have worked. I'll try to figure out the problem and submit a patch (whether it turns out to be a documentation patch or a code patch), but it would be great if someone could confirm if it's a bug or a misuse from my end.\n","id":"67314895","title":"Cannot put percolator query on geo_shape field","reopen_by":"clintongormley","opened_on":"2015-04-09T09:04:49Z","closed_by":"martijnvg"},{"number":"10361","comments":[{"date":"2015-04-05T14:37:36Z","author":"clintongormley","text":"Hi @nithyanv \n\nWhen I try out your examples, the `_ttl` looks correct.  I wonder if you ran an update request on this document?\n"},{"date":"2015-04-06T12:48:33Z","author":"nithyanv","text":"We are populating using bulk input. Sometimes, we see overwrites (i.e) version is incremented some of our records may have same _id in the test data. But timestamp is the same even in these records. How does update affect _ttl?\n"},{"date":"2015-04-06T18:26:57Z","author":"clintongormley","text":"@nithyanv an `update` request could mess with the TTL, depending on what you set.\n\nPerhaps check the `version` of the document with the bad TTL value? See if it is `1` or higher.  Also try inserting a new document with the same timestamp, and see if it is also reflecting the wrong TTL?\n\nIt seems to work for me, so I'm trying to figure out what is different about your setup.\n"},{"date":"2015-04-21T11:37:42Z","author":"nithyanv","text":"@clintongormley I believe it is related to Update. We were also getting _version conflict as multiple threads were updating the same record over and over again. That was resolved with setting retryOnconflict(). But now we still get \"org.elasticsearch.index.mapper.MapperParsingException: failed to parse [_ttl]\" when writing over the same record. This is even though document is well within the ttl.\n"},{"date":"2015-06-23T18:18:10Z","author":"clintongormley","text":"Closing in favour of #11809\n"},{"date":"2015-06-26T13:25:33Z","author":"clintongormley","text":"I was incorrect - #11809 doesn't fix this issue.  Reopening.\n\nRelated to #11802 \n"},{"date":"2016-05-12T12:56:09Z","author":"clintongormley","text":"Closing in favour of #18280\n"}],"reopen_on":"2015-06-26T13:25:33Z","opened_by":"nithyanv","closed_on":"2016-05-12T12:56:09Z","description":"Hi,\n\nI believe _ttl is counting down faster than it should.  \n\nI have the following resultset for an index. \n\n{\n_index: some_index\n_type: some_type\n_id: some_id\n_score: 1\nfields: {\nmy_id: some_Id\nmy_timestamp: [\n2015-03-26 16:05:00+0530\n]\n_ttl: 1581673831\n}\n\nSince today is 01\/04\/15, the document is around 5 days old. However, if we were to check _ttl (1581673831 milliseconds = 18.3 days left). How is that possible? It should be around 25 days to go The index was mapped with the _template mapping given below. I have also given the _mapping that was set up dynamically by Elastic when data was fed into it. Please note that _ttl is set correctly as 2592000000 milliseconds = 30 days as per _template mapping.\n\nFor the following template mapping,\n\n \"my_template\" : {\n    \"order\" : 1,\n    \"template\" : \"my_index*\",\n    \"settings\" : {\n      \"index.mapping.allow_type_wrapper\" : \"true\",\n      \"index.refresh_interval\" : \"5s\",\n      \"index.store.compress.stored\" : \"true\",\n      \"index.cache.field.type\" : \"soft\"\n    },\n    \"mappings\" : {\n      \"_default_\" : {\n        \"_timestamp\" : {\n          \"enabled\" : true,\n          \"path\" : \"my_timestamp\",\n          \"format\" : \"YYYY-MM-dd HH:mm:ssZ\"\n        },\n        \"_ttl\" : {\n          \"enabled\" : true,\n          \"default\" : \"30d\"\n        },\n        \"properties\" : {\n          \"my_timestamp\" : {\n            \"format\" : \"YYYY-MM-dd HH:mm:ssZ\",\n            \"type\" : \"date\"\n          },\n          \"my_id\" : {\n            \"index\" : \"not_analyzed\",\n            \"type\" : \"string\"\n          }\n        },\n        \"_all\" : {\n          \"enabled\" : false\n        }\n      }\n    },\n    \"aliases\" : { }\n  }\n}\n\nIt has the following index mapping,\n\n \"my_index_2015.03.26\" : {\n    \"mappings\" : {\n      \"mytype1\" : {\n        \"_all\" : {\n          \"enabled\" : false\n        },\n        \"_timestamp\" : {\n          \"enabled\" : true,\n          \"path\" : \"my_timestamp\",\n          \"format\" : \"YYYY-MM-dd HH:mm:ssZ\"\n        },\n        \"_ttl\" : {\n          \"enabled\" : true,\n          \"default\" : 2592000000\n        },\n        \"properties\" : {\n          \"my_id\" : {\n            \"type\" : \"string\",\n            \"index\" : \"not_analyzed\"\n          },\n          \"my_timestamp\" : {\n            \"type\" : \"date\",\n            \"format\" : \"YYYY-MM-dd HH:mm:ssZ\"\n          }\n        }\n      }\n    }\n  }\n}\n","id":"65622530","title":"_ttl counting down faster than expected","reopen_by":"clintongormley","opened_on":"2015-04-01T07:27:08Z","closed_by":"clintongormley"},{"number":"10318","comments":[{"date":"2015-03-30T19:53:17Z","author":"bleskes","text":"@EikeDehling can you enable debug logging for `indices.cluster` and grep the logs for the output of this line? I want to see what the difference is between the two sources . A gist will be great.\n\n```\nlogger.debug(\"[{}] parsed mapping [{}], and got different sources\\noriginal:\\n{}\\nparsed:\\n{}\", index, mappingType, mappingSource, mapperService.documentMapper(mappingType).mappingSource());\n```\n"},{"date":"2015-03-31T08:43:42Z","author":"EikeDehling","text":"@bleskes Thanks for the quick reponse!\n\nGist here: https:\/\/gist.github.com\/EikeDehling\/129aa3f8213ad8552f49\n\nThe difference in mapping appears to be in nested elements, apparently they are not ordered alphbetically? The difference in serialisation is under posting.properties.body.fields._text_.fielddata , there entries there are ordered differently in the original\/parsed version.\n"},{"date":"2015-03-31T17:52:13Z","author":"EikeDehling","text":"This gist is a bit easier to read:\n\nhttps:\/\/gist.github.com\/EikeDehling\/fc1289cc443b7acdc3f4\n\nThe issue is under the key `posting.properties.body.fields._text_.fielddata` : Ordering is different for the original\/parsed mapping.\n"},{"date":"2015-03-31T19:27:29Z","author":"bleskes","text":"@EikeDehling thx for that. It's accurate. The problem lies in the way the field data settings are rendered:\n\nhttps:\/\/github.com\/elastic\/elasticsearch\/blob\/master\/src\/main\/java\/org\/elasticsearch\/index\/mapper\/core\/AbstractFieldMapper.java#L756\n\n```\nbuilder.field(\"fielddata\", (Map) fieldDataType.getSettings().getAsMap());\n```\n\nThe order of the keys in that map is arbitrary (practically). It may be different between master and nodes causing this endless loop.\n\nTo work around this, you can set `indices.cluster.send_refresh_mapping` to false (requires node restart). This will disable the sending of mapping refresh instructions. You must remember to remove this settings before you upgrade to the next ES version, which will have a fix for this.\n"},{"date":"2015-03-31T19:36:04Z","author":"bleskes","text":"@EikeDehling do you run on Java8 by any chance? (wondering to better understand how frequently this can happen)\n"},{"date":"2015-04-01T08:25:01Z","author":"EikeDehling","text":"We're running Java7, 1.7.0_45\n\nThanks for the tip about settings.\n\nI also found that line of code indeed, i'll try and make a patch\/test.\n"},{"date":"2015-04-01T08:32:54Z","author":"bleskes","text":"Cool. If you wait an hour or two, I\u2019ll probably make a PR with a fix.\n\n> On 01 Apr 2015, at 10:25, EikeDehling notifications@github.com wrote:\n> \n> We're running Java7, 1.7.0_45\n> \n> Thanks for the tip about settings.\n> \n> I also found that line of code indeed, i'll try and make a patch\/test.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n"},{"date":"2015-04-01T08:45:12Z","author":"EikeDehling","text":"This is my initial patch+unit test, happy to compare to what you're producing. Sorry, i'm not that handy with github\/PR's yet.\n\nhttps:\/\/gist.github.com\/EikeDehling\/2e34a78a54de646b71ca\n\nAny chance there will also be a 1.4 release with a fix?\n"},{"date":"2015-04-01T09:24:24Z","author":"wkoot","text":"@bleskes You said that the `indices.cluster.send_refresh_mapping` requires node restart, what would the effect be if you only have a few (say half of) the nodes which have this setting set to false?\n"},{"date":"2015-04-01T09:28:23Z","author":"bleskes","text":"Then the other half might still send refresh mapping to the master. You need it on all data nodes. But you can do a rolling restart, one by one.\n\n> On 01 Apr 2015, at 11:24, wkoot notifications@github.com wrote:\n> \n> @bleskes You said that the indices.cluster.send_refresh_mapping requires node restart, what would the effect be if you only have a few (say half of) the nodes which have this setting set to false?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub.\n"},{"date":"2015-04-01T09:47:52Z","author":"EikeDehling","text":"I am trying this fix in our staging environment, i'll let you know if that fixes the issue.\n\nhttps:\/\/github.com\/EikeDehling\/elasticsearch\/commit\/cc79d71bbc4d55cb12a50df2acc67ca6ba4ac5dc\n"},{"date":"2015-04-01T11:43:51Z","author":"bleskes","text":"@EikeDehling looks good can you make a PR ? see https:\/\/www.elastic.co\/contributing-to-elasticsearch . Also it would be great if you simplify the test and add random keys (but we can iterate on the PR). \n"},{"date":"2015-04-01T14:52:53Z","author":"EikeDehling","text":"I made a PR, and afterwards signed the contributor license, i hope that's ok.\n\nI randomized the test and simplified a bit, happy to hear suggestions for improvements.\n"}],"reopen_on":"2015-04-01T14:52:57Z","opened_by":"EikeDehling","closed_on":"2015-04-02T20:01:47Z","description":"- We're running ES 1.4.2\n- Can sometimes be solved by closing\/re-opening affected indices, but the issue usually returns prettys soon\n- This \"clogs\" up pending tasks and some tasks get stuck \/ cluster actions will now always timeout (e.g. loading\/removing warmers)\n- This consumes all network capacity on the ES nodes\n\nhttps:\/\/gist.github.com\/EikeDehling\/a015a5137ac5d99dc850\n\n[buzzcapture@hermes ~]$ curl http:\/\/artemis3:9200\/_cat\/pending_tasks\n18149502    1s HIGH   refresh-mapping [postings-5360000000][[posting]] \n18149503 745ms HIGH   refresh-mapping [postings-5500000000][[posting]] \n18149509 736ms HIGH   refresh-mapping [postings-5360000000][[posting]] \n18149504 737ms HIGH   refresh-mapping [postings-4180000000][[posting]] \n18149510 735ms HIGH   refresh-mapping [postings-5190000000][[posting]] \n18149512 735ms HIGH   refresh-mapping [postings-5430000000][[posting]] \n18149506 736ms HIGH   refresh-mapping [postings-5430000000][[posting]] \n18149505 736ms HIGH   refresh-mapping [postings-5500000000][[posting]] \n18149511 735ms HIGH   refresh-mapping [postings-5430000000][[posting]] \n18149515 732ms HIGH   refresh-mapping [postings-5430000000][[posting]] \n18149519 290ms HIGH   refresh-mapping [postings-5430000000][[posting]] \n18149521 289ms HIGH   refresh-mapping [postings-5190000000][[posting]] \n18149513 734ms HIGH   refresh-mapping [postings-5100000000][[posting]] \n18149525 287ms HIGH   refresh-mapping [postings-5100000000][[posting]] \n18149507 736ms HIGH   refresh-mapping [postings-5500000000][[posting]] \n18149508 736ms HIGH   refresh-mapping [postings-5500000000][[posting]] \n18149514 733ms HIGH   refresh-mapping [postings-5500000000][[posting]] \n18149516 299ms HIGH   refresh-mapping [postings-4180000000][[posting]] \n18149517 298ms HIGH   refresh-mapping [postings-5360000000][[posting]] \n18149518 291ms HIGH   refresh-mapping [postings-5430000000][[posting]] \n12674966  1.7d NORMAL master ping (from: [Y8LnaPqjTv-4Vn4CWXWWlQ])  \n18149520 290ms HIGH   refresh-mapping [postings-5430000000][[posting]] \n12676681  1.7d NORMAL master ping (from: [Y8LnaPqjTv-4Vn4CWXWWlQ])  \n18149522 289ms HIGH   refresh-mapping [postings-5190000000][[posting]] \n18149523 288ms HIGH   refresh-mapping [postings-5100000000][[posting]] \n18149524 288ms HIGH   refresh-mapping [postings-4180000000][[posting]] \n12678378  1.7d NORMAL master ping (from: [Y8LnaPqjTv-4Vn4CWXWWlQ])  \n18149526 286ms HIGH   refresh-mapping [postings-5430000000][[posting]] \n18149527 286ms HIGH   refresh-mapping [postings-5190000000][[posting]] \n18149528 286ms HIGH   refresh-mapping [postings-4180000000][[posting]] \n18149529 286ms HIGH   refresh-mapping [postings-5500000000][[posting]] \n18149530 284ms HIGH   refresh-mapping [postings-5500000000][[posting]] \n18149531 284ms HIGH   refresh-mapping [postings-5360000000][[posting]] \n18149532 284ms HIGH   refresh-mapping [postings-5100000000][[posting]] \n18149533 284ms HIGH   refresh-mapping [postings-5500000000][[posting]] \n18149534 281ms HIGH   refresh-mapping [postings-5360000000][[posting]]\n","id":"65210200","title":"Endless mapping refresh","reopen_by":"EikeDehling","opened_on":"2015-03-30T13:22:04Z","closed_by":"bleskes"},{"number":"9956","comments":[{"date":"2015-03-03T09:17:46Z","author":"clintongormley","text":"Hi @coxchen \n\nThe `_ttl` is added to the `_timestamp`, so you need to update both otherwise it uses the old `_timestamp` as a base.\n"},{"date":"2015-03-03T14:52:15Z","author":"coxchen","text":"Hi @clintongormley \n\nThanks for your reply.\n\nSorry for the confusion. I didn't update the _timestamp field (or log_time in my example), instead I did partial update to the **log_count** field with:\n\n```\ncurl -XPOST 'localhost:9200\/activity\/log\/1\/_update' -d '{ \"script\" : \"ctx._source.log_count+=1\" }'\n```\n\nI just want to update fields **other than** the _timestamp filed and _ttl field, so I don't see why I hit the AlreadyExpiredException.\n"},{"date":"2015-03-03T19:01:33Z","author":"clintongormley","text":"@coxchen It means that the document has actually expired, but not yet been deleted. Expired docs are only removed every 60 seconds.\n"},{"date":"2015-03-04T02:59:02Z","author":"coxchen","text":"Hi @clintongormley \n\nNope, it's not that case. The document I'd like to update still has valid _ttl (>> 0).\n\nI first found this issue in a production system I'm working on. In this case, I had a document with _ttl originally set to **7d**, and I got the AlreadyExpiredException from updating that document on the **4th day** after it been indexed in ES. At that time, the document **still has its _ttl about 3 days left**.\n\nI did some experiments, which you can read the details at https:\/\/medium.com\/@coxchen\/document-obsolescence-in-elasticsearch-c5973dd9e68d if you have time.\n"},{"date":"2015-03-04T03:34:51Z","author":"coxchen","text":"@clintongormley \n\nBelow is the result of my experiments, showing how _ttl of document changes over time.\n\n![ttl_change_chart](https:\/\/cloud.githubusercontent.com\/assets\/503112\/6477649\/c19516c0-c260-11e4-9195-e42f1dd9564f.png)\n\nI have four documents indexed in ES\n- \/activity\/log1\/1 is _green_ curve\n- \/activity\/log1\/2 is _yellow_ curve\n- \/activity\/log2\/3 is _blue_ curve\n- \/activity\/log2\/4 is _orange_ curve\n\nwith the following index mapping\n\n```\n{\n  \"activity\" : {\n    \"mappings\" : {\n      \"log1\" : {\n        \"_timestamp\" : {\"enabled\" : true, \"path\" : \"log_time\"},\n        \"_ttl\" : {\"enabled\" : true, \"default\" : 300000},\n        \"properties\" : {\n          \"log_count\" : {\"type\" : \"integer\"},\n          \"log_time\" : {\"type\" : \"date\"}\n        }\n      },\n      \"log2\" : {\n        \"_ttl\" : {\"enabled\" : true, \"default\" : 300000},\n        \"properties\" : {\n          \"log_count\" : {\"type\" : \"integer\"},\n          \"log_time\" : {\"type\" : \"date\"}\n        }\n      }\n    }\n  }\n}\n```\n\nI also have script to update the **log_count** field to document **2** and **4** periodically.\n\n```\ncurl -XPOST 'localhost:9200\/activity\/log1\/2\/_update' -d '{ \"script\" : \"ctx._source.log_count+=1\" }'\n\ncurl -XPOST 'localhost:9200\/activity\/log2\/4\/_update' -d '{ \"script\" : \"ctx._source.log_count+=1\" }'\n```\n\nSupposedly, the curves of the 4 document should overlap. But you can see the **yellow** curve (document 2) is outstanding. It doesn't look right to me. Can you help to explain?\n"},{"date":"2015-04-12T10:34:40Z","author":"coxchen","text":"Hi @clintongormley \n\nI trace the source code and found that, in **TTLFieldMapper.java**, the _timestamp in document source will be used to check expiration when updating a document. So the workaround for my issue is to **always provide the original _ttl value** when updating the document, even though I don't have the intention to update _ttl.\n\nYou can check my article for details: https:\/\/medium.com\/@coxchen\/saving-document-half-life-in-es-89be764f21ca\n"},{"date":"2015-04-13T11:38:18Z","author":"clintongormley","text":"Hi @coxchen \n\nThanks for digging!  I've just tried my own (slightly different) test and see the `_ttl` increasing by leaps and bounds:\n\n```\nDELETE activity\n\nPUT activity\n{\n  \"mappings\": {\n    \"log\": {\n      \"_timestamp\": {\n        \"enabled\": true,\n        \"path\": \"log_time\",\n        \"store\": true\n      },\n      \"_ttl\": {\n        \"enabled\": true,\n        \"default\": 300000\n      },\n      \"properties\": {\n        \"log_count\": {\n          \"type\": \"integer\"\n        },\n        \"log_time\": {\n          \"type\": \"date\"\n        }\n      }\n    }\n  }\n}\n```\n\nIndex a document using tomorrow's date:\n\n```\nPUT activity\/log\/1\n{\n  \"log_time\": \"2015-04-14\"\n}\n```\n\nRepeat these two steps to see the `_ttl` just keep on growing\n\n```\nGET _search?fields=_ttl\n\nPOST activity\/log\/1\/_update \n{\n  \"doc\": { \"foo\": \"bar\" }\n}\n```\n"},{"date":"2015-04-13T14:16:39Z","author":"coxchen","text":"Hi @clintongormley \n\nInteresting, the log_time is some time in the future, making the _ttl growing with UPDATE.\n"},{"date":"2015-05-07T16:30:47Z","author":"darylrobbins","text":"I have hit this same issue when upgrading from 1.4.2 to 1.5.2. I'm using a mvel script to update attributes in the document (neither the _timestamp or _ttl). The issue appears to happen for some documents but not others.\n"},{"date":"2016-05-12T12:56:15Z","author":"clintongormley","text":"Closing in favour of #18280\n"}],"reopen_on":"2015-04-13T11:36:32Z","opened_by":"coxchen","closed_on":"2016-05-12T12:56:15Z","description":"Hi,\n\nI hit the AlreadyExpiredException from updating document with valid _ttl, i.e. _ttl greater than 0.\nHere is my index mapping (simplified), both **_timestamp** and **_ttl** are enabled, and _timestamp is provided with _customized path_:\n\n```\n{\n  \"activity\" : {\n    \"mappings\" : {\n      \"log\" : {\n        \"_timestamp\" : {\"enabled\" : true, \"path\" : \"log_time\"},\n        \"_ttl\" : {\"enabled\" : true, \"default\" : 300000},\n        \"properties\" : {\n          \"log_count\" : {\"type\" : \"integer\"},\n          \"log_time\" : {\"type\" : \"date\"}\n        }\n      }\n    }\n  }\n}\n```\n\nI did some experiments as [documented in this article](https:\/\/medium.com\/@coxchen\/document-obsolescence-in-elasticsearch-c5973dd9e68d) and found that, if I have mappings like above (both **_timestamp** and **_ttl** are enabled, and _timestamp is provided with _customized path_), I'll hit the AlreadyExpiredException from updating document with valid _ttl. I have tried both 1.3.1 and 1.4.2, same behavior.\n\nIs this a known issue? Or is there any design intent behind this?\n\nThank you for your help!\n","id":"59589814","title":"Hitting AlreadyExpiredException from updating document that has valid _ttl","reopen_by":"clintongormley","opened_on":"2015-03-03T03:26:26Z","closed_by":"clintongormley"},{"number":"9952","comments":[{"date":"2015-03-04T11:02:38Z","author":"s1monw","text":"I left some comments...\n"},{"date":"2015-03-04T19:26:10Z","author":"brwe","text":"Made a pr for the deletion of index folders here: #9985 Should be easy to remove all the additional deletion code from this pr.\n"},{"date":"2015-03-05T18:09:12Z","author":"brwe","text":"#9985  is merged, I rebased on latest master and changed the code accordingly. I wanted to remove the change in ClusterStateEvent also because I was unable to reproduce the failures I'd seen before without it. But now I found that without the change the tests only pass on my Linux machine but fail every 10 iterations or so on my mac so something is still fishy. I'll try to come up with a detailed failure analysis tomorrow.\n"},{"date":"2015-03-06T10:12:20Z","author":"brwe","text":"I think I know what is going on now: The fresh master with the empty cluster state does (rarely) not send the first cluster state due to a race condition in lifecycles of DiscoveryService and its member Discovery. In DiscoveryService.doStart() the Discovery is started but the lifecyle for DiscoveryService is started only after that. This is why when the first cluster state reaches DiscoveryService.publish the lifecycle might or might not have started.\n\nI added a commit d69f2cf1ac778cc0cfea37b351ae6ff5f1039262 where I removed the ClusterStateEvent change and added an artificial delay to the DiscoveryService.doStart() so that the tests fail reliably just so you can check if you want.\n\nI would suggest we remove the ClusterStateEvent workaround and open another issue for this because this behavior is not a result of this pull request.\n"},{"date":"2015-03-06T12:55:03Z","author":"s1monw","text":"I agree with your idea of opening a new issue for the ClusterSTateEvent problem\n"},{"date":"2015-03-10T23:48:22Z","author":"brwe","text":"Chatted with @s1monw and now rewrote it so that the selection of what to write is not done in GatewayMetaState anymore. I tried to do it similar to #10016. It is still a little raw but but would be great if you could let me know if this is the right direction. \n"},{"date":"2015-03-12T20:13:03Z","author":"s1monw","text":"I left a bunch of comments \n"},{"date":"2015-03-17T20:54:07Z","author":"brwe","text":"Addressed all comments. I am unsure about two things, left comments above about it: We did check on disk if we have the state already before writing in case the in memory state is null. However, this should be a rare event so I am unsure if we need this optimization. In addition, I made it so that whenever a shard is initializing on a node we write the meta state of this index and perform no check if we wrote before already. Should I check instead?\n"},{"date":"2015-03-18T06:03:06Z","author":"s1monw","text":"I like this  a lot - left some minor comments\n"},{"date":"2015-03-18T22:25:15Z","author":"brwe","text":"Addressed all comments except for the version check  thing above. We cannot use index version to ensure that the state is written because the version is not necessarily updated for the index metadata if shards are relocated. But shards should initialize too often on a node so it might be ok if we write the same state each time we see an initializing shard?\n"},{"date":"2015-03-19T18:14:54Z","author":"brwe","text":"Chatted with @s1monw who reminded me that shards can be in initializing state for a while and while they are we would always write, so that is not a good idea. Will now instead check in the event if the last state had shards allocated and if not then write if the new one has.  \n"},{"date":"2015-03-19T19:25:58Z","author":"brwe","text":"pushed a new commit to address this. we now check if a shard was already present in last cluster state and if so not write.\n"},{"date":"2015-03-19T23:25:43Z","author":"s1monw","text":"it LGTM I think @bleskes should take one more look \n"},{"date":"2015-03-30T08:36:13Z","author":"bleskes","text":"Looks good in general. Left some comments that I think will simplify things.\n"},{"date":"2015-04-01T10:29:45Z","author":"brwe","text":"Implemented all suggestions. \n@bleskes thanks for the tip with the indices list caching, it simplifies think indeed! Please take another look.\n"},{"date":"2015-04-28T20:07:24Z","author":"bleskes","text":"Left some very minor comments. Feel free to push without another review. LGTM!\n"},{"date":"2015-04-29T15:24:04Z","author":"brwe","text":"need to investigate https:\/\/github.com\/elastic\/elasticsearch\/issues\/10017 before we can push\n"},{"date":"2015-04-30T08:34:58Z","author":"brwe","text":"The reason why the tests failed on CI is the same I described in the beginning https:\/\/github.com\/elastic\/elasticsearch\/pull\/9952#issue-59520914 : a data node receives a new cluster state from a master that does not have the index in its state but the data node missed the state with a no master block before and so state persistence was not disabled. the fact that an index is not in the cluster state is then interpreted as delete command. This can happen here for the reasons described in #10017 but there might be other reasons as well. I now think we should not delete indices at all if the cluster state that would cause a deletion comes from a new master.\nI added a new commit for this but need someone to confirm that this is actually the right solution.\n"},{"date":"2015-05-04T12:37:02Z","author":"s1monw","text":"> I added a new commit for this but need someone to confirm that this is actually the right solution.\n\n+1 to the solution\n"},{"date":"2015-05-04T15:36:25Z","author":"brwe","text":"Chatted with @kimchy and we decided to push as is and add a \/\/norelease comment and open an issue because the short term fix for the  problem (https:\/\/github.com\/elastic\/elasticsearch\/pull\/9952#issuecomment-97707128) is not very elegant.\nAdded another commit to address the latest comments.\n"},{"date":"2015-05-05T09:40:06Z","author":"s1monw","text":"LGTM\n"}],"reopen_on":"2015-04-29T15:24:04Z","opened_by":"brwe","closed_on":"2015-05-05T10:25:46Z","description":"When a node was a data node only then the index state was not written.\nIn case this node connected to a master that did not have the index\nin the cluster state, for example because a master was restarted and\nthe data folder was lost, then the indices were not imported as dangling\nbut instead deleted.\nThis commit makes sure that index state for data nodes is also written\nif they have at least one shard of this index allocated.\n\nI am a little lost with this. I found that the index can still be deleted\nfrom a data node if the state was written but the node gets a new cluster state from a \nmaster that does not have it, for example because it was restarted without data folder. Happens\nif the data node does not get the initial cluster state from the new but a later one and state\npersistence is not disabled. \nI avoid this now by this: https:\/\/github.com\/elasticsearch\/elasticsearch\/pull\/9952\/files#diff-f0f71bedb3d7e6f1cec54e8dddf5c3d3R109\nbut am worried about side effects this might have.  Any feedback appreciated.\n\ncloses #8823\n","id":"59520914","title":"Write state also on data nodes if not master eligible","reopen_by":"brwe","opened_on":"2015-03-02T18:18:55Z","closed_by":"brwe"},{"number":"9874","comments":[{"date":"2015-03-05T15:34:39Z","author":"jpountz","text":"LGTM\n"},{"date":"2015-03-05T15:36:27Z","author":"jpountz","text":"@brwe Since this change looks quite significant, I don't think it should be pushed to 1.4.5.\n"},{"date":"2015-03-06T18:27:53Z","author":"brwe","text":"That was not a good idea - the change of MapperParsingException seemes to have screwed up bwc for Exception serialization, see for example http:\/\/build-us-00.elasticsearch.org\/job\/es_bwc_1x\/8385\/\nI reverted the commit now.\n"},{"date":"2015-03-17T22:02:22Z","author":"brwe","text":"Ok, new plan. Instead of changing the mapper parsing exception we actually can just get the context from the document mapper and ask if the mapping was changed - much easier and I cannot see any downside to it. @jpountz would you mind taking another look?\n"},{"date":"2015-03-24T15:20:04Z","author":"jpountz","text":"I'm not too happy with getting back to the cache to check whether mappings have been modified, I liked the previous solution better. But on ther other hand, I don't have a better idea. Can we maybe just use this approach on 1.x and keep the previous approach on master?\n"},{"date":"2015-03-26T14:26:23Z","author":"brwe","text":"Ok, I'll do that.\n"}],"reopen_on":"2015-03-06T18:27:53Z","opened_by":"brwe","closed_on":"2015-03-26T20:06:43Z","description":"...ils for the rest of doc\n\nThe local DocumentMapper is updated while parsing and dynamic fields are added before\nparsing has finished. If parsing fails after a dynamic field has been added already\nthen the field was not added to the cluster state but was present in the local mapper of this\nnode. New documents with the same field would not necessarily cause an update either and\nafter restarting the node the mapping for these fields were lost. Instead the new fields\nshould always be updated.\n\ncloses #9851\n","id":"58886953","title":"Update dynamic fields in mapping on master even if parsing fails for the rest of the doc","reopen_by":"brwe","opened_on":"2015-02-25T11:23:33Z","closed_by":"brwe"},{"number":"9511","comments":[{"date":"2015-02-06T10:16:14Z","author":"colings86","text":"@nknize I think this problem relates to the code to deal with polygons around the international date line. We rely on the fact that a hole is actually a hole (i.e. is within the polygon) to be able to place it properly. By definition a hole which intersects the polygons edges is not actually a hole but a modification to the external shape of the main polygon. Maybe we need to add some logic to rewrite the polygon to cut out the hole from the main polygon shape if it intersects with the edge?\n"},{"date":"2015-03-20T19:40:01Z","author":"tschaub","text":"We've been hitting the same issues with polygons that are not close to the dateline and don't have interior rings that cross an exterior ring.\n\n```\n#!\/bin\/sh\n\ncurl -XDELETE http:\/\/localhost:9200\/test_index\n\ncurl -XPUT http:\/\/localhost:9200\/test_index\/\n\ncurl -XPUT http:\/\/localhost:9200\/test_index\/test_type\/_mapping -d '\n {\n     \"properties\": {\n         \"geom\": {\n             \"type\": \"geo_shape\",\n             \"tree\": \"quadtree\",\n             \"tree_levels\": \"26\"\n         }\n     }\n }'\n\ncurl -XGET localhost:9200\/test_index\/test_type\/_search?pretty -d \\\n    '{\"query\": {\"filtered\": {\"filter\": {\"bool\": {\"must\": [{\"geo_shape\": {\"geom\": {\"shape\": {\"type\": \"polygon\", \"coordinates\": [[[-150.0, -30.0], [-150.0, 30.0], [150.0, 30.0], [150.0, -30.0], [-150.0, -30.0]], [[0.01, 0.01], [0.02, 0.02], [0.01, 0.02], [0.01, 0.01]]]}}}}, \"from\": 0, \"size\": 50}'\n```\n\nHere's the polygon geometry from that query: http:\/\/bl.ocks.org\/anonymous\/raw\/0fb706605fe9e53c52f8\/\n\nIf the exterior ring is changed so that it spans from -90 (west) to 90 (east) longitude, the query succeeds.  When the exterior ring spans more than 180 degrees, the query fails - even if the exterior ring is not near the dateline. \n"},{"date":"2015-03-20T19:54:45Z","author":"tschaub","text":"Are you by chance assuming that rings spanning more than 180 degrees need to be \"inverted\"?  In my opinion, you're best off pretending the world is flat and accepting that 0, 0 is inside a polygon spanning -91 (west) to 91 (east) - regardless of the winding order of exterior rings.\n"},{"date":"2015-03-20T20:02:15Z","author":"tschaub","text":"I should debug to confirm, but it feels like you may be testing whether an interior ring is inside an exterior ring with different logic than the intersect query.  Perhaps the query above fails because you decide that the interior ring (a small ring near 0, 0) is not inside the exterior ring.  Maybe that is because you assume that because the exterior ring spans more than 180 degrees longitude you need to invert it.\n\nBut a similar query without the interior ring returns results that are inside the exterior ring in the same sense that the interior ring is inside (i.e. without assuming that the > 180 spanning exterior ring implies a ring that crosses the dateline).\n"},{"date":"2015-03-21T01:55:22Z","author":"nknize","text":"Hey @bictorman and Tim, thanks for the update.  I'll bump this up on the priority list to review before the next release. After looking at the provided examples it appears to be working as expected. We comply with OGC SFA spec (https:\/\/portal.opengeospatial.org\/files\/?artifact_id=829 see section 2.1.10 - specifically figure 2.5). \"Figure 2.5 shows some examples of geometric objects that violate the above assertions and are not representable as single instances of Polygon.\"  That is, holes should not intersect edges (these need to be converted to multi-polygon or multiple polygon objects).\n\nJust to make sure there isn't some other latent bug, which version are you working with?  Prior to 1.4.3 the ring orientation logic was incorrectly computed for polys > 180 (leading to behavior Tim described).  That was corrected in 1.4.3+ to follow the OGC SFF right-hand rule along with an added `orientation` option to explicitly define orientation behavior.  Per the right-hand rule, the polygon provided crosses the dateline, so the hole is outside the poly. A simple fix for this case should be to add \n\n```\n\"orientation\": \"left\"\n```\n\nAnd the ring vertices will be clockwise ordered. \n"},{"date":"2015-03-23T17:04:26Z","author":"tschaub","text":"Thanks for the additional detail @nknize.  I'll confirm the version we are using and try the orientation property.  One concern is that GeoJSON doesn't specify the winding order and OGR (and more) don't enforce a winding order when serializing.  Would it be possible to provide an option to disable the winding order check for polygons > 180?  I like the behavior we're getting for polygons that don't span 180 and would find it convenient to force the same for all polygons (dateline be damned).\n"},{"date":"2015-03-23T17:12:17Z","author":"tschaub","text":"We're running 1.4.4.  I'll try to come up with some simple test cases to ensure we can get consistent behavior (for polygons with and without holes, regardless of span).\n"},{"date":"2015-03-23T21:03:39Z","author":"nknize","text":"@tschaub We can certainly add an ignore option to the orientation and provide the \"old\" behavior if that's desired (the default would remain \"right\"). We ultimately chose OGC compliance (despite the GeoJSON indifference) as a solution to the ongoing issues surrounding dateline crossing polys.  If you don't have any dateline crossing polys (or already have application logic to handle it) then an `ignore` option certainly makes sense.  Would this option help you out?\n\nIn the meantime, 1.4.4 supports the `orientation` option which can be applied to the mapping and\/or on a per document basis.  This way you can have a shape field default to the left-hand rule but have specific documents processed using the right hand rule.  In the above case you'd pass this particular document with the `orientation: left` parameter and all is well.\n"},{"date":"2015-03-24T14:08:50Z","author":"nknize","text":"@bictorman There are 2 discussions here (latest surrounding winding order). I want to make sure we answered your original question\/issue.  \n\nWe comply with OGC SFA spec (https:\/\/portal.opengeospatial.org\/files\/?artifact_id=829 see section 2.1.10 - specifically figure 2.5). \"Figure 2.5 shows some examples of geometric objects that violate the above assertions and are not representable as single instances of Polygon.\" That is, holes should not intersect edges.  So you'll need to convert those geometric objects to multi-polygons or multiple polygon objects.\n\nLet us know if there are any other questions.  If not I'll close this as a non-issue.  The second discussion (winding order) has been moved to #10227 \n"},{"date":"2015-03-24T15:45:32Z","author":"bictorman","text":"@nknize Understood. Seems like I was misled by the behaviour of PostGIS and previous ES versions. We'll try to fix the data, thanks for the answer.\n"},{"date":"2015-03-24T19:13:58Z","author":"nknize","text":"Closing as non-issue\n"},{"date":"2015-03-25T18:54:32Z","author":"dbaston","text":"@nknize A lot of other software out there (PostGIS, GEOS, JTS, etc.) interprets the OGC spec to allow an interior ring to touch the outside of a boundary, provided that it only touches at one point.  (If only OGC had included a drawing of this simple case in the spec!)  Does ES have a different interpretation of the spec than PostGIS etc, and if so, how is one to represent a polygon like the one @bictorman posted in ES?  I'm familiar with the ESRI\/ArcGIS way of representing this -- as an exterior ring that loops back on itself -- but I've always understood that to be different from the OGC way.\n\nSome more examples of OGC-valid polygons, including the case @bictorman posted, are found in the PostGIS docs:\nhttp:\/\/postgis.net\/docs\/using_postgis_dbmanagement.html#OGC_Validity\n"},{"date":"2015-03-25T20:50:28Z","author":"nknize","text":"@dbaston  While I cannot comment on OGC compliance for other software packages your question did expose an issue with the ShapeBuilder incorrectly counting the closed coordinate of the interior ring as a violation of 2.1.10 assertion 3 (a conflict between the geojson parser and the OGC builder counting that point as 3 intersections).  I'm going to reopen this issue to correct the closed coordinate issue.   \n\nHopefully @bictorman was able to adjust his polygon accordingly.  If not a working coordinate order can be found at the following GIST:  https:\/\/gist.github.com\/nknize\/7a5d2bf9a9e654e1cbb5\n\nAs to the question of ES interpretation of the spec, its implemented to follow the spec as closely as possible per collaboration with the OGC and OSGeo community.  Variations are often discovered by way of the mailing lists and issue discussions such as these (which keep the implementation in check).  Thanks again for revisiting this.  The feedback is greatly appreciated.\n"},{"date":"2015-11-06T11:31:14Z","author":"muka","text":"+1\n"}],"reopen_on":"2015-03-25T20:50:28Z","opened_by":"bictorman","closed_on":"2015-04-10T17:10:51Z","description":"Hi,\n\nI've had some trouble importing some geo_shapes in 1.4+, I get the exception:\n`ElasticsearchParseException: Invaild shape: Hole is not within polygon`\n\nI got an example. Now, I'm not an expert in GIS, but:\n- The area looks like this: http:\/\/i.imgur.com\/oDzSeDQ.png with one vertex of the hole in the same point as one vertex of the polygon.\n- The data is from postgis' ST_AsGeoJSON and according to ST_isValid it's a valid area.\n- **Every version of ES from 0.90 to 1.3.x takes the area just fine**. The problems happens after 1.4.0.\n\nHere's the gist with all the data to reproduce it:\nhttps:\/\/gist.github.com\/bictorman\/064a777499719ad66194\n","id":"56056128","title":"Hole is not within polygon","reopen_by":"nknize","opened_on":"2015-01-30T16:45:24Z","closed_by":"nknize"},{"number":"9410","comments":[{"date":"2015-01-26T19:56:54Z","author":"clintongormley","text":"First Elasticsearch checks the unknown string field against the list of date formats in `dynamic_date_formats` to determine whether the field contains a date or a string.\n\nOnce it decides that it is a date, it applies the mapping found in the templates, in which you don't specify the date format.  Thus it uses the default matching format which is `dateOptionalTime`.  Why not just specify the format you want in the template mapping?  eg:\n\n```\nDELETE test\n\nPOST \/test\n{\n  \"mappings\": {\n    \"test\": {\n      \"dynamic_date_formats\": [\n        \"yyyy\/MM\/dd\"  \n      ],\n      \"dynamic_templates\": [\n        {\n          \"dates_ignore_malformed\": {\n            \"path_match\": \"*\",\n            \"match_mapping_type\": \"date\",\n            \"mapping\": {\n              \"format\": \"yyyy\/MM\/dd\",\n              \"ignore_malformed\": true\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n\nPOST \/test\/test\/1\n{\n  \"format_one\": \"2014-01-05\"\n}\n\nPOST \/test\/test\/2\n{\n  \"format_two\": \"2014\/01\/05\"\n}\n\nGET _mapping\n```\n\nThis returns:\n\n```\n{\n   \"test\": {\n      \"mappings\": {\n         \"test\": {\n            \"dynamic_date_formats\": [\n               \"yyyy\/MM\/dd\"\n            ],\n            \"dynamic_templates\": [\n               {\n                  \"dates_ignore_malformed\": {\n                     \"mapping\": {\n                        \"ignore_malformed\": true,\n                        \"format\": \"yyyy\/MM\/dd\"\n                     },\n                     \"match_mapping_type\": \"date\",\n                     \"path_match\": \"*\"\n                  }\n               }\n            ],\n            \"properties\": {\n               \"format_one\": {\n                  \"type\": \"string\"\n               },\n               \"format_two\": {\n                  \"type\": \"date\",\n                  \"ignore_malformed\": true,\n                  \"format\": \"yyyy\/MM\/dd\"\n               }\n            }\n         }\n      }\n   }\n} \n```\n"},{"date":"2015-01-27T00:25:45Z","author":"SergeyTsalkov","text":"So what if you wanted to use dynamic_date_formats to specify multiple possible formats? Does that prevent you from also using ignore_malformed?\n"},{"date":"2015-01-27T09:19:41Z","author":"clintongormley","text":"The first time that field is seen, it will be mapped as a date only if it matches one of the formats you listed in `dynamic_date_formats`, otherwise it will be mapped as a string.\n\nOnce the field is mapped, then `ignore_malformed` will allow you to ignore malformed dates later on.  But if the first date seen is malformed, then the field will be a string instead.  \n\nThis is the same way that the mapping would work if you do not specify dynamic_date_formats or templates.\n"},{"date":"2015-01-27T09:51:44Z","author":"SergeyTsalkov","text":"I appreciate you taking the time to help me clear this up, but it still seems like the thing I want to do isn't possible. In my data set, the date might be in one of several formats, and I won't know which one ahead of time. That's why I'm trying to use dynamic_date_formats to specify the possibilities, but I guess my first example was unclear because I only listed one. This is more what I was going for:\n\n```\n{\n  \"mappings\": {\n    \"test\": {\n      \"dynamic_date_formats\": [\n        \"yyyy-MM-dd\",\n        \"yyyy\/MM\/dd\"\n      ],\n      \"dynamic_templates\": [\n        {\n          \"dates_ignore_malformed\": {\n            \"path_match\": \"*\",\n            \"match_mapping_type\": \"date\",\n            \"mapping\": {\n              \"ignore_malformed\": true\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n```\n\nSo whichever format the first date has would be applied to the new field, and the date would be expected in that format from then on. If it came in a different format from then on, ignore_malformed would cause it to be ignored.\n\nI guess that isn't possible with ElasticSearch?\n"},{"date":"2015-01-27T10:16:31Z","author":"clintongormley","text":"@SergeyTsalkov Actually you're completely right - the `dynamic_date_formats` are not used after adding the mapping to determine the `format` for the date field:\n\n```\nDELETE test\n\nPOST \/test\n{\n  \"mappings\": {\n    \"test\": {\n      \"dynamic_date_formats\": [\n        \"yyyy-MM-dd\"  ,\n        \"dd\/MM\/yyyy\"\n      ],\n      \"dynamic_templates\": [\n        {\n          \"dates_ignore_malformed\": {\n            \"path_match\": \"*\",\n            \"match_mapping_type\": \"date\",\n            \"mapping\": {\n              \"type\":\"date\"\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n```\n\nThis results in a date field with format `dateOptionalTime`:\n\n```\nPOST \/test\/test\/1\n{\n  \"format_one\": \"2014-01-05\"\n}\n```\n\nThis throws an exception indicating that it is ignoring the `dynamic_date_formats`:  failed to parse date field [05\/01\/2014], tried both date format [dateOptionalTime], and timestamp number with locale []\n\n```\nPOST \/test\/test\/2\n{\n  \"format_two\": \"05\/01\/2014\"\n}\n\nGET _mapping\n```\n"},{"date":"2015-01-27T15:38:43Z","author":"rjernst","text":"> the dynamic_date_formats are not used after adding the mapping to determine the format for the date field\n\nBut that's the way it should be? As you said before, `dynamic_date_formats` is to determine _what_ is a date.  There was then no format put on the dynamic field, and when it tried to parse the value (with the actual field mapping format of `dateOptionalTime`) it failed. I don't think `dynamic_date_formats` should in any ways adjust the dynamic field's format (that is what the dynamic field mapping is for).\n"},{"date":"2015-01-27T18:03:40Z","author":"clintongormley","text":"@rjernst if you didn't specify any dynamic_date_formats or any templates, then it would check a string against the default list of dynamic date formats and apply the first format that matches to the field.\n\nWith the example above, it uses the specified dynamic date formats to determine whether the field is a date or not, but then it uses the default list of dynamic date formats to determine which format to apply.  Instead, it should use the custom list.\n"},{"date":"2016-10-13T01:01:51Z","author":"sqlboy","text":"This is the way to do it. Set your dynamic_date_format, then set the same ones as the value of \"format\" in your dynamic mapping.\n\n```\n  \"dynamic_date_formats\": [\n        \"yyyy\/MM\/dd HH:mm:ss Z\",\n        \"yyyy\/MM\/dd Z\",\n        \"yyyy\/MM\/dd HH:mm:ss\",\n        \"yyyy\/MM\/dd\",\n        \"yyyy-MM-dd HH:mm:ss Z\",\n        \"yyyy-MM-dd Z\",\n        \"yyyy-MM-dd HH:mm:ss\",\n        \"yyyy-MM-dd\",\n        \"MM\/dd\/yyyy\",\n        \"MM-dd-yyyy\"\n      ],\n```\n\n```\n          \"date_template\": {\n            \"match\": \"*\",\n            \"match_mapping_type\": \"date\",\n            \"mapping\": {\n              \"type\": \"date\",\n              \"index\": \"not_analyzed\",\n              \"format\": \"yyyy\/MM\/dd HH:mm:ss Z||yyyy\/MM\/dd Z||yyyy\/MM\/dd HH:mm:ss||yyyy\/MM\/dd||yyyy-MM-dd HH:mm:ss Z||yyyy-MM-dd Z||yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||MM\/dd\/yyyy||MM-dd-yyyy\"\n            }\n          }\n```\n\nThis way, you can add dates of different format and the field doesn't get locked down to a single format.\n"},{"date":"2017-01-05T18:01:48Z","author":"Fairledger","text":"I'm using Elasticsearch 2.3.1 and the dynamic mapping posted above fails for me:\r\n          \"date_template\": {\r\n            \"match\": \"*\",\r\n            \"match_mapping_type\": \"date\",\r\n            \"mapping\": {\r\n              \"type\": \"date\",\r\n              \"index\": \"not_analyzed\",\r\n              \"format\": \"yyyy\/MM\/dd HH:mm:ss Z||yyyy\/MM\/dd Z||yyyy\/MM\/dd HH:mm:ss||yyyy\/MM\/dd||yyyy-MM-dd HH:mm:ss Z||yyyy-MM-dd Z||yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||MM\/dd\/yyyy||MM-dd-yyyy\"\r\n            }\r\n          }\r\n\r\nI've also tried the following but it too does not give me the behavior I want.  I do not always know the format of my date field for an index before indexing a document.  I want the index to be able to accept different formats.\r\n\r\n  \"dynamic_date_formats\": [\r\n        \"yyyy\/MM\/dd HH:mm:ss Z\",\r\n        \"yyyy\/MM\/dd Z\",\r\n        \"yyyy\/MM\/dd HH:mm:ss\",\r\n        \"yyyy\/MM\/dd\",\r\n        \"yyyy-MM-dd HH:mm:ss Z\",\r\n        \"yyyy-MM-dd Z\",\r\n        \"yyyy-MM-dd HH:mm:ss\",\r\n        \"yyyy-MM-dd\",\r\n        \"MM\/dd\/yyyy\",\r\n        \"MM-dd-yyyy\"\r\n      ],\r\n      \"dynamic_templates\": [\r\n      {\r\n        \"dates\": {\r\n              \"match_mapping_type\": [\"*time\", \"date\", \"TIME\"],\r\n              \"match_pattern\": \"regex\",\r\n              \"mapping\": {\r\n                  \"type\": \"date\",\r\n                  \"format\": \"YYYY-MM-dd HH:mm:ss | YYYY-MM-dd | YYYY-MM-dd HH:mm:ss.SS | YYYY:MM-dd HH:mm:ss.SSSS\"\r\n               }\r\n          }\r\n       }\r\n     ]\r\n"}],"reopen_on":"2015-01-27T10:14:22Z","opened_by":"SergeyTsalkov","closed_on":"2016-12-30T08:48:25Z","description":"My goal was to have a dynamically created date field with the ignore_malformed option set. However, when I have both dynamic_date_formats and dynamic_templates for the date type, the dynamic_date_formats is ignored.\n\nSo let's create our index..\n\n```\ncurl -XPOST localhost:9200\/test -d '{\n  \"mappings\": {\n    \"test\": {\n      \"dynamic_date_formats\": [\n        \"yyyy-MM-dd\"\n      ],\n      \"dynamic_templates\": [\n        {\n          \"dates_ignore_malformed\": {\n            \"path_match\": \"*\",\n            \"match_mapping_type\": \"date\",\n            \"mapping\": {\n              \"ignore_malformed\": true\n            }\n          }\n        }\n      ]\n    }\n  }\n}'\n```\n\nAnd add some data..\n\n```\ncurl -XPOST localhost:9200\/test\/test\/1 -d '{\n  \"something\": \"2014-01-05\"\n}'\n```\n\nAnd now get the mappings back..\n\n```\ncurl -XGET localhost:9200\/test\/_mappings\n->\n{\n  \"test\": {\n    \"mappings\": {\n      \"test\": {\n        \"dynamic_date_formats\": [\n          \"yyyy-MM-dd\"\n        ],\n        \"dynamic_templates\": [\n          {\n            \"dates_ignore_malformed\": {\n              \"mapping\": {\n                \"ignore_malformed\": true\n              },\n              \"match_mapping_type\": \"date\",\n              \"path_match\": \"*\"\n            }\n          }\n        ],\n        \"properties\": {\n          \"something\": {\n            \"type\": \"date\",\n            \"ignore_malformed\": true,\n            \"format\": \"dateOptionalTime\"\n          }\n        }\n      }\n    }\n  }\n}\n```\n\nSee how the format for \"something\" is dateOptionalTime? Had I not included the dynamic_templates, that would have been (and should have been) \"yyyy-MM-dd\".\n","id":"55461241","title":"dynamic_date_formats ignored if dynamic_templates for date is present","reopen_by":"clintongormley","opened_on":"2015-01-26T09:25:19Z","closed_by":"jpountz"},{"number":"9225","comments":[{"date":"2015-02-17T16:42:44Z","author":"nezda","text":"I reported this Dec. 19 https:\/\/groups.google.com\/forum\/?utm_medium=email&utm_source=footer#!msg\/elasticsearch\/N39iejK6c5o\/qj1AUqPxweUJ with a gist but got no feedback whatsoever.\n"},{"date":"2015-02-18T16:54:37Z","author":"jpeddle","text":"@ppf2 - were you able to confirm that the Gist you posted here works with ES 1.4.3?  Using 1.4.3, I've been trying to get the query cache populated with a date range filter, but have been unsuccessful so far - not sure if it's something I'm doing wrong, if the bug is still there.\n"},{"date":"2015-02-26T04:52:30Z","author":"ppf2","text":"@jpountz Just tested 1.4.4 with the gist in the original filing, it looks like it is still not populating the query cache when there is a range filter.\n"},{"date":"2015-02-26T06:53:11Z","author":"javanna","text":"I am reopening this then... thanks for testing\n"},{"date":"2015-02-26T08:31:36Z","author":"jpountz","text":"OK, I just found the issue. There were merge conflicts when I backported to 1.4 and I left a call to SearchContext.nowInMillis. Although it did not break functionality, it disabled the query cache since we do not cache requests when the current timestamp is used. I just pushed a fix and will update the labels. For the record, things were and are still fine on master and 1.x.\n"},{"date":"2015-03-16T17:43:12Z","author":"jpountz","text":"It did not work on 1.4 because of a merging issue. This is fixed now and will be available in 1.4.5. Sorry for the annoyance.\n"}],"reopen_on":"2015-02-26T06:53:12Z","opened_by":"ppf2","closed_on":"2015-03-16T17:43:12Z","description":"Use case below.\n\nhttps:\/\/gist.github.com\/ppf2\/bfa38b8284e09c0740ad\n\nQuery cache is turned on and the query uses search_type=count.  If the query is run with just the aggregation part, the query cache is populated.  As soon as a range filter is added to the query, it stops populating the query cache.  Not sure why that is ...\n","id":"53924887","title":"Shard query cache not populated when there is a range filter","reopen_by":"javanna","opened_on":"2015-01-09T22:47:00Z","closed_by":"jpountz"},{"number":"8943","comments":[{"date":"2014-12-22T23:15:51Z","author":"rjernst","text":"I don't think we should do this. `_analyzer` should be removed altogether with 2.0.  Setting a different analyzer per document doesn't really work except in extreme cases, and that edge case can be worked around by having multiple fields and setting a query over those multiple fields.\n"},{"date":"2014-12-23T09:57:53Z","author":"javanna","text":"I agree with @rjernst. \n\nAlso, I wonder if this PR relates to #5497 and #6267 somehow. What confuses me is that the issue is around highlighting but the fix is not isolated to highlighting, but in generic mappings code.\n"},{"date":"2014-12-24T05:23:38Z","author":"masaruh","text":"Fair enough. If `_analyzer` is going away, we wouldn't need to worry about it.\n(I agree that using multiple fields is usually better than analyzer per document)\n"},{"date":"2014-12-24T11:12:28Z","author":"dakrone","text":"Re-opening, `_analyzer` is still used by many people that need to support more than a few languages.\n\nAn example is a company that runs language analysis and would like to use all of the 33 [language-specific analyzers](http:\/\/www.elasticsearch.org\/guide\/en\/elasticsearch\/reference\/current\/analysis-lang-analyzer.html#analysis-lang-analyzer) that ES provides. Duplicating the entire type 33 times is not a good solution, neither is re-analyzing each field 32 additional times for a multi-field with each language. Using `_analyzer` with `path` is helpful in this narrow case (when the language being searched is known in advance also).\n\nWhile `_analyzer` is not the best choice for this (ngrams would be much better) it's still used and should not be removed.\n"},{"date":"2014-12-29T22:45:47Z","author":"rjernst","text":"I do not think because a feature is used by very few that it should be precluded from the possibility of removal.  In this case, I think this feature is trappy.  With your example, this would mean these 33 languages are indexed into the same field, but with different analyzers (ie the purpose of `_analyzer`). But how is the field then queried? It would have to either choose a specific language's analyzer (in which case it is no different than having a separate field for each language) or have some frankenstein analyzer that is \"good\" for most languages, in which case there could be terms which might never be found. \n\nHaving 33 types, one for each language, may seem like a lot, but this is an extreme edge case, and I don't see any problem with edge cases needing to do more work in configuration. However, I also think this can be accomplished by having different fields for each language, and selecting all these fields to query over like in the simple query parser's fields list. \n"},{"date":"2015-05-01T09:53:39Z","author":"javanna","text":"We removed the per document _analyzer on master, so this commit won't go there anymore. In 1.x, we still support it though, and we did add support for it to highlighting with #6267. Seems that the highlighting support was incomplete though given this other PR. As far as I understand we only support it when specified through a path in the mapping? @masaruh can you confirm?\n\nIf we do support this in 1.x but it's buggy I think we should fix it by getting this in 1.x. @clintongormley thoughts?\n"},{"date":"2015-05-04T10:52:40Z","author":"clintongormley","text":"@javanna it seems like a small enough fix\n"},{"date":"2015-05-04T12:02:21Z","author":"masaruh","text":"@javanna I should have put more descriptive comment...\nThe issue is that highlight doesn't work when mapping has a field with an analyzer specified and `_analyzer`. In this case, we should use analyzer specified on the field for highlighting. But currently, we use analyzer specified in a field of `_analyzer` path.\nSo, yes, highlighting support was incomplete (it works when analyzer isn't set on a field).\n\nWith this fix, it first look for analyzer set on the field. If not found, try `_analyzer`'s path. If not found or `_analyzer`'s path is null, try analyzer set on type level.\n"},{"date":"2015-05-04T12:10:35Z","author":"javanna","text":"Thanks for the explanation @masaruh . I reviewed this and left one comment around testing.\n"},{"date":"2015-05-04T13:47:55Z","author":"masaruh","text":"@javanna thanks for the review. Made a separate test with comment. Hopefully, it's less confusing.\n"},{"date":"2015-05-05T08:14:16Z","author":"javanna","text":"LGTM thanks @masaruh \n"},{"date":"2015-05-07T07:24:08Z","author":"masaruh","text":"Thanks @javanna. Pushed to 1.5 and 1.x branch.\n"}],"reopen_on":"2014-12-24T11:12:28Z","opened_by":"masaruh","closed_on":"2015-05-07T07:24:08Z","description":"Make highlighter use analyzer set on the field.\n\nCloses #8757\n","id":"51910688","title":"Use analyzer set on the field","reopen_by":"dakrone","opened_on":"2014-12-14T06:06:43Z","closed_by":"masaruh"},{"number":"8940","comments":[{"date":"2014-12-15T09:34:30Z","author":"s1monw","text":"@roytmana can you give it a try if your problem is solved? thanks for reporting\n"},{"date":"2014-12-15T15:31:36Z","author":"roytmana","text":"@s1monw \n\npulled from 1.x branch and made a build. The issue is still present for me.\n\nThis is from property file in the JAR file:\nversion=1.5.0-SNAPSHOT\nhash=c98e07a9ac19b7eca412ac6f00e178a348064923\ntimestamp=1418655613243\n"},{"date":"2014-12-15T15:34:43Z","author":"s1monw","text":"@roytmana can you reproduce this in a standalone testcase? I wonder if I can also see your shutdown code? are you using any plugins etc?\n"},{"date":"2014-12-15T16:12:29Z","author":"roytmana","text":"@s1monw \n\nI am afraid this week I won't be able to work on a standalone recreation (I will just have to try to write it from scratch as it is intermingled within the app (startup\/shutdown activities as well as app logic for exporting queries to excel via scrolls) but if you'd like me to try anything in particular, I will try to do it.\n\nthe reason that I think is not so much the shutdown sequence in itself is that removing .setScroll(SCROLL_KEEP_ALIVE) call resolves the issue in my test immediately\n\nI use transport-wares NodeServlet 2.4.1 with Tomcat 7\n\nTomcat calls its destroy() on shutdown prior to any of my other shutdown activities which are triggered by JAX-RS Jersey shutdown later than NodeServlet is done\n\n```\n  NodeServlet:\n\n    public void destroy() {\n        if (node != null) {\n            getServletContext().removeAttribute(NODE_KEY);\n            node.close();\n        }\n    }\n```\n\nPrior to call to destroy in my subclass of the NodeServlet i close the Client obtained from the node via node.client()\n\n```\n  @Override public void init() throws ServletException {\n    ...\n    super.init();\n    client = node.client();\n    ... \/\/here I grab node client so I can publish it in my application as a @Singleton. Same single instance of the client is used by all threads to access ES\n\n  }\n```\n\n```\nINFO  15\/12\/14 10:30:54 reports.ElasticSearchNodeServlet - Destroying ...\nINFO  15\/12\/14 10:30:54 reports.ElasticSearchNodeServlet - ElasticSearchNodeServlet Close ES Client\nINFO  15\/12\/14 10:30:54 elasticsearch.node - [es_alexr_gctrack_n1] stopping ...\nWARN  15\/12\/14 10:31:24 elasticsearch.indices - [es_alexr_gctrack_n1] Not all shards are closed yet, waited 30sec - stopping service\nINFO  15\/12\/14 10:31:24 elasticsearch.node - [es_alexr_gctrack_n1] stopped\nINFO  15\/12\/14 10:31:24 elasticsearch.node - [es_alexr_gctrack_n1] closing ...\nINFO  15\/12\/14 10:31:24 elasticsearch.node - [es_alexr_gctrack_n1] closed\nINFO  15\/12\/14 10:31:24 reports.ElasticSearchNodeServlet - Destroyed\n```\n"},{"date":"2014-12-15T20:41:06Z","author":"s1monw","text":"I can reproduce it now locally - I will open a new PR to fix it\n"},{"date":"2014-12-15T23:36:35Z","author":"roytmana","text":"Thanks @s1monw it took care of the issue.\n\nI have not tested 1.4.x but wonder if it is present there as well. What's approximate release timeline for 1.5? If it is within a month maximum two I will wait for ES 1.5 for our next production release \n\nWe use 1.3.x in production, developing against 1.4.x and doing some future stuff that needs aggs filtering against 1.5\n"},{"date":"2014-12-16T09:03:57Z","author":"s1monw","text":"@roytmana no this is not an issue in `1.4.x` since we added the shard locking only in 1.5 and 2.0 ie. 1.x & master branches. I can't tell about the release timeline as usual. I think a 1.4 upgrade is the safest option at this point.\n"},{"date":"2014-12-16T14:50:37Z","author":"roytmana","text":"Thanks @s1monw !\n"},{"date":"2015-06-22T13:04:39Z","author":"cleemansen","text":"I have exactly the same problem during closing & opening an index.\n\nES-Version: 1.6.0\n\nSay my scroll keep alive time is set to 1 minute.\n1. I'm executing a search with that scroll settings\n2. Immediately after receiving my search result I'm closing and reopening the index\n3. For exactly one minute I get the following error message and my cluster is red\n4. after 1 minute my cluster is green.\n\n```\n[2015-06-22 14:48:13,846][INFO ][cluster.metadata         ] [Grim Hunter] closing indices [[de_v4]]\n[2015-06-22 14:48:22,263][INFO ][cluster.metadata         ] [Grim Hunter] opening indices [[de_v4]]\n[2015-06-22 14:48:27,308][WARN ][indices.cluster          ] [Grim Hunter] [[de_v4][2]] marking and sending shard failed due to [failed to create shard]\norg.elasticsearch.index.shard.IndexShardCreationException: [de_v4][2] failed to create shard\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [de_v4][2], timed out after 5000ms\n    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)\n    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)\n    ... 9 more\n[2015-06-22 14:48:27,309][WARN ][cluster.action.shard     ] [Grim Hunter] [de_v4][2] received shard failed for [de_v4][2], node[iTOuyuGSRLab6ZZxwnhb3g], [P], s[INITIALIZING], indexUUID [jsOkLa-_Q8GSPkKAXXfsoQ], reason [shard failure [failed to create shard][IndexShardCreationException[[de_v4][2] failed to create shard]; nested: LockObtainFailedException[Can't lock shard [de_v4][2], timed out after 5000ms]; ]]\n[2015-06-22 14:48:32,309][WARN ][indices.cluster          ] [Grim Hunter] [[de_v4][0]] marking and sending shard failed due to [failed to create shard]\norg.elasticsearch.index.shard.IndexShardCreationException: [de_v4][0] failed to create shard\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:357)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:704)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:605)\n    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:480)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.lucene.store.LockObtainFailedException: Can't lock shard [de_v4][0], timed out after 5000ms\n    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:576)\n    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:504)\n    at org.elasticsearch.index.IndexService.createShard(IndexService.java:310)\n    ... 9 more\n\n[... trimmed ...]\n\n[2015-06-22 14:49:17,414][WARN ][cluster.action.shard     ] [Grim Hunter] [de_v4][0] received shard failed for [de_v4][0], node[iTOuyuGSRLab6ZZxwnhb3g], [P], s[INITIALIZING], indexUUID [jsOkLa-_Q8GSPkKAXXfsoQ], reason [shard failure [failed to create shard][IndexShardCreationException[[de_v4][0] failed to create shard]; nested: LockObtainFailedException[Can't lock shard [de_v4][0], timed out after 5000ms]; ]]\n[2015-06-22 14:49:17,414][WARN ][cluster.action.shard     ] [Grim Hunter] [de_v4][3] received shard failed for [de_v4][3], node[iTOuyuGSRLab6ZZxwnhb3g], [P], s[INITIALIZING], indexUUID [jsOkLa-_Q8GSPkKAXXfsoQ], reason [master [Grim Hunter][iTOuyuGSRLab6ZZxwnhb3g][u-excus][inet[\/192.168.1.181:9300]] marked shard as initializing, but shard is marked as failed, resend shard failure]\n```\n\n_Update:_\nToday I adjusted the logging settings and briefly before the cluster is getting green:\n\n```\n[2015-06-23 12:21:00,100][DEBUG][search                   ] [Prime Mover] freeing search context [1], time [1435054860015], lastAccessTime [1435054776777], keepAlive [60000]\n[2015-06-23 12:21:00,101][DEBUG][search                   ] [Prime Mover] freeing search context [2], time [1435054860015], lastAccessTime [1435054776777], keepAlive [60000]\n```\n\nI hope this is the correct issue and it is okay to comment it again. \nOtherwise please let me know and I will open a new issue.\n"},{"date":"2015-06-23T09:53:55Z","author":"cleemansen","text":"btw: [clearing](https:\/\/www.elastic.co\/guide\/en\/elasticsearch\/reference\/current\/search-request-scroll.html#_clear_scroll_api) (manually) all scroll ids during closing \/ opening process solves the problem (as temporary workaround); cluster state will be green immediately after reopening the index.\n\n```\ncurl -XDELETE localhost:9200\/_search\/scroll\/_all\n```\n"},{"date":"2015-06-23T19:06:18Z","author":"clintongormley","text":"@cleemansen please open as a new ticket\n"}],"reopen_on":"2014-12-15T20:41:06Z","opened_by":"roytmana","closed_on":"2014-12-15T21:49:42Z","description":"I have a single ES 1.5-SNAPSHOT node (embedded ES in servlet container) and use the node's client.\nWhen I do \n\n```\nclient.prepareSearch(INDEX_NAME)\n        .setSearchType(SearchType.DEFAULT).setScroll(SCROLL_KEEP_ALIVE)\n        .setExtraSource(query).execute().actionGet() \n...\nsearchResponse = client.prepareSearchScroll(searchResponse.getScrollId())\n   .setScroll(SCROLL_KEEP_ALIVE).execute().actionGet();\n```\n\nand the stop my servlet container it gets stuck and I get\n\"Not all shards are closed yet, waited 30sec - stopping service\" in the logs\n\nif I wait after running the search a bit a minute or so the shutdown is instantaneous. The time to wait seem to be related to specified keep alive time \n\n_If I do not use scroll I do not have issue_\n\nI might be doing something wrong but ES examples do not indicate that I need to somehow close my search \n\nI do close the client prior to letting ES NodeServlet close the node (even thoug samles do not seem to imply the need for closing node client) but it ma\n","id":"51867206","title":"Using Search Scroll with keep alive prevent shards from closing","reopen_by":"s1monw","opened_on":"2014-12-13T00:00:58Z","closed_by":"s1monw"},{"number":"8830","comments":[{"date":"2014-12-08T22:08:41Z","author":"bleskes","text":"Agreed it's a pain.  This is already fixed in https:\/\/github.com\/elasticsearch\/elasticsearch\/pull\/8321 , which will be released with 1.5.0  \n\nI'm closing this, but if you feel something is missing from that PR, please feel free to re-open.\n"},{"date":"2014-12-08T22:13:54Z","author":"grantr","text":"Excellent. Thanks much @bleskes!\n"},{"date":"2014-12-10T20:49:54Z","author":"grantr","text":"@bleskes it just occurred to me that restoring global state from a snapshot can update persistent settings:\n\nhttp:\/\/www.elasticsearch.org\/guide\/en\/elasticsearch\/reference\/current\/modules-snapshots.html#_restore\n\n> The restored persistent settings are added to the existing persistent settings.\n\n@imotov what happens if a snapshot restores a persistent setting for `minimum_master_nodes` that is greater than the current master count?\n"},{"date":"2014-12-24T01:12:15Z","author":"imotov","text":"@grantr yes, this indeed can be an issue. Thanks!\n"},{"date":"2014-12-24T01:45:36Z","author":"grantr","text":"Thanks for fixing this @imotov!\n"}],"reopen_on":"2014-12-24T01:12:15Z","opened_by":"grantr","closed_on":"2015-01-18T02:31:07Z","description":"If I have 3 masters and accidentally set `minimum_master_nodes` to 4 dynamically, the cluster will stop. While the cluster is stopped, there's no way to update settings, so I have to do a full restart.\n\nIf I updated a persistent setting, then not even a full restart will fix the issue. I have to manually edit cluster state.\n\nYou could get out of this situation by adding master nodes until the new minimum is reached, but that's not a complete solution because I might have fat fingered `minimum_master_nodes` to 100 or something like that.\n\nI think it'd be worth adding some validation to ensure that an update to `minimum_master_nodes` won't accidentally put the cluster in an unrecoverable state.\n","id":"51358266","title":"Restore process can restore incompatible `minimum_master_nodes` setting","reopen_by":"imotov","opened_on":"2014-12-08T22:00:47Z","closed_by":"imotov"},{"number":"8788","comments":[{"date":"2014-12-09T12:14:51Z","author":"clintongormley","text":"Hi @bobrik \n\nIs there any chance this index was written with Elasticsearch 1.2.0?\n\nPlease could you provide the output of this request:\n\n```\ncurl -s \"http:\/\/web245:9200\/statistics-20141110\/_search?pretty&q=_id:1jC2LxTjTMS1KHCn0Prf1w&explain&fields=_source,_routing\"\n```\n"},{"date":"2014-12-09T12:33:09Z","author":"bobrik","text":"Routing is automatically inferred from `@key`\n\n``` json\n{\n  \"took\" : 1744,\n  \"timed_out\" : false,\n  \"_shards\" : {\n    \"total\" : 5,\n    \"successful\" : 5,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : 2,\n    \"max_score\" : 1.0,\n    \"hits\" : [ {\n      \"_shard\" : 3,\n      \"_node\" : \"YOK_20U7Qee-XSasg0J8VA\",\n      \"_index\" : \"statistics-20141110\",\n      \"_type\" : \"events\",\n      \"_id\" : \"1jC2LxTjTMS1KHCn0Prf1w\",\n      \"_score\" : 1.0,\n      \"_source\":{\"@timestamp\":\"2014-11-10T14:30:00+0300\",\"@key\":\"client_belarussia_msg_sended_from_mutual__22_1\",\"@value\":\"149\"},\n      \"fields\" : {\n        \"_routing\" : \"client_belarussia_msg_sended_from_mutual__22_1\"\n      },\n      \"_explanation\" : {\n        \"value\" : 1.0,\n        \"description\" : \"ConstantScore(_uid:events#1jC2LxTjTMS1KHCn0Prf1w _uid:markers#1jC2LxTjTMS1KHCn0Prf1w _uid:precise#1jC2LxTjTMS1KHCn0Prf1w _uid:rfm_users#1jC2LxTjTMS1KHCn0Prf1w), product of:\",\n        \"details\" : [ {\n          \"value\" : 1.0,\n          \"description\" : \"boost\"\n        }, {\n          \"value\" : 1.0,\n          \"description\" : \"queryNorm\"\n        } ]\n      }\n    }, {\n      \"_shard\" : 3,\n      \"_node\" : \"YOK_20U7Qee-XSasg0J8VA\",\n      \"_index\" : \"statistics-20141110\",\n      \"_type\" : \"events\",\n      \"_id\" : \"1jC2LxTjTMS1KHCn0Prf1w\",\n      \"_score\" : 1.0,\n      \"_source\":{\"@timestamp\":\"2014-11-10T14:30:00+0300\",\"@key\":\"client_belarussia_msg_sended_from_mutual__22_1\",\"@value\":\"149\"},\n      \"fields\" : {\n        \"_routing\" : \"client_belarussia_msg_sended_from_mutual__22_1\"\n      },\n      \"_explanation\" : {\n        \"value\" : 1.0,\n        \"description\" : \"ConstantScore(_uid:events#1jC2LxTjTMS1KHCn0Prf1w _uid:markers#1jC2LxTjTMS1KHCn0Prf1w _uid:precise#1jC2LxTjTMS1KHCn0Prf1w _uid:rfm_users#1jC2LxTjTMS1KHCn0Prf1w), product of:\",\n        \"details\" : [ {\n          \"value\" : 1.0,\n          \"description\" : \"boost\"\n        }, {\n          \"value\" : 1.0,\n          \"description\" : \"queryNorm\"\n        } ]\n      }\n    } ]\n  }\n}\n```\n\nIndex was created on 1.3.4, we upgraded from 1.0.1 to 1.3.2 on 2014-09-22\n"},{"date":"2014-12-09T13:44:59Z","author":"clintongormley","text":"Hi @bobrik \n\nHmm, these two docs are on the same shard!  Do you ever run updates on these docs? Could you send the output of this command please?\n\n```\ncurl -s \"http:\/\/web245:9200\/statistics-20141110\/_search?pretty&q=_id:1jC2LxTjTMS1KHCn0Prf1w&explain&fields=_source,_routing,_version\"\n```\n"},{"date":"2014-12-09T13:48:41Z","author":"bobrik","text":"Of course they are, that's how routing works :)\n\nI didn't run any updates, because my code only does indexing. It doesn't even know ids that are assigned by elasticsearch.\n\n``` json\n{\n  \"took\" : 51,\n  \"timed_out\" : false,\n  \"_shards\" : {\n    \"total\" : 5,\n    \"successful\" : 5,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : 2,\n    \"max_score\" : 1.0,\n    \"hits\" : [ {\n      \"_shard\" : 3,\n      \"_node\" : \"YOK_20U7Qee-XSasg0J8VA\",\n      \"_index\" : \"statistics-20141110\",\n      \"_type\" : \"events\",\n      \"_id\" : \"1jC2LxTjTMS1KHCn0Prf1w\",\n      \"_score\" : 1.0,\n      \"_source\":{\"@timestamp\":\"2014-11-10T14:30:00+0300\",\"@key\":\"client_belarussia_msg_sended_from_mutual__22_1\",\"@value\":\"149\"},\n      \"fields\" : {\n        \"_routing\" : \"client_belarussia_msg_sended_from_mutual__22_1\"\n      },\n      \"_explanation\" : {\n        \"value\" : 1.0,\n        \"description\" : \"ConstantScore(_uid:events#1jC2LxTjTMS1KHCn0Prf1w _uid:markers#1jC2LxTjTMS1KHCn0Prf1w _uid:precise#1jC2LxTjTMS1KHCn0Prf1w _uid:rfm_users#1jC2LxTjTMS1KHCn0Prf1w), product of:\",\n        \"details\" : [ {\n          \"value\" : 1.0,\n          \"description\" : \"boost\"\n        }, {\n          \"value\" : 1.0,\n          \"description\" : \"queryNorm\"\n        } ]\n      }\n    }, {\n      \"_shard\" : 3,\n      \"_node\" : \"YOK_20U7Qee-XSasg0J8VA\",\n      \"_index\" : \"statistics-20141110\",\n      \"_type\" : \"events\",\n      \"_id\" : \"1jC2LxTjTMS1KHCn0Prf1w\",\n      \"_score\" : 1.0,\n      \"_source\":{\"@timestamp\":\"2014-11-10T14:30:00+0300\",\"@key\":\"client_belarussia_msg_sended_from_mutual__22_1\",\"@value\":\"149\"},\n      \"fields\" : {\n        \"_routing\" : \"client_belarussia_msg_sended_from_mutual__22_1\"\n      },\n      \"_explanation\" : {\n        \"value\" : 1.0,\n        \"description\" : \"ConstantScore(_uid:events#1jC2LxTjTMS1KHCn0Prf1w _uid:markers#1jC2LxTjTMS1KHCn0Prf1w _uid:precise#1jC2LxTjTMS1KHCn0Prf1w _uid:rfm_users#1jC2LxTjTMS1KHCn0Prf1w), product of:\",\n        \"details\" : [ {\n          \"value\" : 1.0,\n          \"description\" : \"boost\"\n        }, {\n          \"value\" : 1.0,\n          \"description\" : \"queryNorm\"\n        } ]\n      }\n    } ]\n  }\n}\n```\n"},{"date":"2014-12-09T16:56:22Z","author":"clintongormley","text":"Sorry @bobrik - I gave you the wrong request, it should be:\n\n```\ncurl -s \"http:\/\/web245:9200\/statistics-20141110\/_search?pretty&q=_id:1jC2LxTjTMS1KHCn0Prf1w&explain&fields=_source,_routing,version\"\n```\n\nAnd so you're using auto-assigned IDs?  Did any of your shards migrate to other nodes, or did a primary fail during optimization?\n"},{"date":"2014-12-09T17:05:25Z","author":"s1monw","text":"I think this is caused by https:\/\/github.com\/elasticsearch\/elasticsearch\/pull\/7729 @bobrik are you coming from < 1.3.3 with this index and are you using bulk?\n"},{"date":"2014-12-09T17:06:19Z","author":"bobrik","text":"```\ncurl -s 'http:\/\/web605:9200\/statistics-20141110\/_search?pretty&q=_id:1jC2LxTjTMS1KHCn0Prf1w&explain&fields=_source,_routing,version'\n```\n\n``` json\n{\n  \"took\" : 46,\n  \"timed_out\" : false,\n  \"_shards\" : {\n    \"total\" : 5,\n    \"successful\" : 5,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : 2,\n    \"max_score\" : 1.0,\n    \"hits\" : [ {\n      \"_shard\" : 3,\n      \"_node\" : \"YOK_20U7Qee-XSasg0J8VA\",\n      \"_index\" : \"statistics-20141110\",\n      \"_type\" : \"events\",\n      \"_id\" : \"1jC2LxTjTMS1KHCn0Prf1w\",\n      \"_score\" : 1.0,\n      \"_source\":{\"@timestamp\":\"2014-11-10T14:30:00+0300\",\"@key\":\"client_belarussia_msg_sended_from_mutual__22_1\",\"@value\":\"149\"},\n      \"fields\" : {\n        \"_routing\" : \"client_belarussia_msg_sended_from_mutual__22_1\"\n      },\n      \"_explanation\" : {\n        \"value\" : 1.0,\n        \"description\" : \"ConstantScore(_uid:events#1jC2LxTjTMS1KHCn0Prf1w _uid:markers#1jC2LxTjTMS1KHCn0Prf1w _uid:precise#1jC2LxTjTMS1KHCn0Prf1w _uid:rfm_users#1jC2LxTjTMS1KHCn0Prf1w), product of:\",\n        \"details\" : [ {\n          \"value\" : 1.0,\n          \"description\" : \"boost\"\n        }, {\n          \"value\" : 1.0,\n          \"description\" : \"queryNorm\"\n        } ]\n      }\n    }, {\n      \"_shard\" : 3,\n      \"_node\" : \"YOK_20U7Qee-XSasg0J8VA\",\n      \"_index\" : \"statistics-20141110\",\n      \"_type\" : \"events\",\n      \"_id\" : \"1jC2LxTjTMS1KHCn0Prf1w\",\n      \"_score\" : 1.0,\n      \"_source\":{\"@timestamp\":\"2014-11-10T14:30:00+0300\",\"@key\":\"client_belarussia_msg_sended_from_mutual__22_1\",\"@value\":\"149\"},\n      \"fields\" : {\n        \"_routing\" : \"client_belarussia_msg_sended_from_mutual__22_1\"\n      },\n      \"_explanation\" : {\n        \"value\" : 1.0,\n        \"description\" : \"ConstantScore(_uid:events#1jC2LxTjTMS1KHCn0Prf1w _uid:markers#1jC2LxTjTMS1KHCn0Prf1w _uid:precise#1jC2LxTjTMS1KHCn0Prf1w _uid:rfm_users#1jC2LxTjTMS1KHCn0Prf1w), product of:\",\n        \"details\" : [ {\n          \"value\" : 1.0,\n          \"description\" : \"boost\"\n        }, {\n          \"value\" : 1.0,\n          \"description\" : \"queryNorm\"\n        } ]\n      }\n    } ]\n  }\n}\n```\n\nI bet you wanted this:\n\n```\ncurl -s 'http:\/\/web605:9200\/statistics-20141110\/_search?pretty&q=_id:1jC2LxTjTMS1KHCn0Prf1w&explain&fields=_source,_routing' -d '{\"version\":true}'\n```\n\n``` json\n{\n  \"took\" : 1,\n  \"timed_out\" : false,\n  \"_shards\" : {\n    \"total\" : 5,\n    \"successful\" : 5,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : 2,\n    \"max_score\" : 1.0,\n    \"hits\" : [ {\n      \"_shard\" : 3,\n      \"_node\" : \"YOK_20U7Qee-XSasg0J8VA\",\n      \"_index\" : \"statistics-20141110\",\n      \"_type\" : \"events\",\n      \"_id\" : \"1jC2LxTjTMS1KHCn0Prf1w\",\n      \"_version\" : 1,\n      \"_score\" : 1.0,\n      \"_source\":{\"@timestamp\":\"2014-11-10T14:30:00+0300\",\"@key\":\"client_belarussia_msg_sended_from_mutual__22_1\",\"@value\":\"149\"},\n      \"fields\" : {\n        \"_routing\" : \"client_belarussia_msg_sended_from_mutual__22_1\"\n      },\n      \"_explanation\" : {\n        \"value\" : 1.0,\n        \"description\" : \"ConstantScore(_uid:events#1jC2LxTjTMS1KHCn0Prf1w _uid:markers#1jC2LxTjTMS1KHCn0Prf1w _uid:precise#1jC2LxTjTMS1KHCn0Prf1w _uid:rfm_users#1jC2LxTjTMS1KHCn0Prf1w), product of:\",\n        \"details\" : [ {\n          \"value\" : 1.0,\n          \"description\" : \"boost\"\n        }, {\n          \"value\" : 1.0,\n          \"description\" : \"queryNorm\"\n        } ]\n      }\n    }, {\n      \"_shard\" : 3,\n      \"_node\" : \"YOK_20U7Qee-XSasg0J8VA\",\n      \"_index\" : \"statistics-20141110\",\n      \"_type\" : \"events\",\n      \"_id\" : \"1jC2LxTjTMS1KHCn0Prf1w\",\n      \"_version\" : 1,\n      \"_score\" : 1.0,\n      \"_source\":{\"@timestamp\":\"2014-11-10T14:30:00+0300\",\"@key\":\"client_belarussia_msg_sended_from_mutual__22_1\",\"@value\":\"149\"},\n      \"fields\" : {\n        \"_routing\" : \"client_belarussia_msg_sended_from_mutual__22_1\"\n      },\n      \"_explanation\" : {\n        \"value\" : 1.0,\n        \"description\" : \"ConstantScore(_uid:events#1jC2LxTjTMS1KHCn0Prf1w _uid:markers#1jC2LxTjTMS1KHCn0Prf1w _uid:precise#1jC2LxTjTMS1KHCn0Prf1w _uid:rfm_users#1jC2LxTjTMS1KHCn0Prf1w), product of:\",\n        \"details\" : [ {\n          \"value\" : 1.0,\n          \"description\" : \"boost\"\n        }, {\n          \"value\" : 1.0,\n          \"description\" : \"queryNorm\"\n        } ]\n      }\n    } ]\n  }\n}\n```\n\nThere were many migrations, but not during optimization, unless es moves shards after new index is created. Basically at 00:00 new index is created and at 00:45 optimization for old indices starts.\n"},{"date":"2014-12-09T17:09:22Z","author":"s1monw","text":"do you have client nodes that are pre 1.3.3?\n"},{"date":"2014-12-09T17:12:53Z","author":"bobrik","text":"@s1monw index is created on 1.3.4:\n\n```\n[2014-09-30 12:03:49,991][INFO ][node                     ] [statistics04] version[1.3.3], pid[17937], build[ddf796d\/2014-09-29T13:39:00Z]\n```\n\n```\n[2014-09-30 14:03:19,205][INFO ][node                     ] [statistics04] version[1.3.4], pid[89485], build[a70f3cc\/2014-09-30T09:07:17Z]\n```\n\nNov 11 is definitely after Sep 30. Shouldn't be #7729 then.\n\nWe don't have client nodes, everything is over http. But yeah, we use bulk indexing and automatically assigned ids.\n"},{"date":"2014-12-09T17:25:10Z","author":"clintongormley","text":"Hi @bobrik \n\n(you guessed right about `version=true` :) )\n\nOK - we're going to need more info. Please could you send:\n\n```\ncurl -s 'http:\/\/web605:9200\/statistics-20141110\/_settings?pretty'\ncurl -s 'http:\/\/web605:9200\/statistics-20141110\/_segments?pretty'\n```\n"},{"date":"2014-12-09T18:18:44Z","author":"bobrik","text":"``` json\n{\n  \"statistics-20141110\" : {\n    \"settings\" : {\n      \"index\" : {\n        \"codec\" : {\n          \"bloom\" : {\n            \"load\" : \"false\"\n          }\n        },\n        \"uuid\" : \"JZXC-8C3TFC71EnMGMHSWw\",\n        \"number_of_replicas\" : \"0\",\n        \"number_of_shards\" : \"5\",\n        \"version\" : {\n          \"created\" : \"1030499\"\n        }\n      }\n    }\n  }\n}\n```\n\n``` json\n{\n  \"_shards\" : {\n    \"total\" : 5,\n    \"successful\" : 5,\n    \"failed\" : 0\n  },\n  \"indices\" : {\n    \"statistics-20141110\" : {\n      \"shards\" : {\n        \"0\" : [ {\n          \"routing\" : {\n            \"state\" : \"STARTED\",\n            \"primary\" : true,\n            \"node\" : \"hBg3FpLGQw6B9l-Hil2c8Q\"\n          },\n          \"num_committed_segments\" : 2,\n          \"num_search_segments\" : 2,\n          \"segments\" : {\n            \"_gga\" : {\n              \"generation\" : 21322,\n              \"num_docs\" : 14939669,\n              \"deleted_docs\" : 0,\n              \"size_in_bytes\" : 1729206228,\n              \"memory_in_bytes\" : 4943008,\n              \"committed\" : true,\n              \"search\" : true,\n              \"version\" : \"4.9.0\",\n              \"compound\" : false\n            },\n            \"_isc\" : {\n              \"generation\" : 24348,\n              \"num_docs\" : 10913518,\n              \"deleted_docs\" : 0,\n              \"size_in_bytes\" : 1254410507,\n              \"memory_in_bytes\" : 4101712,\n              \"committed\" : true,\n              \"search\" : true,\n              \"version\" : \"4.9.0\",\n              \"compound\" : false\n            }\n          }\n        } ],\n        \"1\" : [ {\n          \"routing\" : {\n            \"state\" : \"STARTED\",\n            \"primary\" : true,\n            \"node\" : \"ajMe-w2lSIO0Tz5WEUs4qQ\"\n          },\n          \"num_committed_segments\" : 2,\n          \"num_search_segments\" : 2,\n          \"segments\" : {\n            \"_7i7\" : {\n              \"generation\" : 9727,\n              \"num_docs\" : 7023269,\n              \"deleted_docs\" : 0,\n              \"size_in_bytes\" : 803299557,\n              \"memory_in_bytes\" : 2264472,\n              \"committed\" : true,\n              \"search\" : true,\n              \"version\" : \"4.9.0\",\n              \"compound\" : false\n            },\n            \"_i01\" : {\n              \"generation\" : 23329,\n              \"num_docs\" : 14689581,\n              \"deleted_docs\" : 0,\n              \"size_in_bytes\" : 1659303375,\n              \"memory_in_bytes\" : 4788872,\n              \"committed\" : true,\n              \"search\" : true,\n              \"version\" : \"4.9.0\",\n              \"compound\" : false\n            }\n          }\n        } ],\n        \"2\" : [ {\n          \"routing\" : {\n            \"state\" : \"STARTED\",\n            \"primary\" : true,\n            \"node\" : \"hyUu93q7SRehHBVZfSmvOg\"\n          },\n          \"num_committed_segments\" : 2,\n          \"num_search_segments\" : 2,\n          \"segments\" : {\n            \"_9wx\" : {\n              \"generation\" : 12849,\n              \"num_docs\" : 8995444,\n              \"deleted_docs\" : 0,\n              \"size_in_bytes\" : 1035711205,\n              \"memory_in_bytes\" : 3326288,\n              \"committed\" : true,\n              \"search\" : true,\n              \"version\" : \"4.9.0\",\n              \"compound\" : false\n            },\n            \"_il1\" : {\n              \"generation\" : 24085,\n              \"num_docs\" : 13205585,\n              \"deleted_docs\" : 0,\n              \"size_in_bytes\" : 1510021893,\n              \"memory_in_bytes\" : 4343736,\n              \"committed\" : true,\n              \"search\" : true,\n              \"version\" : \"4.9.0\",\n              \"compound\" : false\n            }\n          }\n        } ],\n        \"3\" : [ {\n          \"routing\" : {\n            \"state\" : \"STARTED\",\n            \"primary\" : true,\n            \"node\" : \"hyUu93q7SRehHBVZfSmvOg\"\n          },\n          \"num_committed_segments\" : 2,\n          \"num_search_segments\" : 2,\n          \"segments\" : {\n            \"_8pc\" : {\n              \"generation\" : 11280,\n              \"num_docs\" : 10046395,\n              \"deleted_docs\" : 0,\n              \"size_in_bytes\" : 1143637974,\n              \"memory_in_bytes\" : 4003824,\n              \"committed\" : true,\n              \"search\" : true,\n              \"version\" : \"4.9.0\",\n              \"compound\" : false\n            },\n            \"_hwt\" : {\n              \"generation\" : 23213,\n              \"num_docs\" : 13226096,\n              \"deleted_docs\" : 0,\n              \"size_in_bytes\" : 1485110397,\n              \"memory_in_bytes\" : 4287544,\n              \"committed\" : true,\n              \"search\" : true,\n              \"version\" : \"4.9.0\",\n              \"compound\" : false\n            }\n          }\n        } ],\n        \"4\" : [ {\n          \"routing\" : {\n            \"state\" : \"STARTED\",\n            \"primary\" : true,\n            \"node\" : \"hyUu93q7SRehHBVZfSmvOg\"\n          },\n          \"num_committed_segments\" : 2,\n          \"num_search_segments\" : 2,\n          \"segments\" : {\n            \"_91i\" : {\n              \"generation\" : 11718,\n              \"num_docs\" : 8328558,\n              \"deleted_docs\" : 0,\n              \"size_in_bytes\" : 953452801,\n              \"memory_in_bytes\" : 2822712,\n              \"committed\" : true,\n              \"search\" : true,\n              \"version\" : \"4.9.0\",\n              \"compound\" : false\n            },\n            \"_hms\" : {\n              \"generation\" : 22852,\n              \"num_docs\" : 14848927,\n              \"deleted_docs\" : 0,\n              \"size_in_bytes\" : 1673336536,\n              \"memory_in_bytes\" : 4777472,\n              \"committed\" : true,\n              \"search\" : true,\n              \"version\" : \"4.9.0\",\n              \"compound\" : false\n            }\n          }\n        } ]\n      }\n    }\n  }\n}\n```\n"},{"date":"2015-01-02T18:23:22Z","author":"brwe","text":"Reopen because the test added with #9125 just failed and the failure is reproducible (about 1\/10 runs with same seed and added stress), see http:\/\/build-us-00.elasticsearch.org\/job\/es_core_master_window-2012\/725\/ \n"},{"date":"2015-04-08T14:59:55Z","author":"mrec","text":"We've just seen this issue for the second time. The first time produced only a single duplicate; this time produced over 16000, across a comparatively tiny index (< 300k docs). We're using 1.3.4, doing bulk indexing with the Java client API's `BulkProcessor` and `TransportClient`. \n\nHowever, we're **not** using autogenerated ids, so from my reading of the fix for this issue it's unlikely to help us. Should I open a separate issue, or should this one be reopened?\n\nMiscellanous other info:\n- The index has not been migrated from an earlier version.\n- Around the time the duplicates appeared, we saw problems in other (non-Elastic) parts of the system. I can't see any way that they could directly cause the duplication, but it's possible that network issues were the common cause of both.\n- We still have the index containing duplicates for now, though it may not last long; this is on an alpha cluster that gets reset fairly often.\n- I'm very much a newbie to Elastic, so may be missing something obvious.\n"},{"date":"2015-04-08T16:26:55Z","author":"brwe","text":"@mrec It would be great if you could open a new issue. Please also add a query that finds duplicates together with `?explain=true` option set and the output of that query like above. \nSomething like: \n\n```\ncurl -s 'http:\/\/HOST:PORT\/YOURINDEX\/_search?pretty&q=_id:A_DUPLICATE_ID&explain&fields=_source,_routing' -d '{\"version\":true}'\n```\n\nIs there a way that you can make available the elasticsearch logs from the time where you had the network issues?\nAlso, the output of\ncurl -s 'http:\/\/web605:9200\/statistics-20141110\/_segments?pretty' might be helpful.\n"}],"reopen_on":"2015-01-02T18:23:22Z","opened_by":"bobrik","closed_on":"2015-01-29T15:37:05Z","description":"I decided to reindex my data to take advantage of `doc_values`, but one of 30 indices (~120m docs in each) got less documents after reindexing. I reindexed again and docs disappeared again.\n\nThen I bisected the problem to specific docs and found that some docs in source index has duplicate ids.\n\n```\ncurl -s \"http:\/\/web245:9200\/statistics-20141110\/_search?pretty&q=_id:1jC2LxTjTMS1KHCn0Prf1w\"\n```\n\n``` json\n{\n  \"took\" : 1156,\n  \"timed_out\" : false,\n  \"_shards\" : {\n    \"total\" : 5,\n    \"successful\" : 5,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : 2,\n    \"max_score\" : 1.0,\n    \"hits\" : [ {\n      \"_index\" : \"statistics-20141110\",\n      \"_type\" : \"events\",\n      \"_id\" : \"1jC2LxTjTMS1KHCn0Prf1w\",\n      \"_score\" : 1.0,\n      \"_source\":{\"@timestamp\":\"2014-11-10T14:30:00+0300\",\"@key\":\"client_belarussia_msg_sended_from_mutual__22_1\",\"@value\":\"149\"}\n    }, {\n      \"_index\" : \"statistics-20141110\",\n      \"_type\" : \"events\",\n      \"_id\" : \"1jC2LxTjTMS1KHCn0Prf1w\",\n      \"_score\" : 1.0,\n      \"_source\":{\"@timestamp\":\"2014-11-10T14:30:00+0300\",\"@key\":\"client_belarussia_msg_sended_from_mutual__22_1\",\"@value\":\"149\"}\n    } ]\n  }\n}\n```\n\nHere are two indices, source and destination:\n\n```\nhealth status index                  pri rep docs.count docs.deleted store.size pri.store.size\ngreen  open   statistics-20141110      5   0  116217042            0     12.3gb         12.3gb\ngreen  open   statistics-20141110-dv   5   1  116216507            0     32.3gb         16.1gb\n```\n\nSegments of problematic index:\n\n```\nindex               shard prirep ip            segment generation docs.count docs.deleted    size size.memory committed searchable version compound\nstatistics-20141110 0     p      192.168.0.190 _gga         21322   14939669            0   1.6gb     4943008 true      true       4.9.0   false\nstatistics-20141110 0     p      192.168.0.190 _isc         24348   10913518            0   1.1gb     4101712 true      true       4.9.0   false\nstatistics-20141110 1     p      192.168.0.245 _7i7          9727    7023269            0   766mb     2264472 true      true       4.9.0   false\nstatistics-20141110 1     p      192.168.0.245 _i01         23329   14689581            0   1.5gb     4788872 true      true       4.9.0   false\nstatistics-20141110 2     p      192.168.1.212 _9wx         12849    8995444            0 987.7mb     3326288 true      true       4.9.0   false\nstatistics-20141110 2     p      192.168.1.212 _il1         24085   13205585            0   1.4gb     4343736 true      true       4.9.0   false\nstatistics-20141110 3     p      192.168.1.212 _8pc         11280   10046395            0     1gb     4003824 true      true       4.9.0   false\nstatistics-20141110 3     p      192.168.1.212 _hwt         23213   13226096            0   1.3gb     4287544 true      true       4.9.0   false\nstatistics-20141110 4     p      192.168.2.88  _91i         11718    8328558            0 909.2mb     2822712 true      true       4.9.0   false\nstatistics-20141110 4     p      192.168.2.88  _hms         22852   14848927            0   1.5gb     4777472 true      true       4.9.0   false\n```\n\nThe only thing that happened with index besides indexing is optimizing to 2 segments per shard.\n","id":"51073197","title":"Duplicate id in index","reopen_by":"brwe","opened_on":"2014-12-05T08:10:06Z","closed_by":"brwe"},{"number":"8490","comments":[{"date":"2014-11-17T11:30:16Z","author":"clintongormley","text":"Hi @aaneja \n\nClosing in favour of #8424\n"},{"date":"2014-11-17T17:50:14Z","author":"aaneja","text":"I read the related issue and the documentation for date rounding (http:\/\/www.elasticsearch.org\/guide\/en\/elasticsearch\/reference\/current\/mapping-date-format.html#date-math)\n\nIt appears that date rounding is to the nearest second - if this is so, and I want millisecond precision is my only option passing range values as UNIX millisecond timestamps ?\n(If I use : \n\n```\n{\n  \"range\": {\n    \"context.data.eventTime\": {\n      \"gte\": \"1415804099000\",\n      \"lte\": \"1415804100000\"\n    }\n  }\n}\n```\n\nI get right results - no records match)\n"},{"date":"2014-11-17T18:00:33Z","author":"clintongormley","text":"Rounding only happens if you specify it, eg \"some_date\/d\"\n\nWhat's happening is that the `date_format` that you're using (`dateOptionalTime`) doesn't include milliseconds. So milliseconds are being ignored both in the values that you index AND in the range clause.  Try using `basic_date_time`.  See http:\/\/www.elasticsearch.org\/guide\/en\/elasticsearch\/reference\/current\/mapping-date-format.html#built-in\n"},{"date":"2014-11-17T18:32:21Z","author":"aaneja","text":"I think `dateOptionalTime` does include milliseconds on indexing. As you see from above one of the records has timestamp : \"2014-11-12T14:55:00.1458607Z\" which in UNIX milliseconds is 1415804100145.\n\nI ran a query with \n\n```\n{\n  \"range\": {\n    \"context.data.eventTime\": {\n      \"gte\": \"1415804100145\",\n      \"lte\": \"1415804100145\"\n    }\n  }\n}\n```\n\nand this record matched. If I try the same range filter with 1415804100146 - 1415804100149 no records match \n"},{"date":"2014-11-17T18:34:42Z","author":"clintongormley","text":"Hmm you may be right - I'll have to take another look at this one tomorrow\n"},{"date":"2014-11-19T17:50:45Z","author":"aaneja","text":"Ping. Any updates on what the expected behavior of `dateOptionalTime` should be ?\n"},{"date":"2014-11-24T11:51:26Z","author":"clintongormley","text":"Hi @aaneja \n\nSo you're correct: `dateOptionalTime` does include milliseconds.  But I can't replicate your findings.  Your query correctly returns no results when I run it.  I tried on 1.2.1, 1.3.4, and 1.4.0.\n\nDo you perhaps have another field with the same name but a different mapping?  Can you provide a working recreation of this issue? What version of Elasticsearch are you running?\n"},{"date":"2014-12-05T00:50:34Z","author":"aaneja","text":"Here's a repro -\n`GET \/`\nResponse  -\n\n```\n{\n   \"status\": 200,\n   \"name\": \"Terror\",\n   \"version\": {\n      \"number\": \"1.3.2\",\n      \"build_hash\": \"dee175dbe2f254f3f26992f5d7591939aaefd12f\",\n      \"build_timestamp\": \"2014-08-13T14:29:30Z\",\n      \"build_snapshot\": false,\n      \"lucene_version\": \"4.9\"\n   },\n   \"tagline\": \"You Know, for Search\"\n}\n```\n\nThen -\n\n```\nDELETE \/megacorp\/tweet\n\nPUT \/megacorp\/_mapping\/tweet\n{\n    \"tweet\" : {\n        \"properties\": {\n              \"eventTime\": {\n                \"type\": \"date\",\n                \"format\": \"dateOptionalTime\"\n              }\n            }\n    }\n}\n\nPUT \/megacorp\/tweet\/1?pretty\n{\n  \"eventTime\": 1415832900146\n}\n\nPUT \/megacorp\/tweet\/2?pretty\n{\n  \"eventTime\": 1415832900597\n}\n\n\nPOST \/megacorp\/tweet\/_search\n{\n  \"size\": 14,\n  \"_source\": \"eventTime\",\n  \"query\": {\n    \"filtered\": {\n      \"filter\": {\n        \"bool\": {\n          \"must\": [\n            {\n              \"range\": {\n                \"eventTime\": {\n                  \"gte\": \"2014-11-12T22:54:00Z\",\n                  \"lte\": \"2014-11-12T22:55:00Z\"\n                }\n              }\n            }\n          ]\n        }\n      }\n    }\n  }\n}\n\n\nPOST \/megacorp\/tweet\/_search\n{\n  \"size\": 14,\n  \"_source\": \"eventTime\",\n  \"query\": {\n    \"filtered\": {\n      \"filter\": {\n        \"bool\": {\n          \"must\": [\n            {\n              \"range\": {\n                \"eventTime\": {\n                  \"gte\": 1415832840000,\n                  \"lte\": 1415832900000\n                }\n              }\n            }\n          ]\n        }\n      }\n    }\n  }\n}\n```\n\nThe last two queries should be, IMO, equivalent.\nHowever I get incorrect results in the first one; the second one correctly matches no records\n"},{"date":"2014-12-05T22:08:58Z","author":"rjernst","text":"I've tracked this down to a bug in the date parser and am working on a fix.\n"},{"date":"2014-12-08T23:24:26Z","author":"rjernst","text":"I have a fix in #8834.  Note that this is really a first step towards eliminating the two different parsing functions in the date math parser (see #8598).\n"},{"date":"2014-12-09T22:16:14Z","author":"rjernst","text":"After discussing the further, I think the best immediate fix here is to set `mapping.date.round_ceil = false`.  This should fix the problem described above (essentially doing via configuration what my patch in #8834 does).  In order to not introduce breaking behavior, I am going to drop #8834 entirely and work on the full removal of `mapping.date.round_ceil` for #8598.\n"},{"date":"2014-12-11T01:18:01Z","author":"rjernst","text":"I'm closing this in favor of #8598 which will be fixed for 2.0.\n"}],"reopen_on":"2014-11-17T18:34:42Z","opened_by":"aaneja","closed_on":"2014-12-11T01:18:06Z","description":"If I run the query :\n\n```\n{\n  \"size\": 14,\n  \"_source\": \"context.data.eventTime\",\n  \"query\": {\n    \"filtered\": {\n      \"filter\": {\n        \"bool\": {\n          \"must\": [\n            {\n              \"range\": {\n                \"context.data.eventTime\": {\n                  \"gte\": \"2014-11-12T14:54:59.000Z\",\n                  \"lte\": \"2014-11-12T14:55:00.000Z\"\n                }\n              }\n            }\n          ]\n        }\n      }\n    }\n  }\n}\n```\n\nI get :\n\n```\n{\n    \"took\": 0,\n    \"timed_out\": false,\n    \"_shards\": {\n        \"total\": 1,\n        \"successful\": 1,\n        \"failed\": 0\n    },\n    \"hits\": {\n        \"total\": 3,\n        \"max_score\": 1,\n        \"hits\": [\n            {\n                \"_index\": \"test\",\n                \"_type\": \"request\",\n                \"_id\": \"4a5acb28-e6e8-4dfc-b86f-80926ed82fd7\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"context\": {\n                        \"data\": {\n                            \"eventTime\": \"2014-11-12T14:55:00.1458607Z\"\n                        }\n                    }\n                }\n            },\n            {\n                \"_index\": \"test\",\n                \"_type\": \"request\",\n                \"_id\": \"22a6d539-06a7-4dd9-860c-2ea5ed0956cf\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"context\": {\n                        \"data\": {\n                            \"eventTime\": \"2014-11-12T14:55:00.5976447Z\"\n                        }\n                    }\n                }\n            },\n            {\n                \"_index\": \"test\",\n                \"_type\": \"request\",\n                \"_id\": \"554b25f3-36a9-4f84-b2a3-69f257cf1ac0\",\n                \"_score\": 1,\n                \"_source\": {\n                    \"context\": {\n                        \"data\": {\n                            \"eventTime\": \"2014-11-12T14:55:00.9979586Z\"\n                        }\n                    }\n                }\n            }\n        ]\n    }\n}\n```\n\neventTime has mappings :\n\n```\n\"eventTime\": {\n                \"type\": \"date\",\n                \"format\": \"dateOptionalTime\"\n              }\n```\n\nShouldn't no records match ?\n\nInterestingly if I run the same query with the range filter using 'lt' instead of a 'lte' I get no records matched.\n","id":"48843314","title":"Date Range Filter unclear rounding behavior","reopen_by":"clintongormley","opened_on":"2014-11-14T21:44:18Z","closed_by":"rjernst"},{"number":"8308","comments":[{"date":"2014-10-31T13:04:29Z","author":"lukas-vlcek","text":"I was testing it with ES 1.3.0\n"},{"date":"2014-10-31T14:15:28Z","author":"lukas-vlcek","text":"@clintongormley thanks for clarifying this. Out of curiosity, does the template example in question work for you?\n"},{"date":"2014-10-31T14:34:29Z","author":"clintongormley","text":"@lukas-vlcek did you wrap it in a `query` element and pass it as an escaped string?  \n\nI've just pushed another improvement to the docs to include both of the above elements\n"},{"date":"2014-10-31T18:06:38Z","author":"lukas-vlcek","text":"@clintongormley thanks for this, but still I am unable to get search template with section work. Here is trivial recreation https:\/\/gist.github.com\/lukas-vlcek\/56540a7e8206d122d55c\nIs there anything I do wrong? Am I missing some escaping?\n"},{"date":"2014-10-31T18:12:00Z","author":"lukas-vlcek","text":"Just in case here is excerpt from server log:\n\n```\n[2014-10-31 19:03:29,575][DEBUG][action.search.type       ] [Jester] [twitter][4], node[ccMkzywSSfi-4ttibpHh9w], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@7de37772] lastShard [true]\norg.elasticsearch.ElasticsearchParseException: Failed to parse template\n    at org.elasticsearch.search.SearchService.parseTemplate(SearchService.java:612)\n    at org.elasticsearch.search.SearchService.createContext(SearchService.java:514)\n    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:487)\n    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:256)\n    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:206)\n    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:203)\n    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:517)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: org.elasticsearch.common.jackson.core.JsonParseException: Unexpected character ('{' (code 123)): was expecting either valid name character (for unquoted name) or double-quote (for quoted) to start field name\n at [Source: [B@438d7c6b; line: 1, column: 4]\n    at org.elasticsearch.common.jackson.core.JsonParser._constructError(JsonParser.java:1419)\n    at org.elasticsearch.common.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:508)\n    at org.elasticsearch.common.jackson.core.base.ParserMinimalBase._reportUnexpectedChar(ParserMinimalBase.java:437)\n    at org.elasticsearch.common.jackson.core.json.UTF8StreamJsonParser._handleOddName(UTF8StreamJsonParser.java:1808)\n    at org.elasticsearch.common.jackson.core.json.UTF8StreamJsonParser._parseName(UTF8StreamJsonParser.java:1496)\n    at org.elasticsearch.common.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:693)\n    at org.elasticsearch.common.xcontent.json.JsonXContentParser.nextToken(JsonXContentParser.java:50)\n    at org.elasticsearch.index.query.TemplateQueryParser.parse(TemplateQueryParser.java:113)\n    at org.elasticsearch.index.query.TemplateQueryParser.parse(TemplateQueryParser.java:103)\n    at org.elasticsearch.search.SearchService.parseTemplate(SearchService.java:605)\n    ... 9 more\n```\n"},{"date":"2014-11-01T14:45:00Z","author":"clintongormley","text":"@lukas-vlcek I can't get this working either. No idea what is going on here :(\n"},{"date":"2014-11-02T13:22:30Z","author":"lukas-vlcek","text":"@clintongormley thanks for look at this. Shall I change the title of this issue and remove the [DOC] part? It is probably not a DOC issue anymore, is it?\n"},{"date":"2014-11-06T01:09:38Z","author":"wwken","text":"After looking deep inside the code, I am wondering has this (i.e. the conditional clause thing) been ever working? The root clause is, inside the JsonXContentParser.java:nextToken() method, the parser.nextToken() throws an exception straight out of the box when it encounters the clause like {{#use_size}}.    I guess such clause has not been implemented to be recognized yet.  Notice that the parser is of type org.elasticsearch.common.jackson.core.json.UTF8StreamJsonParser .  \n\nIt looks like we have to override the UTF8StreamJsonParser's nextToken() method and do some look ahead for the conditional clauses.   \n"},{"date":"2014-11-06T10:05:58Z","author":"clintongormley","text":"@wwken As I understand it, the JSON parser should never see this the `{{#use_size}}`.  The template is passed as a string, not as JSON, so the Mustache should render the template (thus removing the conditionals) to generate JSON which can _then_ be parsed.\n\nThis would be much easier to debug if we had #6821\n"},{"date":"2014-11-07T03:40:07Z","author":"wwken","text":" I fixed it.  Please refer to this pull request: https:\/\/github.com\/elasticsearch\/elasticsearch\/pull\/8376\/files\n(P.S. plz ignore the above few links as i messed up checking in some incorrect commit files before by mistake)\n"},{"date":"2014-11-07T13:18:52Z","author":"clintongormley","text":"thanks @wwken - we'll take a look and get back to you\n"},{"date":"2014-11-07T17:21:43Z","author":"lukas-vlcek","text":"@clintongormley #8393 is my try to fix this issue. If you find this relevant and to the point then it might make sense to consider renaming this issue again to more general description (I think the main issue here is broken parser logic of TemplateQueryParser in case the template value is a single string token. In other words I think there is more general issue not directly related to conditional clauses only).\n"},{"date":"2014-11-07T19:25:23Z","author":"wwken","text":"@lukas-vlcek  I think for sure your solution is better! I have two suggestions on it though (minor, not any important):\n\n1) In this file, https:\/\/github.com\/lukas-vlcek\/elasticsearch\/blob\/8308\/src\/main\/java\/org\/elasticsearch\/search\/SearchService.java, you can omit the lines from 617-635 since they are not needed\n\n2) in this file, https:\/\/github.com\/lukas-vlcek\/elasticsearch\/blob\/8308\/src\/main\/java\/org\/elasticsearch\/index\/query\/TemplateQueryParser.java, you can separate the single if in line 113 to two if statements to make it more elegant :)\n\nthanks \n"},{"date":"2014-11-09T14:28:08Z","author":"lukas-vlcek","text":"@wwken thanks for looking at this. I am going to wait if ES devs give any feedback. May be they will want to fix it in very different approach. Who knows...\n"},{"date":"2014-11-12T09:58:31Z","author":"lukas-vlcek","text":"Bump.\n\nIs there any plan to have this fixed in 1.4? Or even backporting it to 1.3? We would like to make use of Search Templates but this issue is somehow limiting us. How can I help with this?\n"},{"date":"2015-06-23T17:54:59Z","author":"clintongormley","text":"Closed by #11512\n"},{"date":"2015-06-24T06:32:40Z","author":"lukas-vlcek","text":"@clintongormley correct, this issue was created before I opened more general PR. Closing...\n"}],"reopen_on":"2014-11-01T14:44:19Z","opened_by":"lukas-vlcek","closed_on":"2015-06-24T06:32:40Z","description":"Is the example of filtered query template [1] working? It seems to contain extra `<\/6>` string which seems as an issue to me. I am unable to test this example even after I remove this extra string.\n\n[1] http:\/\/www.elasticsearch.org\/guide\/en\/elasticsearch\/reference\/1.3\/search-template.html#_conditional_clauses\n","id":"47391632","title":"Search Template - conditional clauses not rendering correctly","reopen_by":"clintongormley","opened_on":"2014-10-31T13:03:49Z","closed_by":"lukas-vlcek"},{"number":"8280","comments":[{"date":"2014-10-31T19:25:48Z","author":"imotov","text":"When restore operation is interrupted because of an error, the shard that was being restored is most likely in a broken state. So, elasticsearch automatically tries to create this shard on another node in hope that next time it will be successful next time. This process doesn't stop until 1) the shard is successfully restored or 2) the index is [explicitly deleted](http:\/\/www.elasticsearch.org\/guide\/en\/elasticsearch\/reference\/current\/modules-snapshots.html#_stopping_currently_running_snapshot_and_restore_operations) by the user. So, if there is a communication issues with S3, elasticsearch will retry for as long as needed until S3 recovers. In other words, it's not a \"fake\" recovery process, it's recovery processes form S3 repository and it follows the same principles that recovery process from a local gateway.\n"},{"date":"2014-10-31T19:57:33Z","author":"JoeZ99","text":"So, if I understood you right, when there is an error during a restore process, elasticsearch will retry the restore process using another node as the shard's destination, I guess because elasticsearch thinks that the error during the first restore process was related somehow to the node it tried to restore the shard to first.\n- What if you only have one node? (that's precisely our case) does elasticsearch keeps trying on the same node?\n- Even if that's the case (elasticsearch keeps trying on the same node, since there is only one), it's not working in our scenario.the \"read timeout\" s3 error is not a permanent condition. It just happens from time to time when -probably, we haven't nailed it yet- too many requests to the S3 are being made. What this means is that immediately afterwards this error, the S3 is accessible again, and if elasticsearch \"retried\" a restore, it would be successfull, and that's not what is happening. Once this error occurs, no recovery can be made unless we explicitly delete the index.\n"},{"date":"2014-11-02T22:27:33Z","author":"imotov","text":"Yes, in the case of a single node, it might not retry to restore the shard to the same node. That's the common recovery\/restore logic and in most cases I think it makes sense. Should we add some sort of detection of retry-able errors that will be processed differently and where it should reside core or s3 plugin - that's a good question. We need to discuss that. \n"},{"date":"2014-11-03T15:23:59Z","author":"imotov","text":"After a brief discussion with @tlrx, we came to the conclusion that it might make sense to address this as part of https:\/\/github.com\/elasticsearch\/elasticsearch-cloud-aws\/issues\/125. We can add retry check in S3 layer with possible common retry logic on the BlobStore level. Reassigning it to @tlrx.\n"},{"date":"2014-11-21T09:55:15Z","author":"tlrx","text":"Commit elasticsearch\/elasticsearch-cloud-aws@ea91adf should help to resolve this issue.\n\nWhen elasticsearch-cloud-aws will be released for 1.4, could you please test this issue again and reopen if needed?\n\nThanks \n"},{"date":"2014-12-29T18:05:52Z","author":"cregev","text":"When will this issue is going to be fixed ?\n"},{"date":"2015-01-02T14:44:27Z","author":"JoeZ99","text":"@tlrx , I've tested with aws plugin 2.4.1 and elasticsearch 1.4.1, and same behavior, so I'll reopen as instructed\n"},{"date":"2015-01-02T14:46:59Z","author":"JoeZ99","text":"@tlrx , Don't know how to reopen this issue.\n"},{"date":"2015-01-02T14:47:44Z","author":"dadoonet","text":"@JoeZ99 reopened.\n"},{"date":"2015-01-05T09:52:19Z","author":"cregev","text":"Is there a workaround ? \n"},{"date":"2015-01-05T11:11:30Z","author":"JoeZ99","text":"The only\"workaround\" for us has been to make the API call with the\n\"wait_for_completion\" parameter, and oh it takes more than 5 minutes, issue\na delete for the indices being restored to unlock the cluster and assume a\ntimeout read error\nOn Jan 5, 2015 10:53 AM, \"Costya Regev\" notifications@github.com wrote:\n\n> Is there a workaround ?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https:\/\/github.com\/elasticsearch\/elasticsearch\/issues\/8280#issuecomment-68687044\n> .\n"},{"date":"2015-04-03T14:47:49Z","author":"tlrx","text":"Follow up on https:\/\/github.com\/elastic\/elasticsearch-cloud-aws\/issues\/149#issuecomment-89309092\n"},{"date":"2015-11-21T20:16:36Z","author":"clintongormley","text":"Closing in favour of https:\/\/github.com\/elastic\/elasticsearch-cloud-aws\/issues\/149#issuecomment-89309092\n"}],"reopen_on":"2015-01-02T14:47:15Z","opened_by":"JoeZ99","closed_on":"2015-11-21T20:16:37Z","description":"During a snapshot restore process for a snapshot which is stored in a S3 repository, chances are that a Read Timeout error occurs when communicating to S3. Don't know whether S3 or Elasticsearch are to blame.\n\nThe issue is that that timeout error should be translated in some kind of error response from elasticsearch, but elasticsearch just keeps waiting for an answer that never comes. In consequence, besides dead points in the application code, the cluster remains in a fake recovery process, preventing it from allowing other recoveries.\n\nthis is the TimeoutError, as seen in elasticsearch logs\n\nthe endpoint is\n\n```\nhttp:\/\/elasticsearch.server.com\/_snapshot\/repository_name\/snapshot_name\/_restore?wait_for_completion=true\n```\n\n```\n\n```\n\n[2014-10-23 16:37:50,005][INFO ][snapshots                ] [Whiplash] snapshot [backups-6:4af7ac9316f168daccf8af5f42b6271e_ts20141023163749251730] is done\n[2014-10-23 16:37:50,218][INFO ][cluster.metadata         ] [Whiplash] [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632] deleting index\n[2014-10-23 16:37:50,236][INFO ][cluster.metadata         ] [Whiplash] [4af7ac9316f168daccf8af5f42b6271e_phonetic_ts20141023163747520269] deleting index\n[2014-10-23 16:44:28,557][WARN ][cluster.metadata         ] [Whiplash] [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632] re-syncing mappings with cluster state for types [[product]]\n[2014-10-23 16:44:28,558][WARN ][cluster.metadata         ] [Whiplash] [4af7ac9316f168daccf8af5f42b6271e_phonetic_ts20141023163747520269] re-syncing mappings with cluster state for types [[product]]\n[2014-10-23 16:46:24,886][WARN ][indices.cluster          ] [Whiplash] [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] failed to start shard\norg.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] failed recovery\n        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:185)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: org.elasticsearch.index.snapshots.IndexShardRestoreFailedException: [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] restore failed\n        at org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.restore(IndexShardSnapshotAndRestoreService.java:130)\n        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:127)\n        ... 3 more\nCaused by: org.elasticsearch.index.snapshots.IndexShardRestoreFailedException: [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] failed to restore snapshot [4af7ac9316f168daccf8af5f42b6271e_ts20141023163749251730]\n        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:158)\n        at org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.restore(IndexShardSnapshotAndRestoreService.java:124)\n        ... 4 more\nCaused by: org.elasticsearch.index.snapshots.IndexShardRestoreFailedException: [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] Failed to recover index\n        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:741)\n        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:155)\n        ... 5 more\nCaused by: java.net.SocketTimeoutException: Read timed out\n        at java.net.SocketInputStream.socketRead0(Native Method)\n        at java.net.SocketInputStream.read(SocketInputStream.java:152)\n        at java.net.SocketInputStream.read(SocketInputStream.java:122)\n        at sun.security.ssl.InputRecord.readFully(InputRecord.java:442)\n        at sun.security.ssl.InputRecord.readV3Record(InputRecord.java:554)\n        at sun.security.ssl.InputRecord.read(InputRecord.java:509)\n        at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:927)\n        at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:884)\n        at sun.security.ssl.AppInputStream.read(AppInputStream.java:102)\n        at org.apache.http.impl.io.AbstractSessionInputBuffer.read(AbstractSessionInputBuffer.java:204)\n        at org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:182)\n        at org.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:138)\n        at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:71)\n        at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:71)\n        at java.security.DigestInputStream.read(DigestInputStream.java:161)\n        at com.amazonaws.services.s3.internal.DigestValidationInputStream.read(DigestValidationInputStream.java:59)\n        at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:71)\n        at java.io.FilterInputStream.read(FilterInputStream.java:107)\n        at org.elasticsearch.cloud.aws.blobstore.AbstractS3BlobContainer$1.run(AbstractS3BlobContainer.java:99)\n        ... 3 more\n[2014-10-23 16:46:24,887][WARN ][cluster.action.shard     ] [Whiplash] [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] sending failed shard for [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0], node[4dZyhkKySw-HpxhLjDYp4A], [P], restoring[backups-6:4af7ac9316f168daccf8af5f42b6271e_ts20141023163749251730], s[INITIALIZING], indexUUID [P6kfnE_UTt6Caae5KAIXHg], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] failed recovery]; nested: IndexShardRestoreFailedException[[4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] restore failed]; nested: IndexShardRestoreFailedException[[4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] failed to restore snapshot [4af7ac9316f168daccf8af5f42b6271e_ts20141023163749251730]]; nested: IndexShardRestoreFailedException[[4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] Failed to recover index]; nested: SocketTimeoutException[Read timed out]; ]]\n[2014-10-23 16:46:24,887][WARN ][cluster.action.shard     ] [Whiplash] [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] received shard failed for [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0], node[4dZyhkKySw-HpxhLjDYp4A], [P], restoring[backups-6:4af7ac9316f168daccf8af5f42b6271e_ts20141023163749251730], s[INITIALIZING], indexUUID [P6kfnE_UTt6Caae5KAIXHg], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] failed recovery]; nested: IndexShardRestoreFailedException[[4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] restore failed]; nested: IndexShardRestoreFailedException[[4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] failed to restore snapshot [4af7ac9316f168daccf8af5f42b6271e_ts20141023163749251730]]; nested: IndexShardRestoreFailedException[[4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] Failed to recover index]; nested: SocketTimeoutException[Read timed out]; ]]\n\n```\n\n```\n","id":"47216263","title":"Read time out errors when recovering a snapshot from S3 repository lets elasticsearch hanging","reopen_by":"dadoonet","opened_on":"2014-10-29T23:20:30Z","closed_by":"clintongormley"},{"number":"8254","comments":[{"date":"2014-10-28T14:16:34Z","author":"clintongormley","text":"This appears to be an artifact of running Elasticsearch in Eclipse.  And the DELETE _all doesn't hang - it times out after 30 sec and returns `{acknowledged: false}`.\n\nClosing\n"},{"date":"2014-10-28T15:04:52Z","author":"uschindler","text":"If there would have been an error like this, forbidden-apis should have catched it :-)\n"},{"date":"2014-10-28T15:07:57Z","author":"clintongormley","text":"@s1monw has a fix for this in the works\n"},{"date":"2014-11-12T10:01:14Z","author":"s1monw","text":"@clintongormley is this still happening?\n"},{"date":"2014-11-12T13:14:59Z","author":"clintongormley","text":"@s1monw This still happens in 1.3 and 1.4, in master it is worse.\n\nThis request no longer logs an error:\n\n```\ncurl -XPOST 'http:\/\/localhost:9200\/test-weird-index-%E4%B8%AD%E6%96%87\/weird.type\/1?pretty=1' -d '\n{\n   \"foo\" : \"bar\"\n}\n'\n```\n\nHowever it creates two separate directories, one for the segments and one for the translogs:\n\n```\ndata\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\n├── 0\n│   ├── index\n│   └── translog\n│       └── translog-1415797929651\n├── 1\n│   ├── index\n│   └── translog\n│       └── translog-1415797929667\n├── 2\n│   ├── index\n│   └── translog\n│       └── translog-1415797929681\n├── 3\n│   ├── index\n│   └── translog\n│       └── translog-1415797929655\n└── 4\n    ├── index\n    └── translog\n        └── translog-1415797929711\ndata\/elasticsearch\/nodes\/0\/indices\/test-weird-index-中�\\226\\207\n├── 0\n│   ├── _state\n│   │   └── state-2.st\n│   └── index\n│       ├── segments_1\n│       └── write.lock\n├── 1\n│   ├── _state\n│   │   └── state-2.st\n│   └── index\n│       ├── segments_1\n│       └── write.lock\n├── 2\n│   ├── _state\n│   │   └── state-2.st\n│   └── index\n│       ├── segments_1\n│       └── write.lock\n├── 3\n│   ├── _state\n│   │   └── state-2.st\n│   └── index\n│       ├── _0.cfe\n│       ├── _0.cfs\n│       ├── _0.si\n│       ├── segments_1\n│       └── write.lock\n├── 4\n│   ├── _state\n│   │   └── state-2.st\n│   └── index\n│       ├── segments_1\n│       └── write.lock\n└── _state\n    ├── state-1.st\n    └── state-2.st\n```\n"},{"date":"2014-12-03T07:51:02Z","author":"javanna","text":"Hye @s1monw this was closed by a commit that went only to master if I'm not mistaken, but it looks like an actual problem in 1.4 and 1.x too (also 1.3). Is there a way to fix this there as well?\n"}],"reopen_on":"2014-10-28T15:07:37Z","opened_by":"clintongormley","closed_on":"2014-12-02T20:52:06Z","description":"The test in https:\/\/github.com\/elasticsearch\/elasticsearch\/blob\/master\/rest-api-spec\/test\/index\/10_with_id.yaml uses Unicode characters in the index name: `test-weird-index-中文`.  After running the test, the test runner tries to clear up the created indices with a `DELETE \/_all`.\n\nAt least on OSX, the name of the index is being changed to `test-weird-index-??`. \n\nIn 1.4, the test succeeds but Elasticsearch logs errors about not being able to delete the index, but DELETE all at least returns.  In 1.x, the DELETE all just hangs.\n\nBoth versions leave the `test-weird-index-??` in place.\n\nRelates to #6736 \n\nLogs from 1.x:\n\n```\n[2014-10-28 14:07:54,695][INFO ][node                     ] [Turbo] version[1.5.0-SNAPSHOT], pid[60278], build[${build\/NA]\n[2014-10-28 14:07:54,696][INFO ][node                     ] [Turbo] initializing ...\n[2014-10-28 14:07:54,701][INFO ][plugins                  ] [Turbo] loaded [], sites []\n[2014-10-28 14:07:57,345][INFO ][node                     ] [Turbo] initialized\n[2014-10-28 14:07:57,345][INFO ][node                     ] [Turbo] starting ...\n[2014-10-28 14:07:57,457][INFO ][transport                ] [Turbo] bound_address {inet[\/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[\/192.168.5.103:9300]}\n[2014-10-28 14:07:57,479][INFO ][discovery                ] [Turbo] elasticsearch\/aprLOBxNRXKldBazmm8VTg\n[2014-10-28 14:08:01,253][INFO ][cluster.service          ] [Turbo] new_master [Turbo][aprLOBxNRXKldBazmm8VTg][Slim.local][inet[\/192.168.5.103:9300]], reason: zen-disco-join (elected_as_master)\n[2014-10-28 14:08:01,270][INFO ][http                     ] [Turbo] bound_address {inet[\/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[\/192.168.5.103:9200]}\n[2014-10-28 14:08:01,270][INFO ][node                     ] [Turbo] started\n[2014-10-28 14:08:01,283][INFO ][gateway                  ] [Turbo] recovered [0] indices into cluster_state\n[2014-10-28 14:10:37,337][INFO ][cluster.metadata         ] [Turbo] [test-weird-index-中文] creating index, cause [auto(index api)], shards [5]\/[1], mappings []\n[2014-10-28 14:10:37,586][WARN ][gateway.local.state.meta ] [Turbo] [test-weird-index-中文]: failed to write index state\njava.nio.file.NoSuchFileException: \/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-中文\/_state\/state-1.st.tmp\n    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)\n    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n    at sun.nio.fs.UnixFileSystemProvider.newFileChannel(UnixFileSystemProvider.java:176)\n    at java.nio.channels.FileChannel.open(FileChannel.java:287)\n    at java.nio.channels.FileChannel.open(FileChannel.java:334)\n    at org.apache.lucene.util.IOUtils.fsync(IOUtils.java:313)\n    at org.elasticsearch.gateway.local.state.meta.MetaDataStateFormat.write(MetaDataStateFormat.java:117)\n    at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.writeIndex(LocalGatewayMetaState.java:371)\n    at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.clusterChanged(LocalGatewayMetaState.java:218)\n    at org.elasticsearch.gateway.local.LocalGateway.clusterChanged(LocalGateway.java:207)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:437)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:722)\n[2014-10-28 14:10:37,624][WARN ][gateway.local.state.meta ] [Turbo] [test-weird-index-中文]: failed to write index state\njava.nio.file.NoSuchFileException: \/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-中文\/_state\/state-1.st.tmp\n    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)\n    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n    at sun.nio.fs.UnixFileSystemProvider.newFileChannel(UnixFileSystemProvider.java:176)\n    at java.nio.channels.FileChannel.open(FileChannel.java:287)\n    at java.nio.channels.FileChannel.open(FileChannel.java:334)\n    at org.apache.lucene.util.IOUtils.fsync(IOUtils.java:313)\n    at org.elasticsearch.gateway.local.state.meta.MetaDataStateFormat.write(MetaDataStateFormat.java:117)\n    at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.writeIndex(LocalGatewayMetaState.java:371)\n    at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.clusterChanged(LocalGatewayMetaState.java:218)\n    at org.elasticsearch.gateway.local.LocalGateway.clusterChanged(LocalGateway.java:207)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:437)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:722)\n[2014-10-28 14:10:37,633][WARN ][gateway.local.state.meta ] [Turbo] [test-weird-index-中文]: failed to write index state\njava.nio.file.NoSuchFileException: \/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-中文\/_state\/state-1.st.tmp\n    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)\n    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n    at sun.nio.fs.UnixFileSystemProvider.newFileChannel(UnixFileSystemProvider.java:176)\n    at java.nio.channels.FileChannel.open(FileChannel.java:287)\n    at java.nio.channels.FileChannel.open(FileChannel.java:334)\n    at org.apache.lucene.util.IOUtils.fsync(IOUtils.java:313)\n    at org.elasticsearch.gateway.local.state.meta.MetaDataStateFormat.write(MetaDataStateFormat.java:117)\n    at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.writeIndex(LocalGatewayMetaState.java:371)\n    at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.clusterChanged(LocalGatewayMetaState.java:218)\n    at org.elasticsearch.gateway.local.LocalGateway.clusterChanged(LocalGateway.java:207)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:437)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:722)\n[2014-10-28 14:10:37,702][WARN ][index.mapper             ] [Turbo] [test-weird-index-中文] Type [weird.type] contains a '.', it is recommended not to include it within a type name\n[2014-10-28 14:10:37,725][WARN ][index.mapper             ] [Turbo] [test-weird-index-中文] Type [weird.type] contains a '.', it is recommended not to include it within a type name\n[2014-10-28 14:10:37,727][INFO ][cluster.metadata         ] [Turbo] [test-weird-index-中文] update_mapping [weird.type] (dynamic)\n[2014-10-28 14:10:37,732][WARN ][gateway.local.state.meta ] [Turbo] [test-weird-index-中文]: failed to write index state\njava.nio.file.NoSuchFileException: \/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-中文\/_state\/state-2.st.tmp\n    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)\n    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n    at sun.nio.fs.UnixFileSystemProvider.newFileChannel(UnixFileSystemProvider.java:176)\n    at java.nio.channels.FileChannel.open(FileChannel.java:287)\n    at java.nio.channels.FileChannel.open(FileChannel.java:334)\n    at org.apache.lucene.util.IOUtils.fsync(IOUtils.java:313)\n    at org.elasticsearch.gateway.local.state.meta.MetaDataStateFormat.write(MetaDataStateFormat.java:117)\n    at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.writeIndex(LocalGatewayMetaState.java:371)\n    at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.clusterChanged(LocalGatewayMetaState.java:218)\n    at org.elasticsearch.gateway.local.LocalGateway.clusterChanged(LocalGateway.java:207)\n    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:437)\n    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:722)\n[2014-10-28 14:10:37,791][INFO ][cluster.metadata         ] [Turbo] [test-weird-index-中文] deleting index\n```\n\nLogs from 1.4:\n\n```\n[2014-10-28 14:12:34,362][INFO ][node                     ] [Burstarr] version[1.4.0-SNAPSHOT], pid[60351], build[${build\/NA]\n[2014-10-28 14:12:34,362][INFO ][node                     ] [Burstarr] initializing ...\n[2014-10-28 14:12:34,367][INFO ][plugins                  ] [Burstarr] loaded [], sites []\n[2014-10-28 14:12:36,930][INFO ][node                     ] [Burstarr] initialized\n[2014-10-28 14:12:36,930][INFO ][node                     ] [Burstarr] starting ...\n[2014-10-28 14:12:37,065][INFO ][transport                ] [Burstarr] bound_address {inet[\/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[\/192.168.5.103:9300]}\n[2014-10-28 14:12:37,080][INFO ][discovery                ] [Burstarr] elasticsearch\/M_KcoO3qQCOBp9VuHZ31AA\n[2014-10-28 14:12:40,854][INFO ][cluster.service          ] [Burstarr] new_master [Burstarr][M_KcoO3qQCOBp9VuHZ31AA][Slim.local][inet[\/192.168.5.103:9300]], reason: zen-disco-join (elected_as_master)\n[2014-10-28 14:12:40,869][INFO ][http                     ] [Burstarr] bound_address {inet[\/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[\/192.168.5.103:9200]}\n[2014-10-28 14:12:40,869][INFO ][node                     ] [Burstarr] started\n[2014-10-28 14:12:40,883][INFO ][gateway                  ] [Burstarr] recovered [0] indices into cluster_state\n[2014-10-28 14:12:50,301][INFO ][cluster.metadata         ] [Burstarr] [test-weird-index-中文] creating index, cause [auto(index api)], shards [5]\/[1], mappings []\n[2014-10-28 14:12:50,580][INFO ][gateway.local.state.meta ] [Burstarr] [test-weird-index-??] dangling index, exists on local file system, but not in cluster metadata, scheduling to delete in [2h], auto import to cluster state [YES]\n[2014-10-28 14:12:50,581][INFO ][gateway.local.state.meta ] [Burstarr] dangled index directory name is [test-weird-index-??], state name is [test-weird-index-中文], renaming to directory name\n[2014-10-28 14:12:50,611][INFO ][gateway.local.state.meta ] [Burstarr] dangled index directory name is [test-weird-index-??], state name is [test-weird-index-中文], renaming to directory name\n[2014-10-28 14:12:50,617][INFO ][gateway.local.state.meta ] [Burstarr] dangled index directory name is [test-weird-index-??], state name is [test-weird-index-中文], renaming to directory name\n[2014-10-28 14:12:50,624][INFO ][gateway.local.state.meta ] [Burstarr] auto importing dangled indices [test-weird-index-??\/OPEN] from [[Burstarr][M_KcoO3qQCOBp9VuHZ31AA][Slim.local][inet[\/192.168.5.103:9300]]]\n[2014-10-28 14:12:50,682][WARN ][index.mapper             ] [Burstarr] [test-weird-index-中文] Type [weird.type] contains a '.', it is recommended not to include it within a type name\n[2014-10-28 14:12:50,709][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-??][2] shard is locked, releasing lock\n[2014-10-28 14:12:50,733][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-??][3] shard is locked, releasing lock\n[2014-10-28 14:12:50,760][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-??][1] shard is locked, releasing lock\n[2014-10-28 14:12:50,782][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-??][4] shard is locked, releasing lock\n[2014-10-28 14:12:50,828][WARN ][index.mapper             ] [Burstarr] [test-weird-index-中文] Type [weird.type] contains a '.', it is recommended not to include it within a type name\n[2014-10-28 14:12:50,831][INFO ][cluster.metadata         ] [Burstarr] [test-weird-index-中文] update_mapping [weird.type] (dynamic)\n[2014-10-28 14:12:50,837][INFO ][cluster.metadata         ] [Burstarr] [test-weird-index-??] deleting index\n[2014-10-28 14:12:55,730][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-??][2] Could not lock IndexWriter isLocked [true]\norg.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/2\/index\/write.lock\n    at org.apache.lucene.store.Lock.obtain(Lock.java:89)\n    at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:753)\n    at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1448)\n    at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:273)\n    at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:732)\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:231)\n    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:722)\n[2014-10-28 14:12:55,732][WARN ][indices.cluster          ] [Burstarr] [test-weird-index-??][2] failed to start shard\norg.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][2] failed recovery\n    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:185)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:722)\nCaused by: org.elasticsearch.index.engine.EngineCreationFailureException: [test-weird-index-??][2] failed to create engine\n    at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:288)\n    at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:732)\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:231)\n    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)\n    ... 3 more\nCaused by: org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/2\/index\/write.lock\n    at org.apache.lucene.store.Lock.obtain(Lock.java:89)\n    at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:753)\n    at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1448)\n    at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:273)\n    ... 6 more\n[2014-10-28 14:12:55,749][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-??][3] Could not lock IndexWriter isLocked [true]\norg.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/3\/index\/write.lock\n    at org.apache.lucene.store.Lock.obtain(Lock.java:89)\n    at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:753)\n    at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1448)\n    at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:273)\n    at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:732)\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:231)\n    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:722)\n[2014-10-28 14:12:55,750][WARN ][indices.cluster          ] [Burstarr] [test-weird-index-??][3] failed to start shard\norg.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][3] failed recovery\n    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:185)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:722)\nCaused by: org.elasticsearch.index.engine.EngineCreationFailureException: [test-weird-index-??][3] failed to create engine\n    at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:288)\n    at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:732)\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:231)\n    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)\n    ... 3 more\nCaused by: org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/3\/index\/write.lock\n    at org.apache.lucene.store.Lock.obtain(Lock.java:89)\n    at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:753)\n    at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1448)\n    at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:273)\n    ... 6 more\n[2014-10-28 14:12:55,778][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-??][1] Could not lock IndexWriter isLocked [true]\norg.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/1\/index\/write.lock\n    at org.apache.lucene.store.Lock.obtain(Lock.java:89)\n    at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:753)\n    at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1448)\n    at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:273)\n    at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:732)\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:231)\n    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:722)\n[2014-10-28 14:12:55,795][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-??][4] Could not lock IndexWriter isLocked [true]\norg.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/4\/index\/write.lock\n    at org.apache.lucene.store.Lock.obtain(Lock.java:89)\n    at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:753)\n    at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1448)\n    at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:273)\n    at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:732)\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:231)\n    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:722)\n[2014-10-28 14:12:55,800][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][3] sending failed shard for [test-weird-index-??][3], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][3] failed recovery]; nested: EngineCreationFailureException[[test-weird-index-??][3] failed to create engine]; nested: LockObtainFailedException[Lock obtain timed out: NativeFSLock@\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/3\/index\/write.lock]; ]]\n[2014-10-28 14:12:55,800][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][3] received shard failed for [test-weird-index-??][3], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][3] failed recovery]; nested: EngineCreationFailureException[[test-weird-index-??][3] failed to create engine]; nested: LockObtainFailedException[Lock obtain timed out: NativeFSLock@\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/3\/index\/write.lock]; ]]\n[2014-10-28 14:12:55,800][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][2] sending failed shard for [test-weird-index-??][2], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][2] failed recovery]; nested: EngineCreationFailureException[[test-weird-index-??][2] failed to create engine]; nested: LockObtainFailedException[Lock obtain timed out: NativeFSLock@\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/2\/index\/write.lock]; ]]\n[2014-10-28 14:12:55,801][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][2] received shard failed for [test-weird-index-??][2], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][2] failed recovery]; nested: EngineCreationFailureException[[test-weird-index-??][2] failed to create engine]; nested: LockObtainFailedException[Lock obtain timed out: NativeFSLock@\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/2\/index\/write.lock]; ]]\n[2014-10-28 14:12:55,806][INFO ][cluster.metadata         ] [Burstarr] [test-weird-index-中文] deleting index\n[2014-10-28 14:12:55,808][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-中文][0] failed to rollback writer on close\norg.apache.lucene.store.NoSuchDirectoryException: directory '\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/0\/index' does not exist\n    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:218)\n    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:242)\n    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)\n    at org.elasticsearch.index.store.DistributorDirectory.listAll(DistributorDirectory.java:88)\n    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)\n    at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:424)\n    at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:460)\n    at org.apache.lucene.index.IndexWriter.rollbackInternal(IndexWriter.java:2134)\n    at org.apache.lucene.index.IndexWriter.rollback(IndexWriter.java:2074)\n    at org.elasticsearch.index.engine.internal.InternalEngine.close(InternalEngine.java:1295)\n    at org.elasticsearch.index.service.InternalIndexService.removeShard(InternalIndexService.java:419)\n    at org.elasticsearch.index.service.InternalIndexService$1.run(InternalIndexService.java:279)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:722)\n[2014-10-28 14:12:55,813][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-中文][2] failed to rollback writer on close\norg.apache.lucene.store.NoSuchDirectoryException: directory '\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/2\/index' does not exist\n    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:218)\n    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:242)\n    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)\n    at org.elasticsearch.index.store.DistributorDirectory.listAll(DistributorDirectory.java:88)\n    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)\n    at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:424)\n    at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:460)\n    at org.apache.lucene.index.IndexWriter.rollbackInternal(IndexWriter.java:2134)\n    at org.apache.lucene.index.IndexWriter.rollback(IndexWriter.java:2074)\n    at org.elasticsearch.index.engine.internal.InternalEngine.close(InternalEngine.java:1295)\n    at org.elasticsearch.index.service.InternalIndexService.removeShard(InternalIndexService.java:419)\n    at org.elasticsearch.index.service.InternalIndexService$1.run(InternalIndexService.java:279)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:722)\n[2014-10-28 14:12:55,815][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-中文][1] failed to rollback writer on close\norg.apache.lucene.store.NoSuchDirectoryException: directory '\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/1\/index' does not exist\n    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:218)\n    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:242)\n    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)\n    at org.elasticsearch.index.store.DistributorDirectory.listAll(DistributorDirectory.java:88)\n    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)\n    at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:424)\n    at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:460)\n    at org.apache.lucene.index.IndexWriter.rollbackInternal(IndexWriter.java:2134)\n    at org.apache.lucene.index.IndexWriter.rollback(IndexWriter.java:2074)\n    at org.elasticsearch.index.engine.internal.InternalEngine.close(InternalEngine.java:1295)\n    at org.elasticsearch.index.service.InternalIndexService.removeShard(InternalIndexService.java:419)\n    at org.elasticsearch.index.service.InternalIndexService$1.run(InternalIndexService.java:279)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:722)\n[2014-10-28 14:12:55,817][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-中文][4] failed to rollback writer on close\norg.apache.lucene.store.NoSuchDirectoryException: directory '\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/4\/index' does not exist\n    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:218)\n    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:242)\n    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)\n    at org.elasticsearch.index.store.DistributorDirectory.listAll(DistributorDirectory.java:88)\n    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)\n    at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:424)\n    at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:460)\n    at org.apache.lucene.index.IndexWriter.rollbackInternal(IndexWriter.java:2134)\n    at org.apache.lucene.index.IndexWriter.rollback(IndexWriter.java:2074)\n    at org.elasticsearch.index.engine.internal.InternalEngine.close(InternalEngine.java:1295)\n    at org.elasticsearch.index.service.InternalIndexService.removeShard(InternalIndexService.java:419)\n    at org.elasticsearch.index.service.InternalIndexService$1.run(InternalIndexService.java:279)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:722)\n[2014-10-28 14:12:55,819][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-中文][3] failed to rollback writer on close\norg.apache.lucene.store.NoSuchDirectoryException: directory '\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/3\/index' does not exist\n    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:218)\n    at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:242)\n    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)\n    at org.elasticsearch.index.store.DistributorDirectory.listAll(DistributorDirectory.java:88)\n    at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)\n    at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:424)\n    at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:460)\n    at org.apache.lucene.index.IndexWriter.rollbackInternal(IndexWriter.java:2134)\n    at org.apache.lucene.index.IndexWriter.rollback(IndexWriter.java:2074)\n    at org.elasticsearch.index.engine.internal.InternalEngine.close(InternalEngine.java:1295)\n    at org.elasticsearch.index.service.InternalIndexService.removeShard(InternalIndexService.java:419)\n    at org.elasticsearch.index.service.InternalIndexService$1.run(InternalIndexService.java:279)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:722)\n[2014-10-28 14:12:55,822][INFO ][gateway.local.state.meta ] [Burstarr] auto importing dangled indices [test-weird-index-??\/OPEN] from [[Burstarr][M_KcoO3qQCOBp9VuHZ31AA][Slim.local][inet[\/192.168.5.103:9300]]]\n[2014-10-28 14:12:55,881][WARN ][indices.cluster          ] [Burstarr] [test-weird-index-??][2] failed to start shard\norg.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][2] failed to fetch index version after copying it over\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:158)\n    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:722)\nCaused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][2] shard allocated for local recovery (post api), should exist, but doesn't, current files: []\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:132)\n    ... 4 more\nCaused by: org.apache.lucene.index.IndexNotFoundException: no segments* file found in store(least_used[rate_limited(niofs(\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/2\/index), type=MERGE, rate=20.0)]): files: []\n    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:864)\n    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:752)\n    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:450)\n    at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:85)\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:123)\n    ... 4 more\n[2014-10-28 14:12:55,900][WARN ][indices.cluster          ] [Burstarr] [test-weird-index-??][0] failed to start shard\norg.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][0] failed to fetch index version after copying it over\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:158)\n    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:722)\nCaused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][0] shard allocated for local recovery (post api), should exist, but doesn't, current files: []\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:132)\n    ... 4 more\nCaused by: org.apache.lucene.index.IndexNotFoundException: no segments* file found in store(least_used[rate_limited(niofs(\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/0\/index), type=MERGE, rate=20.0)]): files: []\n    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:864)\n    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:752)\n    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:450)\n    at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:85)\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:123)\n    ... 4 more\n[2014-10-28 14:12:55,913][WARN ][indices.cluster          ] [Burstarr] [test-weird-index-??][3] failed to start shard\norg.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][3] failed to fetch index version after copying it over\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:158)\n    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:722)\nCaused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][3] shard allocated for local recovery (post api), should exist, but doesn't, current files: []\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:132)\n    ... 4 more\nCaused by: org.apache.lucene.index.IndexNotFoundException: no segments* file found in store(least_used[rate_limited(niofs(\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/3\/index), type=MERGE, rate=20.0)]): files: []\n    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:864)\n    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:752)\n    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:450)\n    at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:85)\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:123)\n    ... 4 more\n[2014-10-28 14:12:55,929][WARN ][indices.cluster          ] [Burstarr] [test-weird-index-??][4] failed to start shard\norg.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][4] failed to fetch index version after copying it over\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:158)\n    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:722)\nCaused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][4] shard allocated for local recovery (post api), should exist, but doesn't, current files: []\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:132)\n    ... 4 more\nCaused by: org.apache.lucene.index.IndexNotFoundException: no segments* file found in store(least_used[rate_limited(niofs(\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/4\/index), type=MERGE, rate=20.0)]): files: []\n    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:864)\n    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:752)\n    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:450)\n    at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:85)\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:123)\n    ... 4 more\n[2014-10-28 14:12:55,929][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][3] sending failed shard for [test-weird-index-??][3], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][3] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][3] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/3\/index), type=MERGE, rate=20.0)]): files: []]; ]]\n[2014-10-28 14:12:55,930][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][3] received shard failed for [test-weird-index-??][3], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][3] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][3] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/3\/index), type=MERGE, rate=20.0)]): files: []]; ]]\n[2014-10-28 14:12:55,930][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][0] sending failed shard for [test-weird-index-??][0], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][0] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][0] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/0\/index), type=MERGE, rate=20.0)]): files: []]; ]]\n[2014-10-28 14:12:55,931][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][0] received shard failed for [test-weird-index-??][0], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][0] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][0] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/0\/index), type=MERGE, rate=20.0)]): files: []]; ]]\n[2014-10-28 14:12:55,931][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][2] sending failed shard for [test-weird-index-??][2], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][2] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][2] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/2\/index), type=MERGE, rate=20.0)]): files: []]; ]]\n[2014-10-28 14:12:55,931][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][2] received shard failed for [test-weird-index-??][2], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][2] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][2] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/2\/index), type=MERGE, rate=20.0)]): files: []]; ]]\n[2014-10-28 14:12:55,932][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][4] sending failed shard for [test-weird-index-??][4], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][4] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][4] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/4\/index), type=MERGE, rate=20.0)]): files: []]; ]]\n[2014-10-28 14:12:55,932][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][4] received shard failed for [test-weird-index-??][4], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][4] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][4] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/4\/index), type=MERGE, rate=20.0)]): files: []]; ]]\n[2014-10-28 14:12:55,934][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][4] received shard failed for [test-weird-index-??][4], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [master [Burstarr][M_KcoO3qQCOBp9VuHZ31AA][Slim.local][inet[\/192.168.5.103:9300]] marked shard as initializing, but shard is marked as failed, resend shard failure]\n[2014-10-28 14:12:55,951][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][2] received shard failed for [test-weird-index-??][2], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [master [Burstarr][M_KcoO3qQCOBp9VuHZ31AA][Slim.local][inet[\/192.168.5.103:9300]] marked shard as initializing, but shard is marked as failed, resend shard failure]\n[2014-10-28 14:12:55,953][WARN ][indices.cluster          ] [Burstarr] [test-weird-index-??][1] failed to start shard\norg.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][1] failed to fetch index version after copying it over\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:158)\n    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:722)\nCaused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][1] shard allocated for local recovery (post api), should exist, but doesn't, current files: []\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:132)\n    ... 4 more\nCaused by: org.apache.lucene.index.IndexNotFoundException: no segments* file found in store(least_used[rate_limited(niofs(\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/1\/index), type=MERGE, rate=20.0)]): files: []\n    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:864)\n    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:752)\n    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:450)\n    at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:85)\n    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:123)\n    ... 4 more\n[2014-10-28 14:12:55,958][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][1] sending failed shard for [test-weird-index-??][1], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][1] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][1] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/1\/index), type=MERGE, rate=20.0)]): files: []]; ]]\n[2014-10-28 14:12:55,958][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][1] received shard failed for [test-weird-index-??][1], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][1] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][1] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(\/Users\/clinton\/workspace\/elasticsearch\/data\/elasticsearch\/nodes\/0\/indices\/test-weird-index-??\/1\/index), type=MERGE, rate=20.0)]): files: []]; ]]\n```\n","id":"47024428","title":"Unicode index names causing DELETE index to hang","reopen_by":"clintongormley","opened_on":"2014-10-28T13:18:25Z","closed_by":"s1monw"},{"number":"8102","comments":[{"date":"2014-10-16T18:19:02Z","author":"clintongormley","text":"@webmstr Snapshots are still moment in time while updates are happening.  You don't need to lock anything.  A snapshot will only backup the state of the index at the point that the backup starts, it won't take any later changes into account.\n"},{"date":"2014-10-16T20:09:17Z","author":"webmstr","text":"As I mentioned, snapshots - as currently implemented - are an unreasonable method of performing a consistent backup prior to an upgrade.  This enhancement would have allowed that option.\n\nWithout the enhancement, snapshots should not be used before an upgrade, because the indexes may have been changed while the snapshot was running.  As such, the upgrade documentation should be changed to not propose the use of snapshots as backups, and a \"full\" backup procedure should be documented in its place.\n"},{"date":"2014-10-17T05:04:47Z","author":"clintongormley","text":"Out of interest, why don't you just stop writing to your cluster?  Reopening for discussion.\n"},{"date":"2014-10-17T05:05:03Z","author":"clintongormley","text":"@imotov what are your thoughts?\n"},{"date":"2014-10-17T06:18:42Z","author":"webmstr","text":"I could turn off logstash, but that's just one potential client.  Someone could be curl'ing, or using an ES plugin (like head), etc.  If you need a consistent backup, you have to disconnect and lock out the clients from the server side.\n"},{"date":"2014-10-17T13:33:58Z","author":"imotov","text":"@clintongormley see https:\/\/github.com\/elasticsearch\/elasticsearch\/pull\/5876 I think this one is similar. \n"},{"date":"2014-10-17T13:39:15Z","author":"clintongormley","text":"@imotov thanks, so setting `index.blocks.write` to `true` on all indices would be a reasonable workaround, at least until #5855 is resolved.\n"},{"date":"2014-11-12T23:26:35Z","author":"saahn","text":"@clintongormley Actually, I discovered that the `index.blocks.write` attribute only prevents writes to **existing** indices. If a client tries to create a new index, that request succeeds, which brings us back to the same problem. My workaround was to shutdown the proxy node though which our clients access our ES cluster.\nI am running into the same issue as @webmstr , but for different reason: I cannot create a consistent backup for a restore to a secondary datacenter because each snapshot takes ~1 hour to complete and we cannot afford to block writes from our clients for such a long period of time. \nI am still trying to root cause why snapshots are taking so long; the time required for snapshot completion increases with each snapshot. However, when i restore the same data to a new cluster, snapshotting that data to a new S3 bucket takes less than a minute. \n\nEDIT: I may have a theory on why the snapshots were taking so long... i was taking a snapshot every two hours, and the s3 bucket has a LOT of snapshots now (49). I'm thinking that the calls the ES aws plugin makes to the S3 endpoint slow down over time as the number of snapshots increase. \n\nOr may be it's just the number of snapshots that's causing the slowness...i.e. regardless of whether the backend repository is S3 or fs? I guess I should have an additional cron job that deletes older snaphots. Is there a good rule of thumb on the number of snapshots to retain?\n"},{"date":"2015-02-20T10:37:12Z","author":"colings86","text":"@imotov we discussed this issue but were unclear on what the differences are between the index.blocks.\\* options are and why the snapshot fails with read_only set to false?\n"},{"date":"2015-02-20T15:54:26Z","author":"imotov","text":"@colings86 there is an ongoing effort to resolve this issue in #9203\n"},{"date":"2015-05-19T16:35:32Z","author":"imotov","text":"After discussing this with @tlrx it looks like the best way to address this issue is by moving snapshot and restore cluster state elements from cluster metadata to a custom cluster element where it seems to belong (since information about currently running snapshot and restore hardly qualifies as metadata).\n"}],"reopen_on":"2014-10-17T05:04:47Z","opened_by":"webmstr","closed_on":"2015-06-11T19:35:51Z","description":"I was trying to make a full, consistent backup before an upgrade.  Snapshots are at a moment of time, which doesn't work if clients are still updating your indexes.\n\nI tried putting the cluster into read_only mode by setting cluster.blocks.read_only: true, but running a snapshot returned this error:\n\n```\n{\"error\":\"ClusterBlockException[blocked by: [FORBIDDEN\/6\/cluster read-only (api)];]\",\"status\":403}\n```\n\nPlease consider allowing snapshots to provide a consistent backup by running when in read-only mode.\n","id":"45917528","title":"snapshot should work when cluster is in read_only mode.","reopen_by":"clintongormley","opened_on":"2014-10-15T20:48:17Z","closed_by":"imotov"},{"number":"8078","comments":[{"date":"2014-10-14T23:30:24Z","author":"rjernst","text":"This was fixed in #7468 but was not backported to 1.3.\n"},{"date":"2014-10-15T07:46:08Z","author":"s1monw","text":"@rjernst should we backport to 1.3 - it's a bugfix from a sparc user perspective?\n"},{"date":"2014-10-16T12:39:06Z","author":"clintongormley","text":"Agreed \n"},{"date":"2014-10-16T13:39:14Z","author":"s1monw","text":"I backported this to `1.3.5`\n"}],"reopen_on":"2014-10-16T12:38:53Z","opened_by":"decuypeb","closed_on":"2014-10-16T13:39:14Z","description":"The changes for Issue 6962 is present in 1.3.2, but there are still uses of Unsafe methods in other classes, apart from UnsafeUtils.\n\njprante signalled one occurrence on the site below:\nhttp:\/\/www.snip2code.com\/Snippet\/140415\/Solaris-SPARC-JVM-64bit-crash-with-Java-\n\nbut I could not find a reference to it here at ElasticSearch.\n\nThese classes are involved.\n\nUnsafeChunkDecoder.class\nUnsafeChunkEncoder.class\nUnsafeChunkEncoderBE.class\nUnsafeChunkEncoderLE.class\nUnsafeChunkEncoders.class\nUnsafeDynamicChannelBuffer.class\n","id":"45737013","title":"Still use of unsafe methods in 1.3.4 - causing crashes on SPARC","reopen_by":"clintongormley","opened_on":"2014-10-14T10:59:17Z","closed_by":"s1monw"},{"number":"7926","comments":[{"date":"2014-10-14T12:21:10Z","author":"clintongormley","text":"Hi @lnxg33k \n\nJust to note: you shouldn't use such big sizes. The whole point of scrolling is that you can keep pulling smaller batches of results until you have enough.\n\nThat said, an NPE is always a bug.  I've tried replicating this with two shards and 300,000 documents, but it is working fine for me.\n\nCould you provide the stack trace from the logs so that we can investigate further?\n\nthanks\n"},{"date":"2014-10-23T10:01:13Z","author":"clintongormley","text":"Hi @lnxg33k \n\nAny chance of getting the stack trace please?\n"},{"date":"2014-10-28T08:21:22Z","author":"lnxg33k","text":"@clintongormley I am sorry but I couldn't reporoduce it anymore and don't have the stack trace. \n"},{"date":"2014-10-28T10:28:54Z","author":"clintongormley","text":"OK, thanks @lnxg33k \n\nI'll close this issue as we have been unable to replicate, but please feel free to reopen if you see it happen again.\n"},{"date":"2015-06-30T18:55:34Z","author":"l15k4","text":"There is a stack trace with `ArrayIndexOutOfBounds` that occurs when scrolling, it started after upgrade to 1.6 : \n\n```\norg.elasticsearch.transport.RemoteTransportException: [Book][inet[\/172.31.13.26:9300]][indices:data\/read\/scroll]\nCaused by: org.elasticsearch.action.search.ReduceSearchPhaseException: Failed to execute phase [fetch], [reduce] \n    at org.elasticsearch.action.search.type.TransportSearchScrollScanAction$AsyncAction.finishHim(TransportSearchScrollScanAction.java:190) ~[gwiq.jar:0.6-SNAPSHOT]\n    at org.elasticsearch.action.search.type.TransportSearchScrollScanAction$AsyncAction.access$800(TransportSearchScrollScanAction.java:71) ~[gwiq.jar:0.6-SNAPSHOT]\n    at org.elasticsearch.action.search.type.TransportSearchScrollScanAction$AsyncAction$1.onResult(TransportSearchScrollScanAction.java:164) ~[gwiq.jar:0.6-SNAPSHOT]\n    at org.elasticsearch.action.search.type.TransportSearchScrollScanAction$AsyncAction$1.onResult(TransportSearchScrollScanAction.java:159) ~[gwiq.jar:0.6-SNAPSHOT]\n    at org.elasticsearch.search.action.SearchServiceTransportAction$22.handleResponse(SearchServiceTransportAction.java:533) ~[gwiq.jar:0.6-SNAPSHOT]\n    at org.elasticsearch.search.action.SearchServiceTransportAction$22.handleResponse(SearchServiceTransportAction.java:524) ~[gwiq.jar:0.6-SNAPSHOT]\n    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:163) ~[gwiq.jar:0.6-SNAPSHOT]\n    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:132) ~[gwiq.jar:0.6-SNAPSHOT]\n    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) ~[gwiq.jar:0.6-SNAPSHOT]\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[gwiq.jar:0.6-SNAPSHOT]\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) ~[gwiq.jar:0.6-SNAPSHOT]\n    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296) ~[gwiq.jar:0.6-SNAPSHOT]\n    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462) ~[gwiq.jar:0.6-SNAPSHOT]\n    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443) ~[gwiq.jar:0.6-SNAPSHOT]\n    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303) ~[gwiq.jar:0.6-SNAPSHOT]\n    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) ~[gwiq.jar:0.6-SNAPSHOT]\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[gwiq.jar:0.6-SNAPSHOT]\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559) ~[gwiq.jar:0.6-SNAPSHOT]\n    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268) ~[gwiq.jar:0.6-SNAPSHOT]\n    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255) ~[gwiq.jar:0.6-SNAPSHOT]\n    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88) ~[gwiq.jar:0.6-SNAPSHOT]\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108) ~[gwiq.jar:0.6-SNAPSHOT]\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337) ~[gwiq.jar:0.6-SNAPSHOT]\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) ~[gwiq.jar:0.6-SNAPSHOT]\n    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) ~[gwiq.jar:0.6-SNAPSHOT]\n    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) ~[gwiq.jar:0.6-SNAPSHOT]\n    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) ~[gwiq.jar:0.6-SNAPSHOT]\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_75]\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_75]\n    at java.lang.Thread.run(Thread.java:745) ~[na:1.7.0_75]\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 70\n    at org.elasticsearch.action.search.type.TransportSearchScrollScanAction$AsyncAction.innerFinishHim(TransportSearchScrollScanAction.java:209) ~[gwiq.jar:0.6-SNAPSHOT]\n    at org.elasticsearch.action.search.type.TransportSearchScrollScanAction$AsyncAction.finishHim(TransportSearchScrollScanAction.java:188) ~[gwiq.jar:0.6-SNAPSHOT]\n    ... 29 common frames omitted\n```\n"},{"date":"2015-07-01T08:38:39Z","author":"l15k4","text":"I'm doing a lot of scrollings  (with sliding time constraint over the same indices) in a for loop. I'll try to clear scroll context after each iteration to see if it helps...\n"},{"date":"2015-07-01T08:39:24Z","author":"clintongormley","text":"@l15k4 also, are you using `scan`? and do you have any shard exceptions while scrolling?\n"},{"date":"2015-07-01T08:44:37Z","author":"l15k4","text":"@clintongormley Yes I'm doing `scan` with `range` over 110 indices, `page-size=70`, tried `keepAlive=10s-30s`. When I didn't get `ArrayIndexOutOfBoundsException` I got shardFailures, like ~ 10-50 of the same identical failures : \n\n```\nfailure.index() == null\nfailure.shardId == -1`\nfailure.reason == NodeDisconnectedException\n```\n"},{"date":"2015-07-01T09:21:04Z","author":"clintongormley","text":"Hi @l15k4 \n\nThanks for the info.  I've asked @martijnvg to have a look at it when he has a moment.  Any more info that you can provide to help us track it down would be useful.  also, why so many node disconnected exceptions?  that seems weird.  Do you see any exceptions on those nodes?\n"},{"date":"2015-07-01T09:28:38Z","author":"l15k4","text":"Imho it was all caused by leaving too many \"15s\" scroll contexts alive because I wasn't clearing them and I was performing 8760 tiny scans in for loop (sequentially) ... After I deployed the application with `clearing-scroll-context-feature` it works like a charm...\n\nSorry but those logs were temporary, they are gone with the old docker container...\n"},{"date":"2015-07-01T14:20:42Z","author":"martijnvg","text":"@l15k4 Did the errors occur while there were nodes of mixed versions in the cluster? Or were all nodes on the same version?\n"},{"date":"2015-07-01T14:25:32Z","author":"l15k4","text":"@martijnvg At the time of the error being thrown all 4 nodes were `1.6.0` but a week ago we managed to run cluster [1.6.0, 1.6.0, 1.6.0, 1.5.1] for 3 hours before we noticed it was having `yellow` status indefinitely... Could it affect future well being of the cluster? \n"},{"date":"2015-07-01T16:16:57Z","author":"martijnvg","text":"@l15k4 no, but I don't recommend to do this is for a long period of time. Not sure what the cause of the exception was here, but I think the code where the exception occurs can be written in such a way that an `ArrayIndexOutOfBoundsException` can never occur.\n"},{"date":"2015-08-24T14:48:33Z","author":"jpountz","text":"Fixed via #11978\n"}],"reopen_on":"2015-07-01T08:31:50Z","opened_by":"lnxg33k","closed_on":"2015-08-24T14:48:33Z","description":"I am using the latest version of elasticsearch and I got this error when I use scroll with large number size and scan as a search type\n\n{\"error\":\"ArrayIndexOutOfBoundsException[-131072]\",\"status\":500}\n\nthous it perfectly works woth small sizes\n\nex. \n\n``` bash\n[01:21:39] lnxg33k@ruined-sec ➜ ~: curl -XGET \"http:\/\/localhost:9200\/dns_logs\/pico\/_search?search_type=scan&scroll=1m\" -d \"{\n                                   \"query\": { \"match_all\": {}},\n                                   \"size\":  100000\n                                   }\"\n{\"_scroll_id\":\"c2Nhbjs1OzUxOjhmSjY4NkFVVG55WmxsT3RZcXgyamc7NTM6OGZKNjg2QVVUbnlabGxPdFlxeDJqZzs1Mjo4Zko2ODZBVVRueVpsbE90WXF4MmpnOzU0OjhmSjY4NkFVVG55WmxsT3RZcXgyamc7NTU6OGZKNjg2QVVUbnlabGxPdFlxeDJqZzsxO3RvdGFsX2hpdHM6NTIwNzY2ODg7\",\"took\":132,\"timed_out\":false,\"_shards\":{\"total\":5,\"successful\":5,\"failed\":0},\"hits\":{\"total\":52076688,\"max_score\":0.0,\"hits\":[]}}⏎                                                                                                                     [01:21:50] lnxg33k@ruined-sec ➜ ~: curl -XGET \"http:\/\/localhost:9200\/_search\/scroll?scroll=1m&scroll_id=c2Nhbjs1OzUxOjhmSjY4NkFVVG55WmxsT3RZcXgyamc7NTM6OGZKNjg2QVVUbnlabGxPdFlxeDJqZzs1Mjo4Zko2ODZBVVRueVpsbE90WXF4MmpnOzU0OjhmSjY4NkFVVG55WmxsT3RZcXgyamc7NTU6OGZKNjg2QVVUbnlabGxPdFlxeDJqZzsxO3RvdGFsX2hpdHM6NTIwNzY2ODg7\" > xxx.json\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  262M  100  262M    0     0   129M      0  0:00:02  0:00:02 --:--:--  129M\n[01:22:03] lnxg33k@ruined-sec ➜ ~: du -sh xxx.json \n263M    xxx.json\n```\n\n``` bash\n[01:22:07] lnxg33k@ruined-sec ➜ ~: curl -XGET \"http:\/\/localhost:9200\/dns_logs\/pico\/_search?search_type=scan&scroll=1m\" -d \"{\n                                   \"query\": { \"match_all\": {}},\n                                   \"size\":  1000000\n                                   }\"\n{\"_scroll_id\":\"c2Nhbjs1OzU2OjhmSjY4NkFVVG55WmxsT3RZcXgyamc7NTc6OGZKNjg2QVVUbnlabGxPdFlxeDJqZzs1ODo4Zko2ODZBVVRueVpsbE90WXF4MmpnOzU5OjhmSjY4NkFVVG55WmxsT3RZcXgyamc7NjA6OGZKNjg2QVVUbnlabGxPdFlxeDJqZzsxO3RvdGFsX2hpdHM6NTIwNzY2ODg7\",\"took\":128,\"timed_out\":false,\"_shards\":{\"total\":5,\"successful\":5,\"failed\":0},\"hits\":{\"total\":52076688,\"max_score\":0.0,\"hits\":[]}}⏎                                                                                                                     [01:22:38] lnxg33k@ruined-sec ➜ ~: curl -XGET \"http:\/\/localhost:9200\/_search\/scroll?scroll=1m&scroll_id=c2Nhbjs1OzU2OjhmSjY4NkFVVG55WmxsT3RZcXgyamc7NTc6OGZKNjg2QVVUbnlabGxPdFlxeDJqZzs1ODo4Zko2ODZBVVRueVpsbE90WXF4MmpnOzU5OjhmSjY4NkFVVG55WmxsT3RZcXgyamc7NjA6OGZKNjg2QVVUbnlabGxPdFlxeDJqZzsxO3RvdGFsX2hpdHM6NTIwNzY2ODg7\"\n{\"error\":\"ArrayIndexOutOfBoundsException[null]\",\"status\":500}⏎                                                           \n```\n","id":"44421817","title":"ArrayIndexOutOfBoundsException","reopen_by":"clintongormley","opened_on":"2014-09-30T10:24:49Z","closed_by":"jpountz"},{"number":"7902","comments":[{"date":"2014-10-10T13:51:12Z","author":"colings86","text":"@nwarz Thanks for the PR. I made a left a small comment but otherwise it's looking good\n"},{"date":"2014-11-14T10:07:57Z","author":"colings86","text":"pushed to master in https:\/\/github.com\/elasticsearch\/elasticsearch\/commit\/e77f9720d2fb74afb5ce82b3ad7d75d21a00e74f\n\npushed to 1.x in https:\/\/github.com\/elasticsearch\/elasticsearch\/commit\/6e98bd73b43079847f0143c6ddf77b26ff441f55\n"}],"reopen_on":"2014-11-14T10:18:14Z","opened_by":"nwarz","closed_on":"2014-11-14T10:19:55Z","description":"Closes #2716\n\nWhen merging two mappings, default index analyzers are represented by either null or by a \"default\"-named index analyzer object. Fixed a spot where only the null representation was being considered as default.\n\nWrote a simple REST test to confirm the behavior is fixed. All tests pass.\n","id":"44099297","title":"Posting a mapping with default analyzer fails","reopen_by":"colings86","opened_on":"2014-09-26T19:44:30Z","closed_by":"colings86"},{"number":"7863","comments":[{"date":"2014-09-29T18:00:30Z","author":"grantr","text":"I learned in #7864 that empty aliases are technically impossible, so I edited the issue to recommend raising an error.\n"},{"date":"2014-10-10T20:54:36Z","author":"polyfractal","text":"Fixed in https:\/\/github.com\/elasticsearch\/elasticsearch\/commit\/ee857bc07302b5bfcb327b3be9d07d9c6de28254, thanks for the bug report!\n\nEdit: this change was backed out due to a testing problem.  See correction below\n"}],"reopen_on":"2014-10-27T15:21:20Z","opened_by":"grantr","closed_on":"2014-10-27T18:40:39Z","description":"Creating an alias with a null `index` matches all indices instead of none:\n\n```\n$ curl -XPUT localhost:9200\/index1\n{\"acknowledged\":true}\n$ curl -XPUT localhost:9200\/index2\n{\"acknowledged\":true}\n$ curl localhost:9200\/_aliases -d '{\"actions\":[{\"add\":{\"alias\":\"empty-alias\", \"index\":null}}]}'\n{\"acknowledged\":true}\n$ curl localhost:9200\/_aliases\n{\"index1\":{\"aliases\":{\"empty-alias\":{}}},\"index2\":{\"aliases\":{\"empty-alias\":{}}}}\n```\n\nThis behavior is surprising and dangerous. Someone who didn't know what to expect might accidentally delete all their indices:\n\n```\n$ curl -XDELETE localhost:9200\/empty-alias\n{\"acknowledged\":true}\n$ curl localhost:9200\/index1\/_count\n{\"error\":\"IndexMissingException[[index1] missing]\",\"status\":404}\n$ curl localhost:9200\/index2\/_count\n{\"error\":\"IndexMissingException[[index1] missing]\",\"status\":404}\n```\n\nWhen given a null `index` value, the alias action should raise an error. Creating an alias to match all indices should require a `*` wildcard as the `index` value.\n","id":"43825079","title":"Null index in alias POST matches all indices","reopen_by":"polyfractal","opened_on":"2014-09-24T23:02:08Z","closed_by":"polyfractal"},{"number":"7640","comments":[{"date":"2014-11-14T11:53:39Z","author":"clintongormley","text":"Jackson 2.4.3 now contains the above fixes.  We should upgrade and add the changes mentioned above.\n"},{"date":"2014-11-21T09:39:05Z","author":"s1monw","text":"if we get a fix for this I think it should go into `1.3.6`\n"},{"date":"2014-11-23T12:55:00Z","author":"s1monw","text":"do we need to do anything else than upgrading jackson? @pickypg do you have a ETA for this?\n"},{"date":"2014-11-24T05:48:16Z","author":"pickypg","text":"I should have this up for review on Monday.\n\nWe need to change `XContentFactory.xContentType(...)` to support the new header. By default, the new `CBORGenerator.Feature.WRITE_TYPE_HEADER` feature is `false`, so just upgrading will do nothing (nothing breaks, but nothing improves).\n"},{"date":"2014-11-25T19:04:53Z","author":"pickypg","text":"Merged\n"},{"date":"2014-11-25T21:57:11Z","author":"pickypg","text":"This was reverted because the JSON tokenizer was acting up in some of the randomized tests. I am looking at the root cause (my change or just incoming changes from 2.4.3).\n"}],"reopen_on":"2014-11-25T21:57:11Z","opened_by":"clintongormley","closed_on":"2015-03-20T21:06:51Z","description":"Currently we only check if the first byte of the body is a `BYTE_OBJECT_INDEFINITE` to determine whether the content is CBOR or not.  However, what we should actually do is to check whether the \"major type\" is an object.\n\nSee:\n- https:\/\/github.com\/FasterXML\/jackson-dataformat-cbor\/blob\/master\/src\/main\/java\/com\/fasterxml\/jackson\/dataformat\/cbor\/CBORParser.java#L614\n- https:\/\/github.com\/FasterXML\/jackson-dataformat-cbor\/blob\/master\/src\/main\/java\/com\/fasterxml\/jackson\/dataformat\/cbor\/CBORParser.java#L682\n\nAlso, CBOR can be prefixed with a self-identifying tag, `0xd9d9f7`,  which we should check for as well.  Currently Jackson doesn't recognise this tag, but it looks like that will change in the future: https:\/\/github.com\/FasterXML\/jackson-dataformat-cbor\/issues\/6\n","id":"42189557","title":"CBOR: Improve recognition of CBOR data format","reopen_by":"pickypg","opened_on":"2014-09-08T12:09:52Z","closed_by":"kimchy"},{"number":"7386","comments":[{"date":"2014-08-21T19:09:42Z","author":"s1monw","text":"could this be related to #6692 did you upgrade all nodes to 1.3 or do you still have nodes < 1.3.0 in the cluster?\n"},{"date":"2014-08-21T19:16:30Z","author":"nik9000","text":"Only about 1\/3 of the nodes before we got warnings about disk space.\n"},{"date":"2014-08-21T19:30:02Z","author":"s1monw","text":"I guess it's not freeing the space unless an upgraded node holds a copy of the shard. That is new in 1.3 and I still try to remember what the background was. Can you check if that assumption is true, are the shards that are not delete allocated on old nodes? \n"},{"date":"2014-08-21T19:32:12Z","author":"nik9000","text":"Well, this is almost certainly the cause:\n\n``` java\n            \/\/ If all nodes have been upgraded to >= 1.3.0 at some point we get back here and have the chance to\n            \/\/ run this api. (when cluster state is then updated)\n            if (node.getVersion().before(Version.V_1_3_0)) {\n                logger.debug(\"Skip deleting deleting shard instance [{}], a node holding a shard instance is < 1.3.0\", shardRouting);\n                return false;\n            }\n```\n\n1.3 won't delete stuff from the disks until the whole cluster is 1.3.  That's ugly.  I run with disks 50% full and the upgrade process almost filled them just with shuffling.\n\nSide note:  if the shards are still in the routing table it'd be nice to see them.  Right now they seem to be invisble to he _cat api.\n"},{"date":"2014-08-21T19:36:34Z","author":"s1monw","text":"@nik9000 this was a temporary thing to add extra safety. It will get lower the more nodes you upgrade. I agree we could expose some more infos here if stuff is still on disk. \n"},{"date":"2014-08-21T19:44:11Z","author":"nik9000","text":"This gave me quite a scare!  I was running this upgrade over night with a script with extra sleeping to keep the cluster balanced.  It woke me up with 99% disk utilization on one of the nodes.  I'll keep pushing the upgrade through carefully.\n"},{"date":"2014-08-21T19:49:17Z","author":"nik9000","text":"For posterity: if you nuke the contents of your node's disk after stopping Elasticsearch 1.2 but before starting Elasticsearch 1.3 then you won't end up with too much data that can't be cleared.  The more nodes you upgrade the more shards you'll be able to delete any way - like @s1monw said.\n"},{"date":"2014-08-21T20:11:30Z","author":"s1monw","text":"just to clarify a bit more we added some safety in 1.3 that required a new API and we can only call this API if we know that we are allocated on another 1.3 or newer node that is why we keep the data around longer. thanks for opening this nik!\n"},{"date":"2014-08-22T17:30:28Z","author":"nik9000","text":"So far we haven't seen any cleanup of old shards and we've just restarted the last node to pick up 1.3.2.\n![whatson_not_yet_cleaning](https:\/\/cloud.githubusercontent.com\/assets\/215970\/4014563\/c678a198-2a21-11e4-88e2-f5a00fe5c987.png)\nDeleting the contents of the node slowed down the upgrade but allowed us to continue the process without space being taken up by indexes we couldn't remove.\n"},{"date":"2014-08-22T17:48:10Z","author":"martijnvg","text":"The unused shard copies only get deleted if all its active copies can be verified. Maybe shard to be cleaned up had copies on this not yet upgraded node?\n\nUnused shard copies should get cleaned up now, if that isn't the case then that is bad.\n\nIf you enable trace logging for the `indices.store` category then we can get a peek in ES' decision making.\n"},{"date":"2014-08-22T18:19:47Z","author":"nik9000","text":"@martijnvg - I'll see what happens once all the cluster goes green after the last upgrade - that'll be in under an hour.\n\nDid we do anything to allow changing log levels on the fly?  I remember seeing something about it but #6416 is still open.\n"},{"date":"2014-08-22T18:20:04Z","author":"nik9000","text":"And by we I mean you, I guess :)\n"},{"date":"2014-08-22T18:24:30Z","author":"martijnvg","text":":) Well this has been in for a while: #2517\n\nWhich allows to change the log settings via the cluster update api.\n"},{"date":"2014-08-22T18:31:10Z","author":"nik9000","text":"OK!  Here is something:  https:\/\/gist.github.com\/nik9000\/89013550ec78da5808e4\n"},{"date":"2014-08-22T18:36:43Z","author":"nik9000","text":"That is getting spit out constantly.\n"},{"date":"2014-08-22T18:48:15Z","author":"nik9000","text":"Looks like it is on every node as well.\n"},{"date":"2014-08-22T18:50:08Z","author":"nik9000","text":"Cluster is now green and lots of old data still sitting around.\n"},{"date":"2014-08-22T19:06:59Z","author":"bleskes","text":"@nik9000 this is very odd. The line points at a null clusterName . All the nodes are continuously logging this? Can I ask you to enable debug logging for the root logger and share the log? I hope to get more context into when this can happen.\n"},{"date":"2014-08-22T19:08:18Z","author":"nik9000","text":"I see that cluster name is something that as introduced in 1.1.1.  Maybe a coincidence - but I haven't performed a full cluster restart since upgrading to 1.1.0.\n"},{"date":"2014-08-22T19:09:26Z","author":"nik9000","text":"Let me see about that debug logging - seems like that'll be a ton of data.  Also - looks like this is the only thing that doesn't check if the cluster name is non null.  Probably just a coincidence because it supposed to be non-null since 1.1.1 I guess.....\n"},{"date":"2014-08-22T19:13:20Z","author":"bleskes","text":"@nik9000 I'm not sure I follow what you mean by \n\n>  looks like this is the only thing that doesn't check if the cluster name is non null. \n\nI was referring to this line: https:\/\/github.com\/elasticsearch\/elasticsearch\/blob\/v1.3.2\/src\/main\/java\/org\/elasticsearch\/indices\/store\/IndicesStore.java#L418\n"},{"date":"2014-08-22T19:20:54Z","author":"nik9000","text":"@bleskes - sorry, yeah.  I was looking at other code that looked at the cluster name and its pretty careful around the cluster name potentially being null.  Like \nhttps:\/\/github.com\/elasticsearch\/elasticsearch\/blob\/v1.3.2\/src\/main\/java\/org\/elasticsearch\/cluster\/ClusterState.java#L577 and https:\/\/github.com\/elasticsearch\/elasticsearch\/blob\/v1.3.2\/src\/main\/java\/org\/elasticsearch\/discovery\/zen\/ZenDiscovery.java#L551 .\n\nI guess what I'm saying is that if the cluster state never picked up the name somehow this looks like the only thing that would break.\n"},{"date":"2014-08-22T19:25:33Z","author":"nik9000","text":"Tried setting logger to debug and didn't get anything super interesting.  Here is some of it: https:\/\/gist.github.com\/nik9000\/b9c40805abb4bcbb5b61\n"},{"date":"2014-08-22T19:37:40Z","author":"bleskes","text":"Thx Nik.  I have a theory. Indeed the cluster name as part of the _cluster state_ was introduced in 1.1.1 . When a node of version >=1.1.1 reads the cluster state from an older node, that field will be populated with null. During the upgrade from 1.1.0 this happened and the cluster state in memory has it's name set to null. Since you never restarted the complete cluster since then, all nodes have kept communicating it keep it alive. This trips this new code. A full cluster restart should fix it but that's obviously totally not desirable. I'm still trying to come up with a potential work around... \n"},{"date":"2014-08-22T19:40:12Z","author":"bleskes","text":"@nik9000 do you use dedicated master nodes? it doesn't look so from the logs but I want to double check\n"},{"date":"2014-08-22T19:42:46Z","author":"nik9000","text":"@bleskes no dedicated master nodes.\n"},{"date":"2014-08-22T19:44:02Z","author":"nik9000","text":"@bleskes that's what I was thinking - I was digging through places where the cluster state is built from name and they are pretty rare.  Still, it'd take me some time to validate that they never get saved.\n"},{"date":"2014-08-22T21:36:41Z","author":"nik9000","text":"More posterity: this broke for me because when I started the cluster I was using 1.1.0 and I haven't done a full restart since - only rolling restarts.  If you are in that boat - do not upgrade to 1.3 until 1.3.3 is released.\n"},{"date":"2014-08-27T19:42:30Z","author":"bleskes","text":"I'm going to close this as it is fixed by the change my in #7414\n"},{"date":"2014-08-27T20:11:17Z","author":"nik9000","text":"Thanks!\n"}],"reopen_on":"2014-08-22T18:49:48Z","opened_by":"nik9000","closed_on":"2014-08-27T19:42:30Z","description":"Upgrade caused shard data to stay on nodes even after it isn't useful any more.\n\nThis comes from https:\/\/groups.google.com\/forum\/#!topic\/elasticsearch\/Mn1N0xmjsL8\n\nWhat I did:\nStarted upgrading from Elasticsearch 1.2.1 to Elasticsearch 1.3.2.  For each of the 6 nodes I updated:\n- Set allocation to primaries only\n- Sync new plugins into place\n- Update deb package\n- Restart Elasticsearch\n- Wait for Elasticsearch to respond on the local host\n- Set allocation to all\n- Wait for Elasticsearch to report GREEN\n- Sleep for half an hour so the cluster can rebalance itself a bit\n\nWhat happened:\nThe new version of Elasticsearch came up but didn't remove all the shard data it can't use.  This picture from Whatson shows the problem pretty well:\nhttps:\/\/wikitech.wikimedia.org\/wiki\/File:Whatson_out_of_disk.png\nThe nodes on the left were upgraded and blue means disk usage by Elasticsearch and brown is \"other\" disk usage.\n\nWhen I dig around on the filesystem all the space usage is in the shard storage directory (\/var\/lib\/elasticsearch\/production-search-eqiad\/nodes\/0\/indices) but when I compare the list of open files to the list of files on the file system [with this](https:\/\/gist.github.com\/nik9000\/d2dba49c156a5259a7d6) I see that whole directories are just sitting around, unused.  Hitting the `\/_cat\/shards\/<directory_name>` corroborates that the shard in the directory isn't on the node.  Oddly, if we keep poking around we find open files in directories representing shards that we don't expect to be on the node either....\n\nWhat we're doing now:\nWe're going to try restarting the upgrade and blasting the data directory on the node as we upgrade it.\n\nReproduction steps:\nNo idea.  And I'm a bit afraid to keep pushing things on our cluster with it in the state that it is in.\n","id":"40834362","title":"Internal: Upgrade caused shard data to stay on nodes","reopen_by":"nik9000","opened_on":"2014-08-21T18:34:53Z","closed_by":"bleskes"},{"number":"7343","comments":[{"date":"2014-08-19T22:15:09Z","author":"OlegYch","text":"the ticket about 1.3.2 is #7341 \nbefore an upgrade to 1.3.2 i was also getting random ClassCastExceptions inside org.apache.lucene.search.TopDocs#merge when sorting on that field\nnot sure if all of this is an indication of one issue\nCheckIndex does not report any errors\n"},{"date":"2014-08-19T23:42:48Z","author":"OlegYch","text":"same failure is observed when querying multiple indexes instead of alias, i.e GET myindex_7,users_idx3\/mytype\/_search\n"},{"date":"2014-08-20T07:46:50Z","author":"clintongormley","text":"OK, looking at this issue and #7341, it looks like you have fields with the same name in different types in the same index.  When you try to load these field values into fielddata, it chooses one of the field mappings (whichever one it finds first) and loads in that format.\n\nThis is not new - it has been like this since the beginning.  Fields with the same name should be mapped in the same way.\n\nWe're planning on enforcing this in the future.  See #4081 \n"},{"date":"2014-08-20T11:38:05Z","author":"OlegYch","text":"why does it work when searching through single index?\n"},{"date":"2014-08-20T13:19:35Z","author":"clintongormley","text":"It just depends on which field it happens to see first.\n"},{"date":"2014-08-20T13:51:06Z","author":"OlegYch","text":"thanks, got it\n"},{"date":"2014-08-21T15:04:59Z","author":"OlegYch","text":"i made sure all types in all indexes from that alias use the same type for that field but the sorting is still wrong when querying through alias, how can that be ?\n"},{"date":"2014-08-21T15:05:55Z","author":"OlegYch","text":"even cleared the cache for all indexes\n"},{"date":"2014-08-21T15:35:15Z","author":"OlegYch","text":"```\ncurl -XGET myserver:9200\/myalias\/_mapping?pretty 2>\/bla  |grep -A3 start_date\n              \"start_date\" : {\n                \"type\" : \"double\"\n              },\n              \"terrain\" : {\n--\n              \"start_date\" : {\n                \"type\" : \"double\"\n              }\n            }\n\nGET myalias\/mytype\/_search\n{\n  \"query\": {\n    \"constant_score\": {\n      \"filter\": {\n        \"and\": {\n\"filters\": [\n              {\"term\": {\n                \"user_id\": 582313\n              }}\n\n          ]\n        }\n      }\n    }\n  },\n  \"sort\": [\n    {\n      \"data.start_date\": {\n        \"order\": \"asc\",\n        \"ignore_unmapped\": \"true\"\n      }\n    }\n  ]\n,\n\"aggs\": {\n  \"max\": {\n    \"max\": {\n      \"field\": \"data.start_date\"\n    }\n  }\n},\n  \"size\": 1100\n}\n\n{\n   \"took\": 19,\n   \"timed_out\": false,\n   \"_shards\": {\n      \"total\": 20,\n      \"successful\": 20,\n      \"failed\": 0\n   },\n   \"hits\": {\n      \"total\": 5,\n      \"max_score\": null,\n      \"hits\": [\n         {\n            \"_index\": \"myindex\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_F5114468-DA0E-4E9F-A195-5122AAEBC475\",\n            \"_score\": null,\n            \"_source\": {\n               \"_id\": \"582313_F5114468-DA0E-4E9F-A195-5122AAEBC475\",\n               \"data\": {\n                  \"id\": \"F5114468-DA0E-4E9F-A195-5122AAEBC475\",\n                  \"duration\": 897.5677289962769,\n                  \"end_date\": 1408444861.339127,\n                  \"category\": \"sleep\",\n                  \"deep_duration\": 0,\n                  \"quality\": 0.05621061269658856,\n                  \"light_duration\": 50.45283198356628,\n                  \"phases\": [\n                     {\n                        \"duration\": 850.4915599822998,\n                        \"type\": \"awake\",\n                        \"date\": 1408443960.083729\n                     },\n                     {\n                        \"duration\": 50.45283198356628,\n                        \"type\": \"light\",\n                        \"date\": 1408444810.575289\n                     },\n                     {\n                        \"duration\": 0.3110060691833496,\n                        \"type\": \"awake\",\n                        \"date\": 1408444861.028121\n                     }\n                  ],\n                  \"awake_duration\": 850.8025660514832,\n                  \"order_date\": 1408444861.339127,\n                  \"start_date\": 1408443963.771398,\n                  \"editable\": true\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               1408443963.771398\n            ]\n         },\n         {\n            \"_index\": \"myindex\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_61647662-2738-4C6B-A0BD-1536F1EAA313\",\n            \"_score\": null,\n            \"_source\": {\n               \"_id\": \"582313_61647662-2738-4C6B-A0BD-1536F1EAA313\",\n               \"data\": {\n                  \"id\": \"61647662-2738-4C6B-A0BD-1536F1EAA313\",\n                  \"duration\": 780,\n                  \"end_date\": 1408531044,\n                  \"distance\": 200,\n                  \"category\": \"running\",\n                  \"calories\": 292.11,\n                  \"order_date\": 1408531044,\n                  \"start_date\": 1408530264,\n                  \"editable\": true\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               1408530264\n            ]\n         },\n         {\n            \"_index\": \"myindex\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_2FEA2D8A-4B2A-4C71-8A8D-AE0921B3AAAD\",\n            \"_score\": null,\n            \"_source\": {\n               \"_id\": \"582313_2FEA2D8A-4B2A-4C71-8A8D-AE0921B3AAAD\",\n               \"data\": {\n                  \"id\": \"2FEA2D8A-4B2A-4C71-8A8D-AE0921B3AAAD\",\n                  \"duration\": 7505.24406504631,\n                  \"end_date\": 1408541467.491296,\n                  \"category\": \"sleep\",\n                  \"deep_duration\": 0,\n                  \"quality\": 0,\n                  \"light_duration\": 0,\n                  \"phases\": [\n                     {\n                        \"duration\": 7505.244183063507,\n                        \"type\": \"awake\",\n                        \"date\": 1408533962.247113\n                     }\n                  ],\n                  \"awake_duration\": 7505.244183063507,\n                  \"order_date\": 1408541467.491296,\n                  \"start_date\": 1408533962.247231,\n                  \"editable\": false\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               1408533962.247231\n            ]\n         },\n         {\n            \"_index\": \"myindex\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_71F810C0-2290-46F5-84FE-36AE390B06FA\",\n            \"_score\": null,\n            \"_source\": {\n               \"_id\": \"582313_71F810C0-2290-46F5-84FE-36AE390B06FA\",\n               \"data\": {\n                  \"id\": \"71F810C0-2290-46F5-84FE-36AE390B06FA\",\n                  \"duration\": 7620,\n                  \"end_date\": 1408557945.435768,\n                  \"distance\": 100,\n                  \"category\": \"cycling\",\n                  \"calories\": 1189.0375,\n                  \"order_date\": 1408557945.435768,\n                  \"start_date\": 1408550325.435768,\n                  \"editable\": true\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               1408550325.435768\n            ]\n         },\n         {\n            \"_index\": \"myindex\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_5C2797AA-7BFD-40AD-99FF-35278E719293\",\n            \"_score\": null,\n            \"_source\": {\n               \"_id\": \"582313_5C2797AA-7BFD-40AD-99FF-35278E719293\",\n               \"data\": {\n                  \"id\": \"5C2797AA-7BFD-40AD-99FF-35278E719293\",\n                  \"duration\": 3600,\n                  \"end_date\": 1408548615,\n                  \"is_deleted\": true,\n                  \"distance\": 100,\n                  \"category\": \"cycling\",\n                  \"calories\": 561.7500000000001,\n                  \"order_date\": 1408548615,\n                  \"start_date\": 1408545015,\n                  \"editable\": true\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               1408545015\n            ]\n         }\n      ]\n   },\n   \"aggregations\": {\n      \"max\": {\n         \"value\": 1408550325.435768\n      }\n   }\n}\n```\n"},{"date":"2014-08-22T14:18:28Z","author":"clintongormley","text":"@OlegYch Apologies - I was wrong here, didn't read carefully enough, and was confused by the ClassCastException, which is usually related to the problem I described before.\n\nThis does indeed look like a bug. Do you have the stack trace emitted when merging? Also, could you provide:\n\n```\nGET \/_settings\nGET \/_mapping\nGET \/_aliases\n```\n\nthanks\n"},{"date":"2014-08-25T15:49:44Z","author":"OlegYch","text":"@clintongormley there doesn't seem to be any exceptions caused by query\ni've sent you the results of those requests on email, please let me know if there is anything else i can do to diagnoze it\n"},{"date":"2014-09-04T11:26:16Z","author":"jpountz","text":"I'm not sure the bug is related to aliases. The sort values look wrong in both cases (even when querying a single index), see:\n\n``` json\n        {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1408387971.543879-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 871.749,\n                  \"end_date\": 1408395600,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1408387971.543879,\n                  \"number_of_steps\": 1167,\n                  \"duration\": 7628.456121563911,\n                  \"order_date\": 1408395600,\n                  \"id\": \"1408387971.543879-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               6.958361125e-315\n            ]\n         }\n```\n\nThe value in `sort` should be equal to the value in `start_date` but they are completely different. But it happens that if you interpret the bits of `1408387971` (the integer part of start date) as double bits, you get `6.958361125E-315`, which is the sort value.\n\nSo in spite of the fact that the field is mapped as a double in the mappings, it looks like it has been indexed as a long. I will try to dig more...\n"},{"date":"2014-09-04T18:25:19Z","author":"jpountz","text":"The only explanation I can find is that the same field has been dynamically mapped as a long on some shards and as a double on other shards (before mappings propagation  replicated the mappings to all nodes).\n"},{"date":"2014-09-05T13:02:46Z","author":"OlegYch","text":"i explicitly put the mapping for that type before putting any data in it, so i doubt that is the case\ni believe the sort value was borked because there were different types in different mappings for that field\ncurrently the value in sort is correct (as i made sure there are no conflicts, and use full field path including type name), but the sorting is still wrong\n"},{"date":"2015-11-21T17:16:17Z","author":"clintongormley","text":"This should have been resolved by the changes in #8870 in 2.0.  Closing\n"}],"reopen_on":"2014-08-22T14:15:58Z","opened_by":"OlegYch","closed_on":"2015-11-21T17:16:17Z","description":"using 1.3.1 and 1.3.2 the ordering of results differs when searching on alias vs searching on the index with actual data from that alias (only one index in the alias has any data in the type)\n\n```\nGET myindex_search\/_alias\n{\n   \"myindex_7\": {\n      \"aliases\": {\n         \"myindex_search\": {},\n         \"myindex\": {}\n      }\n   },\n   \"users_idx3\": {\n      \"aliases\": {\n         \"users_idx\": {},\n         \"myindex_search\": {},\n         \"users_idx_search\": {},\n      }\n   }\n}\n```\n\n```\nGET myindex_search\/mytype\/_search\n{\n  \"query\": {\n    \"constant_score\": {\n      \"filter\": {\n        \"and\": {\n\"filters\": [\n              {\"term\": {\n                \"user_id\": 582313\n              }}\n\n          ]\n        }\n      }\n    }\n  },\n  \"sort\": [\n    {\n      \"data.start_date\": {\n        \"order\": \"asc\",\n        \"ignore_unmapped\": \"true\"\n      }\n    }\n  ]\n,\n\"aggs\": {\n  \"max\": {\n    \"max\": {\n      \"field\": \"data.order_date\"\n    }\n  }\n}, \n  \"size\": 110\n}\n```\n\nreturns\n\n```\n{\n   \"took\": 5,\n   \"timed_out\": false,\n   \"_shards\": {\n      \"total\": 20,\n      \"successful\": 20,\n      \"failed\": 0\n   },\n   \"hits\": {\n      \"total\": 16,\n      \"max_score\": null,\n      \"hits\": [\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1408441377.377091-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 1210.14,\n                  \"end_date\": 1408449667.817116,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1408441377.377091,\n                  \"number_of_steps\": 1620,\n                  \"duration\": 8290.440024495125,\n                  \"order_date\": 1408449667.817116,\n                  \"id\": \"1408441377.377091-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               6.958624986e-315\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1408452938.713164-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 10.458,\n                  \"end_date\": 1408453879.153991,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1408452938.713164,\n                  \"number_of_steps\": 14,\n                  \"duration\": 940.4408271312714,\n                  \"order_date\": 1408453879.153991,\n                  \"id\": \"1408452938.713164-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               6.958682104e-315\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1408050000-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 5336.568,\n                  \"end_date\": 1408136400,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1408050000,\n                  \"number_of_steps\": 7144,\n                  \"duration\": 86400,\n                  \"order_date\": 1408136400,\n                  \"id\": \"1408050000-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               1408050000\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1408222800-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 4076.379,\n                  \"end_date\": 1408309200,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1408222800,\n                  \"number_of_steps\": 5457,\n                  \"duration\": 86400,\n                  \"order_date\": 1408309200,\n                  \"id\": \"1408222800-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               1408222800\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1408463037.221965-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 177.786,\n                  \"end_date\": 1408470644.548557,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1408463037.221965,\n                  \"number_of_steps\": 238,\n                  \"duration\": 7607.326591610909,\n                  \"order_date\": 1408470644.548557,\n                  \"id\": \"1408463037.221965-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               1408463037.221965\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1408456930.040181-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 5.229,\n                  \"end_date\": 1408463006.851273,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1408456930.040181,\n                  \"number_of_steps\": 7,\n                  \"duration\": 6076.811092078686,\n                  \"order_date\": 1408463006.851273,\n                  \"id\": \"1408456930.040181-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               6.95870183e-315\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1407790800-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 2.241,\n                  \"end_date\": 1407823148.624126,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1407790800,\n                  \"number_of_steps\": 3,\n                  \"duration\": 32348.62412595749,\n                  \"order_date\": 1407823148.624126,\n                  \"id\": \"1407790800-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               1407790800\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1407877200-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 4134.645,\n                  \"end_date\": 1407963600,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1407877200,\n                  \"number_of_steps\": 5535,\n                  \"duration\": 86400,\n                  \"order_date\": 1407963600,\n                  \"id\": \"1407877200-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               1407877200\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1407963600-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 6652.035,\n                  \"end_date\": 1408050000,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1407963600,\n                  \"number_of_steps\": 8905,\n                  \"duration\": 86400,\n                  \"order_date\": 1408050000,\n                  \"id\": \"1407963600-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               1407963600\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1408136400-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 10236.141,\n                  \"end_date\": 1408222800,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1408136400,\n                  \"number_of_steps\": 13703,\n                  \"duration\": 86400,\n                  \"order_date\": 1408222800,\n                  \"id\": \"1408136400-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               1408136400\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1408309200-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 3161.304,\n                  \"end_date\": 1408395600,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1408309200,\n                  \"number_of_steps\": 4232,\n                  \"duration\": 86400,\n                  \"order_date\": 1408395600,\n                  \"id\": \"1408309200-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               1408309200\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1408395600-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 2337.363,\n                  \"end_date\": 1408474407.530582,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1408395600,\n                  \"number_of_steps\": 3129,\n                  \"duration\": 78807.53058201075,\n                  \"order_date\": 1408474407.530582,\n                  \"id\": \"1408395600-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               1408395600\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1408449687.813152-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 5.229,\n                  \"end_date\": 1408450160.890107,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1408449687.813152,\n                  \"number_of_steps\": 7,\n                  \"duration\": 473.0769553780556,\n                  \"order_date\": 1408450160.890107,\n                  \"id\": \"1408449687.813152-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               6.95866604e-315\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1407823148.624126-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 1706.148,\n                  \"end_date\": 1407877200,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1407823148.624126,\n                  \"number_of_steps\": 2284,\n                  \"duration\": 54051.37587404251,\n                  \"order_date\": 1407877200,\n                  \"id\": \"1407823148.624126-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               1407823148.624126\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1408387971.543879-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 871.749,\n                  \"end_date\": 1408395600,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1408387971.543879,\n                  \"number_of_steps\": 1167,\n                  \"duration\": 7628.456121563911,\n                  \"order_date\": 1408395600,\n                  \"id\": \"1408387971.543879-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               6.958361125e-315\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1408450305.447627-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 15.687,\n                  \"end_date\": 1408452812.031366,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1408450305.447627,\n                  \"number_of_steps\": 21,\n                  \"duration\": 2506.583738684654,\n                  \"order_date\": 1408452812.031366,\n                  \"id\": \"1408450305.447627-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               6.958669096e-315\n            ]\n         }\n      ]\n   },\n   \"aggregations\": {\n      \"max\": {\n         \"value\": 4743694330017250000\n      }\n   }\n}\n```\n\nas opposed to correct sorting of data when querying single index\n\n```\nGET myindex_7\/mytype\/_search\n{\n  \"query\": {\n    \"constant_score\": {\n      \"filter\": {\n        \"and\": {\n\"filters\": [\n              {\"term\": {\n                \"user_id\": 582313\n              }}\n\n          ]\n        }\n      }\n    }\n  },\n  \"sort\": [\n    {\n      \"data.start_date\": {\n        \"order\": \"asc\",\n        \"ignore_unmapped\": \"true\"\n      }\n    }\n  ]\n,\n\"aggs\": {\n  \"max\": {\n    \"max\": {\n      \"field\": \"data.order_date\"\n    }\n  }\n}, \n  \"size\": 110\n}\n```\n\n```\n{\n   \"took\": 4,\n   \"timed_out\": false,\n   \"_shards\": {\n      \"total\": 8,\n      \"successful\": 8,\n      \"failed\": 0\n   },\n   \"hits\": {\n      \"total\": 16,\n      \"max_score\": null,\n      \"hits\": [\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1408387971.543879-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 871.749,\n                  \"end_date\": 1408395600,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1408387971.543879,\n                  \"number_of_steps\": 1167,\n                  \"duration\": 7628.456121563911,\n                  \"order_date\": 1408395600,\n                  \"id\": \"1408387971.543879-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               6.958361125e-315\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1408441377.377091-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 1210.14,\n                  \"end_date\": 1408449667.817116,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1408441377.377091,\n                  \"number_of_steps\": 1620,\n                  \"duration\": 8290.440024495125,\n                  \"order_date\": 1408449667.817116,\n                  \"id\": \"1408441377.377091-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               6.958624986e-315\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1408449687.813152-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 5.229,\n                  \"end_date\": 1408450160.890107,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1408449687.813152,\n                  \"number_of_steps\": 7,\n                  \"duration\": 473.0769553780556,\n                  \"order_date\": 1408450160.890107,\n                  \"id\": \"1408449687.813152-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               6.95866604e-315\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1408450305.447627-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 15.687,\n                  \"end_date\": 1408452812.031366,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1408450305.447627,\n                  \"number_of_steps\": 21,\n                  \"duration\": 2506.583738684654,\n                  \"order_date\": 1408452812.031366,\n                  \"id\": \"1408450305.447627-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               6.958669096e-315\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1408452938.713164-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 10.458,\n                  \"end_date\": 1408453879.153991,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1408452938.713164,\n                  \"number_of_steps\": 14,\n                  \"duration\": 940.4408271312714,\n                  \"order_date\": 1408453879.153991,\n                  \"id\": \"1408452938.713164-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               6.958682104e-315\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1408456930.040181-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 5.229,\n                  \"end_date\": 1408463006.851273,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1408456930.040181,\n                  \"number_of_steps\": 7,\n                  \"duration\": 6076.811092078686,\n                  \"order_date\": 1408463006.851273,\n                  \"id\": \"1408456930.040181-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               6.95870183e-315\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1407790800-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 2.241,\n                  \"end_date\": 1407823148.624126,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1407790800,\n                  \"number_of_steps\": 3,\n                  \"duration\": 32348.62412595749,\n                  \"order_date\": 1407823148.624126,\n                  \"id\": \"1407790800-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               1407790800\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1407823148.624126-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 1706.148,\n                  \"end_date\": 1407877200,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1407823148.624126,\n                  \"number_of_steps\": 2284,\n                  \"duration\": 54051.37587404251,\n                  \"order_date\": 1407877200,\n                  \"id\": \"1407823148.624126-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               1407823148.624126\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1407877200-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 4134.645,\n                  \"end_date\": 1407963600,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1407877200,\n                  \"number_of_steps\": 5535,\n                  \"duration\": 86400,\n                  \"order_date\": 1407963600,\n                  \"id\": \"1407877200-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               1407877200\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1407963600-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 6652.035,\n                  \"end_date\": 1408050000,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1407963600,\n                  \"number_of_steps\": 8905,\n                  \"duration\": 86400,\n                  \"order_date\": 1408050000,\n                  \"id\": \"1407963600-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               1407963600\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1408050000-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 5336.568,\n                  \"end_date\": 1408136400,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1408050000,\n                  \"number_of_steps\": 7144,\n                  \"duration\": 86400,\n                  \"order_date\": 1408136400,\n                  \"id\": \"1408050000-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               1408050000\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1408136400-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 10236.141,\n                  \"end_date\": 1408222800,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1408136400,\n                  \"number_of_steps\": 13703,\n                  \"duration\": 86400,\n                  \"order_date\": 1408222800,\n                  \"id\": \"1408136400-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               1408136400\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1408222800-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 4076.379,\n                  \"end_date\": 1408309200,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1408222800,\n                  \"number_of_steps\": 5457,\n                  \"duration\": 86400,\n                  \"order_date\": 1408309200,\n                  \"id\": \"1408222800-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               1408222800\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1408309200-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 3161.304,\n                  \"end_date\": 1408395600,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1408309200,\n                  \"number_of_steps\": 4232,\n                  \"duration\": 86400,\n                  \"order_date\": 1408395600,\n                  \"id\": \"1408309200-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               1408309200\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1408395600-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 2337.363,\n                  \"end_date\": 1408474407.530582,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1408395600,\n                  \"number_of_steps\": 3129,\n                  \"duration\": 78807.53058201075,\n                  \"order_date\": 1408474407.530582,\n                  \"id\": \"1408395600-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               1408395600\n            ]\n         },\n         {\n            \"_index\": \"myindex_7\",\n            \"_type\": \"mytype\",\n            \"_id\": \"582313_1408463037.221965-0\",\n            \"_score\": null,\n            \"_source\": {\n               \"data\": {\n                  \"distance\": 177.786,\n                  \"end_date\": 1408470644.548557,\n                  \"calories\": 0,\n                  \"motion_type\": \"stationary\",\n                  \"start_date\": 1408463037.221965,\n                  \"number_of_steps\": 238,\n                  \"duration\": 7607.326591610909,\n                  \"order_date\": 1408470644.548557,\n                  \"id\": \"1408463037.221965-0\"\n               },\n               \"user_id\": 582313\n            },\n            \"sort\": [\n               1408463037.221965\n            ]\n         }\n      ]\n   },\n   \"aggregations\": {\n      \"max\": {\n         \"value\": 4743694330017250000\n      }\n   }\n}\n```\n\nnevermind the broken aggregations and \"sort\" field values, this started to happen only after upgrade to 1.3.2\n","id":"40642600","title":"sorting does not work when querying an alias","reopen_by":"clintongormley","opened_on":"2014-08-19T22:09:10Z","closed_by":"clintongormley"},{"number":"7265","comments":[{"date":"2014-12-17T11:17:03Z","author":"quasipedia","text":"I'm only 93% confident my bug report should be included here, if no,t just tell me and I will open a new issue...\n\nThe problem I am facing is that on uploading an index template, the boolean values get converted to strings if pertaining to the `settings` section, while treated as boolean when in the `mappings`.  Examples (the `-` refers to the template I send, the `+` to the one I get back from ES after my upload).\n\n```\n-        \"index.store.compress.stored\": true, \n-        \"index.store.compress.tv\": true\n+        \"index.store.compress.stored\": \"true\", \n+        \"index.store.compress.tv\": \"true\"\n```\n\nbut at the same time:\n\n```\n-        \"_source\": {\"compress\": true, \"enabled\": true}\n+        \"_source\": {\"compress\": true, \"enabled\": true}\n```\n\nThis is very annoying as it makes unnecessarily difficult to check if a template on a server differs from a master copy held locally on file, for example.\n"},{"date":"2014-12-17T12:42:13Z","author":"clintongormley","text":"@quasipedia good question...  at the moment, settings are strings and there is no getting around that.  I'm wondering if, as part of the great settings rewrite (https:\/\/github.com\/elasticsearch\/elasticsearch\/issues\/6732) this can be changed or not...\n"},{"date":"2016-11-25T18:30:54Z","author":"clintongormley","text":"Boolean parsing of settings is now strict.  Closing"}],"reopen_on":"2014-08-13T20:02:14Z","opened_by":"prog8","closed_on":"2016-11-25T18:30:54Z","description":"I realized that configuration values \"node.master\" and \"node.data\" are case sensitive. Moreover in one place in code only exact value \"true\" is interpreted as boolean true.\n\nClass DiscoveryNode reads both properties two times. Once calls appear in nodeRequiresLocalStorage, and second time in dataNode and masterNode methods.\n\nIn nodeRequiresLocalStorage method Settings.getAsBoolean is called but in second and third case only Settings.get is called and then value compared with \"equals\" to String \"true\".\n\nI think values shouldn't work different in different places. Second thing is that probably Boolean values shouldn't be case sensitive.\n","id":"40193132","title":"Different interpretation of boolean config values","reopen_by":"prog8","opened_on":"2014-08-13T20:01:43Z","closed_by":"clintongormley"},{"number":"6942","comments":[{"date":"2014-07-21T13:51:29Z","author":"dadoonet","text":"It has been fixed some days ago with this commit: https:\/\/github.com\/elasticsearch\/elasticsearch\/commit\/4eca0499fae2f40f3917b0b970a9a5958a738eec\n\ncc @spinscale \n"},{"date":"2014-07-21T16:21:03Z","author":"mikebaldry","text":"aha, thanks :)\n"},{"date":"2015-11-21T19:37:49Z","author":"ptheofan","text":"Bug reappeared? I'm having the exact same issue despite that $JAVA is in doublequotes.\n\n``` bash\n➜  utils git:(master) plugin install jettro\/elasticsearch-gui\n\/usr\/local\/Cellar\/elasticsearch\/2.0.0_1\/libexec\/bin\/plugin: line 109: \/Library\/Internet: No such file or directory\n```\n\nwhereas\n\n``` bash\n➜  utils git:(master) echo $JAVA_HOME\n\/Library\/Internet Plug-Ins\/JavaAppletPlugin.plugin\/Contents\/Home\n```\n"},{"date":"2015-11-23T15:31:58Z","author":"nik9000","text":"Its possible this came back. We're much better at testing these scripts now but we don't test this case. I'll reopen this so we know to investigate. But I don't know when I'll get a chance to work on it though.\n"},{"date":"2015-11-23T15:41:50Z","author":"rmuir","text":"We can test this by putting a space in the path to JAVA_HOME on jenkins servers.\n"},{"date":"2016-02-16T14:16:19Z","author":"pbaille","text":"I've got this issue to, is there a workaround to install marvel?\n"},{"date":"2016-02-19T19:22:36Z","author":"clintongormley","text":"This is still broken in 2.2 and master\n"},{"date":"2016-02-21T15:20:37Z","author":"kdelchev","text":"I had similar issue (`line 114: \/Library\/Internet: No such file or directory`). Running `sudo bin\/plugin license` did the job.\n"},{"date":"2016-07-03T10:48:45Z","author":"ptheofan","text":"Latest workaround that worked for me like a charm. Escape the JAVA_HOME path (and ignore the fact that it's wrapped in double quotes)!\n\nI went from\n\n``` bash\nexport JAVA_HOME=\"\/Library\/Internet Plug-Ins\/JavaAppletPlugin.plugin\/Contents\/Home\"\n```\n\nto (diff is I escaped the space in Internet Plug-Ins)\n\n``` bash\nexport JAVA_HOME=\"\/Library\/Internet\\ Plug-Ins\/JavaAppletPlugin.plugin\/Contents\/Home\"\n```\n\nand it worked\n"},{"date":"2016-09-22T18:17:44Z","author":"OsamHdz","text":"I had the same problem that @kdelchev, but the sudo command didn't work. Adding ' single quotes at \"$JAVA_HOME\" in eval line worked for me.\n\n``` bash\neval '\"$JAVA\"' -client -Delasticsearch -Des.path.home=\"\\\"$ES_HOME\\\"\" $properties -cp \"\\\"$ES_HOME\/lib\/*\\\"\" org.elasticsearch.plugins.PluginManagerCliParser $args\n```\n\nHere is the info:\nhttp:\/\/unix.stackexchange.com\/questions\/131766\/why-does-my-shell-script-choke-on-whitespace-or-other-special-characters\n"},{"date":"2016-11-25T18:24:08Z","author":"clintongormley","text":"this is fixed in 5.0"}],"reopen_on":"2015-11-23T15:36:42Z","opened_by":"mikebaldry","closed_on":"2016-11-25T18:24:08Z","description":"```\n\/usr\/local\/Cellar\/elasticsearch\/1.2.1\/bin  $ echo $JAVA_HOME\n\/Library\/Internet Plug-Ins\/JavaAppletPlugin.plugin\/Contents\/Home\n```\n\nresults in \n\n```\n\/usr\/local\/Cellar\/elasticsearch\/1.2.1\/bin  $ .\/plugin \n.\/plugin: line 49: \/Library\/Internet: No such file or directory\n.\/plugin: line 49: exec: \/Library\/Internet: cannot execute: No such file or directory\n```\n\nSuggested fix (worked for me, but I'm no bash guru)\nput double quotes around $JAVA in the exec line of plugin\n","id":"38299385","title":"bin\/plugin on a system where JAVA_HOME contains spaces fails","reopen_by":"nik9000","opened_on":"2014-07-21T12:48:06Z","closed_by":"clintongormley"},{"number":"6304","comments":[{"date":"2014-05-24T10:52:40Z","author":"clintongormley","text":"HI @JeffreyZZ \n\nThat mapping is incorrect. `include_in_all` needs to be specified under a field, not at the top level.   In previous versions it was just ignored, but in 1.2 it now tells you that it is incorrect.\n\nYou're probably looking for:\n\n```\nPUT \/test \n{\n  \"mappings\": {\n    \"_default_\": {\n      \"_all\": {\n        \"enabled\": false\n      }\n    }\n  }\n}\n```\n"},{"date":"2014-05-27T17:53:28Z","author":"JeffreyZZ","text":"Make sense.  Thanks @clintongormley  !\n"},{"date":"2014-05-28T11:58:57Z","author":"skurfuerst","text":"Are you sure? According to http:\/\/www.elasticsearch.org\/guide\/en\/elasticsearch\/reference\/current\/mapping-root-object-type.html, it says:\n\n> The root object mapping is an object type mapping that maps the root object (the type itself). On top \n> of all the different mappings that can be set using the object type mapping, it allows for additional, \n> type level mapping definitions.\n\nand in http:\/\/www.elasticsearch.org\/guide\/en\/elasticsearch\/reference\/current\/mapping-object-type.html, it says:\n\n> include_in_all can be set on the object type level. When set, it propagates down to all the inner \n> mappings defined within the object that do no explicitly set it.\n\nGreets, Sebastian\n"},{"date":"2014-05-30T11:15:14Z","author":"clintongormley","text":"@skurfuerst \n\nYou're absolutely right - this is a regression.  \/cc @brwe \n"},{"date":"2014-05-30T13:26:58Z","author":"brwe","text":"Yes, opened pull request #6353\n"},{"date":"2014-05-31T03:19:24Z","author":"JeffreyZZ","text":"@brwe @brwe  For ES 1.2.0, is there any workaround to this issue or any impact of this issue?\n"},{"date":"2014-06-01T17:28:26Z","author":"brwe","text":"You could create a dynamic template for the types (example below). \"include_in_all\" will then not be set in the root object but still be applied to all fields and objects. \n\nAs for the impact, if \"include_in_all\" is already set in the root type and you upgrade to 1.2, a MapperParsingException will be thrown on startup. The next time that the type mapping is updated, the \"include_at_all\" setting will be removed. \n\nUnfortunately, I found that a side effect seems to be that it will not only be removed from the root object, but also from fields. I'll have to look into this further.\n\n\"include_in_all\" via \"dynamic_template\":\n\n```\nPUT test\n{\n   \"mappings\": {\n      \"_default_\": {\n         \"dynamic_templates\": [\n            {\n               \"include_all\": {\n                  \"match\": \"*\",\n                  \"mapping\": {\n                     \"include_in_all\": \"true\"\n                  }\n               }\n            }\n         ]\n      }\n   }\n}\nPOST test\/cat\/1\n{\n    \"text\": \"text\"\n}\nGET test\/_mapping\n\n```\n\nResult should be\n\n```\n{\n   \"test\": {\n      \"mappings\": {\n         \"_default_\": {\n            \"dynamic_templates\": [\n               {\n                  \"include_all\": {\n                     \"mapping\": {\n                        \"include_in_all\": \"true\"\n                     },\n                     \"match\": \"*\"\n                  }\n               }\n            ],\n            \"properties\": {}\n         },\n         \"cat\": {\n            \"dynamic_templates\": [\n               {\n                  \"include_all\": {\n                     \"mapping\": {\n                        \"include_in_all\": \"true\"\n                     },\n                     \"match\": \"*\"\n                  }\n               }\n            ],\n            \"properties\": {\n               \"text\": {\n                  \"type\": \"string\",\n                  \"include_in_all\": true\n               }\n            }\n         }\n      }\n   }\n}\n```\n"},{"date":"2014-06-02T07:00:48Z","author":"JeffreyZZ","text":"@brwe @clintongormley \n\nSeems this regression also has impact on the index template if the template defines 'include_in_all' under the type. It allows you to create the template but you're NOT able to create the index of the template in ES 1.2.0.  There is the repro steps : \n\n[Step 1]  create the following index template against ES 1.2.0 cluster\nPUT \/_template\/template_logs\n{\n   \"order\": 0,\n   \"template\": \"logs*\",\n   \"settings\": {\n      \"index.analysis.analyzer.keyword_analyzer.tokenizer\": \"keyword\"\n   },\n   \"mappings\": {\n      \"LogMessage\": {\n         \"include_in_all\": false,\n         \"properties\": {\n            \"activityId\": {\n               \"analyzer\": \"keyword_analyzer\",\n               \"type\": \"string\"\n            }\n         }\n      }\n   }\n}\n\n[Step 2] Try to index this the following document \nPUT \/logs-monday\/LogMessage\/1\n{\n   \"activityId\": \"fcfdab46-730a-4918-b5bd-1da9448608b1\"\n}\n\n[Result]\n{\n   \"error\": \"RemoteTransportException[[ES-TEST-2-master][inet[\/10.1.0.38:9300]][indices\/create]]; nested: MapperParsingException[mapping [LogMessage]]; nested: MapperParsingException[Root type mapping not empty after parsing! Remaining fields: [include_in_all : false]]; \",\n   \"status\": 400\n}\n\n[Expected] \nIndex 'logs-monday' is created and 1 document is indexed. \n\nTry the above steps against ES 1.1.1, it works.  Any workaround?\n"},{"date":"2014-06-02T15:45:13Z","author":"brwe","text":"There is unfortunately no workaround. \n"},{"date":"2014-06-03T18:59:23Z","author":"JeffreyZZ","text":"@brwe  Thanks for reply! BTW, any ETA for ES 1.2.1 release?\n"},{"date":"2014-06-04T06:07:17Z","author":"brwe","text":"We released yesterday: http:\/\/www.elasticsearch.org\/blog\/elasticsearch-1-2-1-released\/\nSorry for the late reply.\n"},{"date":"2014-06-04T16:25:05Z","author":"JeffreyZZ","text":"@brwe  Excellent, thank you!\n"},{"date":"2014-06-06T10:12:56Z","author":"clintongormley","text":"Your mapping is invalid.  I'm assuming the intent of the above is to create the type `pseudo_doc` in any new index, which has the field `content`?\n\nIn this case, the `config\/mappings\/_default\/pseudo_doc.json` file should look like this:\n\n```\n{\n  \"properties\": {\n    \"content\": {\n      \"dynamic\": false,\n      \"properties\": {\n        \"author_id\": {\n          \"type\": \"string\"\n        },\n        \"author_name\": {\n          \"type\": \"string\"\n        },\n        \"content\": {\n          \"type\": \"string\"\n        },\n        \"title\": {\n          \"type\": \"string\"\n        },\n        \"urls\": {\n          \"type\": \"string\"\n        },\n        \"destination_url\": {\n          \"type\": \"string\"\n        },\n        \"publisher\": {\n          \"type\": \"string\"\n        }\n      }\n    }\n  }\n}\n```\n\nAlso, I really recommend doing something like this with index templates, rather than with config files. It is much easier to manage such things via the API instead of via static config files.\n"}],"reopen_on":"2014-05-30T11:15:14Z","opened_by":"JeffreyZZ","closed_on":"2014-06-02T15:49:18Z","description":"One noticeable issue found with ES 1.2.0 during our deployment is that it threw exception when created default mappings with \u2018include_in_all\u2019 nested under it (doesn\u2019t matter it\u2019s set to true\/false). For example, the following index creation command returns error when against ES 1.2.0 but it works well against ES 1.1.1 \n\nPUT test \n{ \n   \"mappings\": { \n      \"_default_\": { \n        \"include_in_all\": true  \n      } \n   } \n} \n\nResult \n{ \n   \"error\": \"MapperParsingException[mapping [_default_]]; nested: MapperParsingException[Root type mapping not empty after parsing! Remaining fields: [include_in_all : true]]; \", \n   \"status\": 400 \n} \n\nIs this a regression with ES 1.2.0? \n","id":"34221475","title":"Mapping: MapperParsingException when create default mapping with 'include_in_all' nested ","reopen_by":"clintongormley","opened_on":"2014-05-24T01:21:23Z","closed_by":"brwe"},{"number":"6025","comments":[{"date":"2014-05-12T15:37:51Z","author":"xyu","text":"It appears that we are experiencing segment explosion issues during these times and are seeing queues in the merge thread pool. For now we are working around this issue by backing off on bulk index when merges fall behind in our indexing jobs. It looks like this issue is also being worked on in #6066 \n"},{"date":"2014-07-28T09:27:19Z","author":"clintongormley","text":"Fixed by #6066 \n"},{"date":"2015-03-04T19:25:22Z","author":"mikemccand","text":"Alas, deleteByQuery is not throttled when merges are falling behind.\n"},{"date":"2015-03-04T19:38:11Z","author":"mikemccand","text":"I'll make this operation throttled like we do for index\/create ops.\n\nHowever I'm doubtful this is \"enough\" to always prevent segment explosion, i.e. https:\/\/github.com\/elasticsearch\/elasticsearch\/issues\/7052 is a better (trickier) long term solution.\n"},{"date":"2015-05-28T16:01:21Z","author":"s1monw","text":"@mikemccand can we close this?\n"},{"date":"2015-05-28T16:59:42Z","author":"mikemccand","text":"Yeah, I'll close it ... I think you can still easily provoke OOME on 1.6, but in 2.0 we are switching to a plugin that runs scan\/scroll query and then bulk delete the resulting IDs, which does fix it.\n"}],"reopen_on":"2015-03-04T19:25:22Z","opened_by":"xyu","closed_on":"2015-05-28T16:59:42Z","description":"Hello,\n\nWe are running an ES 1.1.1 cluster and when we bulk index into it with a high write load we have found that it can trigger OOM errors if we run deletebyquery calls. The symptoms of the problems appears to be the following:\n1. Index into the cluster with a heavy write load, this will cause merges to pile up. During the pileup the merge thread pool queue goes up and stays at ~120 per node. (Maybe related to #5779?)\n2. Merges are no longer able to keep up with new segments created from indexing operations and segments per node shoots up. (We have seen over 30k segments per node.)\n3. Long GC cycles are triggered which may or may not cause the node to drop out of the cluster.\n4. When a Delete By Query call is issued an internal refresh is triggered (#3593) which fails due to an out of memory error and causes the node to be removed from the cluster for not responding. (See [gist](https:\/\/gist.github.com\/xyu\/4ca1ddb56200479a07d7) for logs. In this example es13.iad sends a call to es13.sat which experiences OOM causing es13.iad to remove it from the cluster.)\n\nPlease let me know if there are any other info I can provide that may be helpful.\n","id":"32714004","title":"Delete By Query under heavy indexing load causes OOM errors","reopen_by":"mikemccand","opened_on":"2014-05-02T18:26:10Z","closed_by":"mikemccand"},{"number":"5968","comments":[{"date":"2014-05-27T15:58:46Z","author":"colings86","text":"This is actually working as expected.  Your first query will resolve as shown in the following GeoJSON gist which will not contain your document:\nhttps:\/\/gist.github.com\/anonymous\/82b50b74a7b6d170bfc6\n\nTo create the desired results you specified you would need to split the polygon in to two polygons, one to the left of the date line and the other to the right.  This can be done with the following query:\n\n```\ncurl -XPOST 'localhost:9200\/geo\/_search?pretty' -d '{\n    \"query\" : {\n        \"filtered\" : {\n            \"query\" : {\n                \"match_all\" : {}\n            },\n            \"filter\" : {\n                \"or\" : [\n                    {\n                        \"geo_polygon\" : {\n                            \"p\" : {\n                                \"points\" : [\n                                     { \"lat\": 42, \"lon\": 178 },\n                                     { \"lat\": 39, \"lon\": 178 },\n                                     { \"lat\": 39, \"lon\": 180 },\n                                     { \"lat\": 42, \"lon\": 180 },\n                                     { \"lat\": 42, \"lon\": 178 }\n                                ]\n                            }\n                        }\n                    },\n                    {\n                        \"geo_polygon\" : {\n                            \"p\" : {\n                                \"points\" : [\n                                     { \"lat\": 42, \"lon\": -180 },\n                                     { \"lat\": 39, \"lon\": -180 },\n                                     { \"lat\": 39, \"lon\": -179 },\n                                     { \"lat\": 42, \"lon\": -179 },\n                                     { \"lat\": 42, \"lon\": -180 }\n                                ]\n                            }\n                        }\n                    }\n                ]\n            }\n        }\n    }\n}'\n```\n\nThe bounding box is a little different since by specifying which coordinate is top_left and which is top_right you are fixing the box to overlap the date line.\n"},{"date":"2014-09-17T21:44:43Z","author":"jtibshirani","text":"For me, I'd expect the line segment between two points to lie in the same direction as the great circle arc. Splitting an arbitrary query into sub-polygons isn't entirely straightforward, because it could intersect longitude 180 multiple times.\n\nI have a rough draft of a commit that fixes this issue by shifting the polygon in GeoPolygonFilter (and all points passed in to pointInPolygon) so that it lies completely on one side of longitude 180. It is low-overhead and has basically no effect on the normal case. The only constraint is that the polygon can't span more than 360 degrees in longitude.\n\nDoes this sound reasonable, and is it worth submitting a PR?\n"},{"date":"2014-09-25T18:21:19Z","author":"clintongormley","text":"@colings86 could this be solved with a `left\/right` parameter or something similar?\n"},{"date":"2014-11-28T09:49:36Z","author":"colings86","text":"@nknize is this fixed by https:\/\/github.com\/elasticsearch\/elasticsearch\/pull\/8521 ?\n"},{"date":"2014-12-01T14:32:21Z","author":"nknize","text":"It does not.  This is the infamous \"ambiguous polygon\" problem that occurs when treating a spherical coordinate system as a cartesian plane.  I opened a discussion and feature branch to address this in #8672 \n\ntldr: GeoJSON doesn't specify order, but OGC does.\n\nFeature fix:  Default behavior = For GeoJSON poly's specified in OGC order (shell: ccw, holes: cw) ES Ring logic will correctly transform and split polys across the dateline (e.g., see https:\/\/gist.github.com\/nknize\/d122b243dc63dcba8474).  For GeoJSON poly's provided in the opposite order original behavior will occur (e.g., @colings86 example https:\/\/gist.github.com\/anonymous\/82b50b74a7b6d170bfc6).   \n\nAdditionally, I like @clintongormley suggestion of adding an optional left\/right parameter.  Its an easy fix letting user's clearly specify intent.\n"},{"date":"2014-12-03T14:00:29Z","author":"nknize","text":"This is now addressed in PR #8762\n"},{"date":"2014-12-16T18:41:49Z","author":"nknize","text":"Optional left\/right parameter added in PR #8978 \n"},{"date":"2014-12-29T22:11:30Z","author":"nknize","text":"Merged in edd33c0\n"},{"date":"2015-01-03T22:17:20Z","author":"jtibshirani","text":"I tried @pablocastro's example on trunk, and unfortunately the issue is still there. There might've been some confusion -- the original example refers to the geo_polygon filter, whereas @nknize's fix is for the polygon geo_shape type.\n\nShould I create a new ticket for the geo_polygon filter, or should we re-open this one?\n"},{"date":"2015-01-04T04:41:55Z","author":"nknize","text":"Good catch @jtibshirani!  We'll go ahead and reopen this ticket since its a separate issue.\n"},{"date":"2015-01-28T15:06:48Z","author":"nknize","text":"Reopening due to #9462 \n"},{"date":"2015-02-12T17:36:56Z","author":"ogerman","text":"The search hits of really huge polygon (elasticsearch 1.4.3)\n\n``` javascript\n{\n  \"query\": {\n    \"filtered\": {\n      \"filter\": {\n        \"geo_shape\": {\n          \"location\": {\n            \"shape\": {\n              \"type\": \"Polygon\",\n              \"coordinates\": [\n                [\n                  [\n                    -70.4873046875,\n                    79.9818262344106\n                  ],\n                  [\n                    -70.4873046875,\n                    -28.07230647927298\n                  ],\n                  [\n                    -103.3583984375,\n                    -28.07230647927298\n                  ],\n                  [\n                    -103.3583984375,\n                    79.9818262344106\n                  ],\n                  [\n                    -70.4873046875,\n                    79.9818262344106\n                  ]\n                ]\n              ],\n              \"orientation\": \"ccw\"\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\ndoesn't include any points inside that polygon even with orientation option.\nIs it related with this issue?\n"},{"date":"2015-02-12T22:49:53Z","author":"nknize","text":"Its related.  For now, if you have an ambiguous poly that crosses the pole you'll need to manually split it into 2 separate explicit polys and put inside a `MultiPolygon`  Depending on the complexity of the poly computing the pole intersections can be non-trivial.  The in-work patch will do this for you.\n\nA separate issue is related to the distance_error_pct parameter.  If not specified, larger filters will have reduced accuracy.  Though this seems unrelated to your GeoJSON\n"}],"reopen_on":"2015-01-04T04:41:55Z","opened_by":"pablocastro","closed_on":null,"description":"Using Elasticsearch 1.1.1. I'm seeing geo_polygon behave oddly when the input polygon cross the date line. From a quick look at GeoPolygonFilter.GeoPolygonDocSet.pointInPolygon it doesn't seem that this is explicitly handled. \n\nThe reproduce this create an index\/mapping as:\n\nPOST \/geo\n\n``` json\n{ \"mappings\": { \"docs\": { \"properties\": { \"p\": { \"type\": \"geo_point\" } } } } }\n```\n\nUpload a document:\n\nPUT \/geo\/docs\/1\n\n``` json\n{ \"p\": { \"lat\": 40, \"lon\": 179 } }\n```\n\nSearch with a polygon that's a box around the uploaded point and that crosses the date line:\n\nPOST \/geo\/docs\/_search\n\n``` json\n{\n  \"filter\": { \"geo_polygon\": { \"p\": { \"points\": [\n      { \"lat\": 42, \"lon\": 178 },\n      { \"lat\": 39, \"lon\": 178 },\n      { \"lat\": 39, \"lon\": -179 },\n      { \"lat\": 42, \"lon\": -179 },\n      { \"lat\": 42, \"lon\": 178 }\n  ] } } }\n}\n```\n\nES returns 0 results. If I use a polygon that stays to the west of the date line I do get results:\n\n``` json\n{\n  \"filter\": { \"geo_polygon\": { \"p\": { \"points\": [\n      { \"lat\": 42, \"lon\": 178 },\n      { \"lat\": 39, \"lon\": 178 },\n      { \"lat\": 39, \"lon\": 179.5 },\n      { \"lat\": 42, \"lon\": 179.5 },\n      { \"lat\": 42, \"lon\": 178 }\n  ] } } }\n}\n```\n\nAlso, if I use a bounding box query with the same coordinates as the initial polygon, it does work:\n\n``` json\n{\n  \"filter\": { \"geo_bounding_box\": { \"p\": \n    { \"top_left\": { \"lat\": 42, \"lon\": 178 },\n       \"bottom_right\": { \"lat\": 39, \"lon\": -179 }\n     }\n  } }\n}\n```\n\nIt seems that this code needs to either split the check into east and west checks or normalize the input values. Am I missing something?\n","id":"32418613","title":"geo_polygon not handling polygons that cross the date line properly","reopen_by":"nknize","opened_on":"2014-04-29T04:41:30Z","closed_by":"nknize"},{"number":"5968","comments":[{"date":"2014-05-27T15:58:46Z","author":"colings86","text":"This is actually working as expected.  Your first query will resolve as shown in the following GeoJSON gist which will not contain your document:\nhttps:\/\/gist.github.com\/anonymous\/82b50b74a7b6d170bfc6\n\nTo create the desired results you specified you would need to split the polygon in to two polygons, one to the left of the date line and the other to the right.  This can be done with the following query:\n\n```\ncurl -XPOST 'localhost:9200\/geo\/_search?pretty' -d '{\n    \"query\" : {\n        \"filtered\" : {\n            \"query\" : {\n                \"match_all\" : {}\n            },\n            \"filter\" : {\n                \"or\" : [\n                    {\n                        \"geo_polygon\" : {\n                            \"p\" : {\n                                \"points\" : [\n                                     { \"lat\": 42, \"lon\": 178 },\n                                     { \"lat\": 39, \"lon\": 178 },\n                                     { \"lat\": 39, \"lon\": 180 },\n                                     { \"lat\": 42, \"lon\": 180 },\n                                     { \"lat\": 42, \"lon\": 178 }\n                                ]\n                            }\n                        }\n                    },\n                    {\n                        \"geo_polygon\" : {\n                            \"p\" : {\n                                \"points\" : [\n                                     { \"lat\": 42, \"lon\": -180 },\n                                     { \"lat\": 39, \"lon\": -180 },\n                                     { \"lat\": 39, \"lon\": -179 },\n                                     { \"lat\": 42, \"lon\": -179 },\n                                     { \"lat\": 42, \"lon\": -180 }\n                                ]\n                            }\n                        }\n                    }\n                ]\n            }\n        }\n    }\n}'\n```\n\nThe bounding box is a little different since by specifying which coordinate is top_left and which is top_right you are fixing the box to overlap the date line.\n"},{"date":"2014-09-17T21:44:43Z","author":"jtibshirani","text":"For me, I'd expect the line segment between two points to lie in the same direction as the great circle arc. Splitting an arbitrary query into sub-polygons isn't entirely straightforward, because it could intersect longitude 180 multiple times.\n\nI have a rough draft of a commit that fixes this issue by shifting the polygon in GeoPolygonFilter (and all points passed in to pointInPolygon) so that it lies completely on one side of longitude 180. It is low-overhead and has basically no effect on the normal case. The only constraint is that the polygon can't span more than 360 degrees in longitude.\n\nDoes this sound reasonable, and is it worth submitting a PR?\n"},{"date":"2014-09-25T18:21:19Z","author":"clintongormley","text":"@colings86 could this be solved with a `left\/right` parameter or something similar?\n"},{"date":"2014-11-28T09:49:36Z","author":"colings86","text":"@nknize is this fixed by https:\/\/github.com\/elasticsearch\/elasticsearch\/pull\/8521 ?\n"},{"date":"2014-12-01T14:32:21Z","author":"nknize","text":"It does not.  This is the infamous \"ambiguous polygon\" problem that occurs when treating a spherical coordinate system as a cartesian plane.  I opened a discussion and feature branch to address this in #8672 \n\ntldr: GeoJSON doesn't specify order, but OGC does.\n\nFeature fix:  Default behavior = For GeoJSON poly's specified in OGC order (shell: ccw, holes: cw) ES Ring logic will correctly transform and split polys across the dateline (e.g., see https:\/\/gist.github.com\/nknize\/d122b243dc63dcba8474).  For GeoJSON poly's provided in the opposite order original behavior will occur (e.g., @colings86 example https:\/\/gist.github.com\/anonymous\/82b50b74a7b6d170bfc6).   \n\nAdditionally, I like @clintongormley suggestion of adding an optional left\/right parameter.  Its an easy fix letting user's clearly specify intent.\n"},{"date":"2014-12-03T14:00:29Z","author":"nknize","text":"This is now addressed in PR #8762\n"},{"date":"2014-12-16T18:41:49Z","author":"nknize","text":"Optional left\/right parameter added in PR #8978 \n"},{"date":"2014-12-29T22:11:30Z","author":"nknize","text":"Merged in edd33c0\n"},{"date":"2015-01-03T22:17:20Z","author":"jtibshirani","text":"I tried @pablocastro's example on trunk, and unfortunately the issue is still there. There might've been some confusion -- the original example refers to the geo_polygon filter, whereas @nknize's fix is for the polygon geo_shape type.\n\nShould I create a new ticket for the geo_polygon filter, or should we re-open this one?\n"},{"date":"2015-01-04T04:41:55Z","author":"nknize","text":"Good catch @jtibshirani!  We'll go ahead and reopen this ticket since its a separate issue.\n"},{"date":"2015-01-28T15:06:48Z","author":"nknize","text":"Reopening due to #9462 \n"},{"date":"2015-02-12T17:36:56Z","author":"ogerman","text":"The search hits of really huge polygon (elasticsearch 1.4.3)\n\n``` javascript\n{\n  \"query\": {\n    \"filtered\": {\n      \"filter\": {\n        \"geo_shape\": {\n          \"location\": {\n            \"shape\": {\n              \"type\": \"Polygon\",\n              \"coordinates\": [\n                [\n                  [\n                    -70.4873046875,\n                    79.9818262344106\n                  ],\n                  [\n                    -70.4873046875,\n                    -28.07230647927298\n                  ],\n                  [\n                    -103.3583984375,\n                    -28.07230647927298\n                  ],\n                  [\n                    -103.3583984375,\n                    79.9818262344106\n                  ],\n                  [\n                    -70.4873046875,\n                    79.9818262344106\n                  ]\n                ]\n              ],\n              \"orientation\": \"ccw\"\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\ndoesn't include any points inside that polygon even with orientation option.\nIs it related with this issue?\n"},{"date":"2015-02-12T22:49:53Z","author":"nknize","text":"Its related.  For now, if you have an ambiguous poly that crosses the pole you'll need to manually split it into 2 separate explicit polys and put inside a `MultiPolygon`  Depending on the complexity of the poly computing the pole intersections can be non-trivial.  The in-work patch will do this for you.\n\nA separate issue is related to the distance_error_pct parameter.  If not specified, larger filters will have reduced accuracy.  Though this seems unrelated to your GeoJSON\n"}],"reopen_on":"2015-01-28T15:06:48Z","opened_by":"pablocastro","closed_on":null,"description":"Using Elasticsearch 1.1.1. I'm seeing geo_polygon behave oddly when the input polygon cross the date line. From a quick look at GeoPolygonFilter.GeoPolygonDocSet.pointInPolygon it doesn't seem that this is explicitly handled. \n\nThe reproduce this create an index\/mapping as:\n\nPOST \/geo\n\n``` json\n{ \"mappings\": { \"docs\": { \"properties\": { \"p\": { \"type\": \"geo_point\" } } } } }\n```\n\nUpload a document:\n\nPUT \/geo\/docs\/1\n\n``` json\n{ \"p\": { \"lat\": 40, \"lon\": 179 } }\n```\n\nSearch with a polygon that's a box around the uploaded point and that crosses the date line:\n\nPOST \/geo\/docs\/_search\n\n``` json\n{\n  \"filter\": { \"geo_polygon\": { \"p\": { \"points\": [\n      { \"lat\": 42, \"lon\": 178 },\n      { \"lat\": 39, \"lon\": 178 },\n      { \"lat\": 39, \"lon\": -179 },\n      { \"lat\": 42, \"lon\": -179 },\n      { \"lat\": 42, \"lon\": 178 }\n  ] } } }\n}\n```\n\nES returns 0 results. If I use a polygon that stays to the west of the date line I do get results:\n\n``` json\n{\n  \"filter\": { \"geo_polygon\": { \"p\": { \"points\": [\n      { \"lat\": 42, \"lon\": 178 },\n      { \"lat\": 39, \"lon\": 178 },\n      { \"lat\": 39, \"lon\": 179.5 },\n      { \"lat\": 42, \"lon\": 179.5 },\n      { \"lat\": 42, \"lon\": 178 }\n  ] } } }\n}\n```\n\nAlso, if I use a bounding box query with the same coordinates as the initial polygon, it does work:\n\n``` json\n{\n  \"filter\": { \"geo_bounding_box\": { \"p\": \n    { \"top_left\": { \"lat\": 42, \"lon\": 178 },\n       \"bottom_right\": { \"lat\": 39, \"lon\": -179 }\n     }\n  } }\n}\n```\n\nIt seems that this code needs to either split the check into east and west checks or normalize the input values. Am I missing something?\n","id":"32418613","title":"geo_polygon not handling polygons that cross the date line properly","reopen_by":"nknize","opened_on":"2014-04-29T04:41:30Z","closed_by":"nknize"},{"number":"5705","comments":[{"date":"2014-04-07T19:32:39Z","author":"imotov","text":"This might be related to #5623.\n"},{"date":"2014-04-14T20:52:38Z","author":"s1monw","text":"@imotov can you try to verify with @alexeymedved that this is fixed now?\n"},{"date":"2014-04-15T00:16:31Z","author":"imotov","text":"@alexeymedved could you try reproducing this issue to see if it was fixed in the latest [nightly build](https:\/\/oss.sonatype.org\/content\/repositories\/snapshots\/org\/elasticsearch\/elasticsearch\/1.1.1-SNAPSHOT\/)? \n"},{"date":"2014-04-15T06:32:14Z","author":"alexey-medvedchikov","text":"@s1monw @imotov With elasticsearch-1.1.1-20140415.001904-55.deb seems everything works fine. Thank you!\n"},{"date":"2014-04-15T09:42:15Z","author":"alexey-medvedchikov","text":"I can break it again:\n- run long-running query like\n\n```\n\/_all\/_search {\n    \"facets\": {\n      \"terms\": {\n          \"terms\": {\n             \"script_field\": \"doc['request_time'].value <= 0.5 ? true : false\",\n             \"size\": 10\n          }\n      }\n    }\n}\n```\n- Insert big bunch of new items (by logstash for example)\n- Send SIGTERM\n\nVoila:\n\n```\nPOST \/logstash-2014.04.15.08\/_search\n{\n    \"script_fields\": {\n       \"s_request_time\": {\n          \"script\": \"doc['request_time'].getValues()\"\n       }\n    }\n}\n...\n         {\n            \"_index\": \"logstash-2014.04.15.08\",\n            \"_type\": \"nginx_accesslog\",\n            \"_id\": \"tjZ9fIqLQWedynr4mBhdEw\",\n            \"_score\": 1,\n            \"fields\": {\n               \"s_request_time\": [\n                  [\n                     \" \\u0001?Kc)}ymHZ\",\n                     \"$\\u000b|^\\u001aOoNlE\",\n                     \"(_eqT~|vd\",\n                     \",\\u0005~\/\\r'wg6\",\n                     \"0\/rxj?>;\",\n                     \"4\\u0002\u007F\\u0017FS{s\",\n                     \"8\\u0017y<5\\u001f_\",\n                     \"<\\u0001?Kc)}\",\n                     \"@\\u000b|^\\u001aO\",\n                     \"D_eqT\",\n                     \"H\\u0005~\/\\r\",\n                     \"L\/rx\",\n                     \"P\\u0002\u007F\\u0017\",\n                     \"T\\u0017y\",\n                     \"X\\u0001?\",\n                     \"\\\\\\u000b\"\n                  ]\n               ]\n            }\n         },\n...\n```\n"},{"date":"2014-04-15T11:03:27Z","author":"imotov","text":"@alexeymedved do you see any exceptions in the log files?\n"},{"date":"2014-04-15T11:29:41Z","author":"alexey-medvedchikov","text":"Exception for query like mentioned in my prev comment:\n\n```\n[2014-04-15 17:05:24,563][DEBUG][action.search.type       ] [Hobgoblin] [logstash-2014.04.14.10][3], node[JrhBvFHpRgyT5RXv8O9HwA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.\norg.elasticsearch.search.query.QueryPhaseExecutionException: [logstash-2014.04.14.10][3]: query[ConstantScore(*:*)],from[0],size[0]: Query Failed [Failed to execute main query]\n        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:127)\n        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:257)\n        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)\n        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)\n        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)\n        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)\n        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:724)\nCaused by: java.lang.RuntimeException: uncomparable values << ^A?WoNlEPrX>> and <<0.05>>\n        at org.elasticsearch.common.mvel2.math.MathProcessor.doOperationNonNumeric(MathProcessor.java:321)\n        at org.elasticsearch.common.mvel2.math.MathProcessor._doOperations(MathProcessor.java:234)\n        at org.elasticsearch.common.mvel2.math.MathProcessor.doOperations(MathProcessor.java:79)\n        at org.elasticsearch.common.mvel2.ast.BinaryOperation.getReducedValueAccelerated(BinaryOperation.java:114)\n        at org.elasticsearch.common.mvel2.MVELRuntime.execute(MVELRuntime.java:86)\n        at org.elasticsearch.common.mvel2.compiler.CompiledExpression.getDirectValue(CompiledExpression.java:123)\n        at org.elasticsearch.common.mvel2.compiler.CompiledExpression.getValue(CompiledExpression.java:119)\n        at org.elasticsearch.script.mvel.MvelScriptEngineService$MvelSearchScript.run(MvelScriptEngineService.java:191)\n        at org.elasticsearch.search.facet.terms.strings.ScriptTermsStringFieldFacetExecutor$Collector.collect(ScriptTermsStringFieldFacetExecutor.java:147)\n        at org.elasticsearch.common.lucene.search.FilteredCollector.collect(FilteredCollector.java:61)\n        at org.elasticsearch.common.lucene.MultiCollector.collect(MultiCollector.java:60)\n        at org.apache.lucene.search.Scorer.score(Scorer.java:65)\n        at org.apache.lucene.search.ConstantScoreQuery$ConstantScorer.score(ConstantScoreQuery.java:256)\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)\n        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:173)\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:309)\n        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:111)\n        ... 9 more\nCaused by: java.lang.ClassCastException: java.lang.Double cannot be cast to java.lang.String\n        at java.lang.String.compareTo(String.java:108)\n        at org.elasticsearch.common.mvel2.math.MathProcessor.doOperationNonNumeric(MathProcessor.java:318)\n        ... 25 more\n```\n\nExceptions appeared on restart mentioned in prev comment:\n\n```\n[2014-04-15 15:45:45,129][WARN ][index.engine.internal    ] [Condor] [logstash-2014.04.14.11][0] Searcher was released twice\norg.elasticsearch.ElasticsearchIllegalStateException: Double release\n        at org.elasticsearch.index.engine.internal.InternalEngine$EngineSearcher.release(InternalEngine.java:1510)\n        at org.elasticsearch.search.internal.DefaultSearchContext.release(DefaultSearchContext.java:212)\n        at org.elasticsearch.search.SearchService.freeContext(SearchService.java:551)\n        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:268)\n        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)\n        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)\n        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)\n        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)\n        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:724)\n```\n\nMaybe I can provide you a trace of execution or broken index file somehow?\n"},{"date":"2014-04-15T11:48:41Z","author":"imotov","text":"@alexeymedved was index logstash-2014.04.15.08 created with the nightly build or before you upgraded? Could you run `curl localhost:9200\/logstash-2014.04.15.08\/_mapping` and post the result here? Are you using percolator on this cluster?\n"},{"date":"2014-04-15T12:35:25Z","author":"alexey-medvedchikov","text":"@imotov \n- index was created by nightly build\n- https:\/\/gist.github.com\/alexeymedved\/dc7a3fb376e6f3c80f23#file-logstash-2014-04-15-08_mapping\n- no, i'm not using percolator\n"},{"date":"2014-04-15T13:43:22Z","author":"imotov","text":"@alexeymedved your mapping looks fine after restart, so I am thinking it might be actually an MVEL issue. For some reason MVEL thinks that it should compare two doubles as strings in your case. I tried to reproduce this issue locally but didn't succeed so far. Any chance you can try reproducing it using [javascript](https:\/\/github.com\/elasticsearch\/elasticsearch-lang-javascript) instead of MVEL?\n"},{"date":"2014-04-15T15:35:04Z","author":"alexey-medvedchikov","text":"MVEL just helps me to show what data actually we operating on. Sorry for misleading queries. It's not an MVEL issue for sure because of:\n\n> Another thing is when I'm using data_histogram facet (in kibana) I got ClassCastException[org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData.\n\nException for this kind of query:\n\n```\n[2014-04-15 15:02:49,538][DEBUG][action.search.type       ] [Condor] [logstash-2014.04.14.09][0], node[OS96wzBvT5WTcPgzKrAHlw], [P], s[STARTED]: Failed to execute [org.el\nasticsearch.action.search.SearchRequest@906a63f] lastShard [true]\norg.elasticsearch.search.SearchParseException: [logstash-2014.04.14.09][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"facets\":{\"0\":{\"date_histogram\":{\"k\ney_field\":\"@timestamp\",\"value_field\":\"request_time\",\"interval\":\"10m\"},\"global\":true,\"facet_filter\":{\"fquery\":{\"query\":{\"filtered\":{\"query\":{\"query_string\":{\"query\":\"type:\nnginx_accesslog  AND request:\\\\\/search\\\\?*\"}},\"filter\":{\"bool\":{\"must\":[{\"range\":{\"@timestamp\":{\"from\":1397462569513,\"to\":1397548969513}}}]}}}}}}}},\"size\":0}]]\n        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:634)\n        at org.elasticsearch.search.SearchService.createContext(SearchService.java:507)\n        at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:480)\n        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:252)\n        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)\n        at org.elasticsearch.action.search.type.TransportSearchCountAction$AsyncAction.sendExecuteFirstPhase(TransportSearchCountAction.java:70)\n        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)\n        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)\n        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:724)\nCaused by: java.lang.ClassCastException: org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData\n        at org.elasticsearch.search.facet.datehistogram.DateHistogramFacetParser.parse(DateHistogramFacetParser.java:188)\n        at org.elasticsearch.search.facet.FacetParseElement.parse(FacetParseElement.java:93)\n        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:622)\n        ... 11 more\n```\n"},{"date":"2014-04-15T16:00:56Z","author":"imotov","text":"@alexeymedved this is different. Here you are searching the yesterday's index `logstash-2014.04.14.09` that most likely contains data indexed while mapping for this index was messed up due to #5623. So you have numbers indexed as numbers and as strings. I would also suspect that mapping for this index is still messed up. Can you run `curl localhost:9200\/logstash-2014.04.14.09\/_mapping` to see if fields are mapped correctly? Can you also try reproducing this failure while searching only indices created with the nightly build and without using MVEL?\n"},{"date":"2014-04-18T14:09:32Z","author":"imotov","text":"That looks like a duplicate of #5623 after all. Please, feel free to reopen if you have any additional data points.\n"},{"date":"2014-04-21T06:52:07Z","author":"alexey-medvedchikov","text":"Well, I fixed this problem for myself by using long for time fields :)\n"},{"date":"2015-03-13T10:22:28Z","author":"UnderGreen","text":"Have same problem with Elastic 1.4.4. Query result without scripting:\n\n```\n{\n    \"took\": 7,\n    \"timed_out\": false,\n    \"_shards\": {\n        \"total\": 2,\n        \"successful\": 2,\n        \"failed\": 0\n    },\n    \"hits\": {\n        \"total\": 33,\n        \"max_score\": null,\n        \"hits\": [{\n            \"_index\": \"logstash-2015.03.13\",\n            \"_type\": \"gelf_photo\",\n            \"_id\": \"AUwSeR5Mf_J86mgo9hKZ\",\n            \"_score\": null,\n            \"_source\": {\n                \"version\": \"1.0\",\n                \"type\": \"gelf_photo\",\n                \"time_done_full\": \"0.0009758472442627\",\n                \"time_done\": 0.0,\n                \"@version\": \"1\",\n                \"@timestamp\": \"2015-03-13T09:31:09.067Z\",\n                \"_class__\": \"\",\n                \"_method__\": \"\",\n                \"_extra__\": \"null\",\n                \"_request_id__\": \"45e21fc5e190de6269567c57c5f2177f0009e188\"\n            },\n            \"sort\": [1426239069067]\n        }]\n    }\n}\n\n```\n\nQuery result with scripting:\n\n```\n{\n    \"took\": 11,\n    \"timed_out\": false,\n    \"_shards\": {\n        \"total\": 2,\n        \"successful\": 2,\n        \"failed\": 0\n    },\n    \"hits\": {\n        \"total\": 33,\n        \"max_score\": null,\n        \"hits\": [{\n            \"_index\": \"logstash-2015.03.13\",\n            \"_type\": \"gelf_photo\",\n            \"_id\": \"AUwSeR5Mf_J86mgo9hKZ\",\n            \"_score\": null,\n            \"fields\": {\n                \"s_time_done\": [\" \\u0001\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\"]\n            },\n            \"sort\": [1426239069067]\n        }]\n    }\n}\n```\n\nResult of`http:\/\/logstash.test:9200\/logstash-2015.03.13\/_mappings`:\n\n```\n...\n\"time_done\": {\n\"type\": \"double\"\n},\n...\n```\n"},{"date":"2015-04-05T19:54:12Z","author":"clintongormley","text":"@UnderGreen please can you open a new issue with all the details.  Although I have a sneaking suspicion that this is an issue with dynamic mappings being applied asynchronously. See https:\/\/github.com\/elastic\/elasticsearch\/issues\/8688 for more\n"}],"reopen_on":"2014-04-15T09:34:36Z","opened_by":"alexey-medvedchikov","closed_on":"2014-04-18T14:09:32Z","description":"Hello, first of all I want to say you making crazy useful things, guys, thank you!\n\nI have two elasticsearch twin (versions, configuration, everything) servers with the same data, replicated by logstash: ES1 and ES2. ES1 I restarted for a few hours ago and now I have a problems.\n\nMy mapping for logstash type, same for both servers.\n\n```\n\"nginx_accesslog\": {\n        \"dynamic_templates\": [\n                {\n                        \"string_template\": {\n                                \"mapping\": { \"index\": \"not_analyzed\", \"type\": \"string\" },\n                                \"match\": \"*\",\n                                \"match_mapping_type\": \"string\"\n                        }\n                }\n        ],\n        \"_all\": { \"enabled\": false },\n        \"_source\": { \"compress\": true },\n        \"properties\": { \n                \"@timestamp\": { \"type\": \"date\", \"format\": \"dateOptionalTime\" },\n                \"@version\": { \"type\": \"long\" },\n                \"api_key\": { \"type\": \"string\", \"index\": \"not_analyzed\" },\n                \"body_bytes_sent\": { \"type\": \"long\" },\n                \"host\": { \"type\": \"string\", \"index\": \"not_analyzed\" },\n                \"http_host\": { \"type\": \"string\", \"index\": \"not_analyzed\" },\n                \"http_method\": { \"type\": \"string\", \"index\": \"not_analyzed\" },\n                \"http_referer\": { \"type\": \"string\", \"index\": \"not_analyzed\" },\n                \"http_user_agent\": { \"type\": \"string\", \"index\": \"not_analyzed\" },\n                \"http_version\": { \"type\": \"string\", \"index\": \"not_analyzed\" },\n                \"http_x_forwarded_for\": { \"type\": \"string\", \"index\": \"not_analyzed\" },\n                \"message\": { \"type\": \"string\", \"index\": \"not_analyzed\" },\n                \"path\": { \"type\": \"string\", \"index\": \"not_analyzed\" },\n                \"remote_addr\": { \"type\": \"string\", \"index\": \"not_analyzed\" },\n                \"remote_user\": { \"type\": \"string\", \"index\": \"not_analyzed\" },\n                \"request\": { \"type\": \"string\", \"index\": \"not_analyzed\" },\n                \"request_time\": { \"type\": \"double\" },\n                \"status\": { \"type\": \"long\" },\n                \"tags\": { \"type\": \"string\", \"index\": \"not_analyzed\" },\n                \"type\": { \"type\": \"string\", \"index\": \"not_analyzed\" }\n        }\n}\n```\n\nQuery (I'm using Sense add-on)\n\n```\nPOST \/logstash-2014.04.07\/_search\n{\n    \"script_fields\": {\n       \"s_request_time\": {\n          \"script\": \"doc['request_time'].value\"\n       }\n    },\n    \"size\": 20\n}\n```\n\nES2, everything is normal:\n\n```\n{\n   \"took\": 63,\n   \"timed_out\": false,\n   \"_shards\": {\n      \"total\": 4,\n      \"successful\": 4,\n      \"failed\": 0\n   },\n   \"hits\": {\n      \"total\": 17041240,\n      \"max_score\": 1,\n      \"hits\": [\n         {\n            \"_index\": \"logstash-2014.04.07\",\n            \"_type\": \"nginx_accesslog\",\n            \"_id\": \"LSfAaBwSSDS5rL6utSHQJA\",\n            \"_score\": 1,\n            \"fields\": {\n               \"s_request_time\": 0.014\n            }\n         },\n...\n```\n\nES1: Ooops!\n\n```\n{\n   \"took\": 51,\n   \"timed_out\": false,\n   \"_shards\": {\n      \"total\": 4,\n      \"successful\": 4,\n      \"failed\": 0\n   },\n   \"hits\": {\n      \"total\": 17041131,\n      \"max_score\": 1,\n      \"hits\": [\n         {\n            \"_index\": \"logstash-2014.04.07\",\n            \"_type\": \"nginx_accesslog\",\n            \"_id\": \"tjZo1_JmRpOu5kE2WRfURw\",\n            \"_score\": 1,\n            \"fields\": {\n               \"s_request_time\": [\n                  \" \\u0001?PZ\\u000e+\\u0001\\u0003\\t\\u001c\" <-- WAT?!\n               ]\n            }\n         },\n```\n\nIt's most obvious demonstration of my problem. Another thing is when I'm using data_histogram facet (in kibana) I got ClassCastException[org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData.\n\nPurging all indexes fixes problem. Not only new, but old data too misenterpreted as arrays of one string.\nWell, I want my doubles back. :)\n","id":"30969037","title":"After restart elasticsearch reads double as array of string","reopen_by":"alexey-medvedchikov","opened_on":"2014-04-07T09:24:13Z","closed_by":"imotov"},{"number":"4978","comments":[{"date":"2014-02-03T09:47:17Z","author":"spinscale","text":"Hey,\n\ncan you please check, if the process started anyway? It should have been. The error is just a permission problem due to your VM, but should not prevent elasticsearch from starting usually.\n"},{"date":"2014-02-24T13:57:18Z","author":"dmikhaylov","text":"I've got the same issue, and on my VM it's not starting.\n"},{"date":"2014-02-24T14:01:22Z","author":"spinscale","text":"can you add `set -x` to the elasticsearch init script and paste the output from `\/etc\/init.d\/elasticsearch start` here?\n"},{"date":"2014-02-24T14:13:35Z","author":"dmikhaylov","text":"do you mean `\/etc\/init.d\/elasticsearch start set -x`?\n"},{"date":"2014-02-24T14:55:26Z","author":"spinscale","text":"sorry for not being clear. You can do the following: Open `\/etc\/init.d\/elasticsearch` in your favourite editor and change the current setup\n\n```\n#!\/bin\/sh\n#\n```\n\nto\n\n```\n#!\/bin\/sh\nset -x\n#\n```\n\nThen run `\/etc\/init.d\/elasticsearch start` and copy paste all that output into this ticket. You might want to comment out or remove that line again after that, so you do not get his verbose output all the time.\n\nThanks a lot for helping!\n"},{"date":"2014-02-24T16:49:29Z","author":"dmikhaylov","text":"- PATH=\/bin:\/usr\/bin:\/sbin:\/usr\/sbin\n- NAME=elasticsearch\n- DESC='ElasticSearch Server'\n- DEFAULT=\/etc\/default\/elasticsearch\n  ++ id -u\n- '[' 0 -ne 0 ']'\n- . \/lib\/lsb\/init-functions\n  ++ FANCYTTY=\n  ++ '[' -e \/etc\/lsb-base-logging.sh ']'\n  ++ . \/etc\/lsb-base-logging.sh\n  +++ LOG_DAEMON_MSG=\n- '[' -r \/etc\/default\/rcS ']'\n- . \/etc\/default\/rcS\n  ++ TMPTIME=0\n  ++ SULOGIN=no\n  ++ DELAYLOGIN=no\n  ++ UTC=yes\n  ++ VERBOSE=no\n  ++ FSCKFIX=no\n- ES_USER=elasticsearch\n- ES_GROUP=elasticsearch\n- JDK_DIRS='\/usr\/lib\/jvm\/java-7-oracle \/usr\/lib\/jvm\/java-7-openjdk \/usr\/lib\/jvm\/java-7-openjdk-amd64\/ \/usr\/lib\/jvm\/java-7-openjdk-armhf \/usr\/lib\/jvm\/java-7-openjdk-i386\/ \/usr\/lib\/jvm\/java-6-sun \/usr\/lib\/jvm\/java-6-openjdk \/usr\/lib\/jvm\/java-6-openjdk-amd64 \/usr\/lib\/jvm\/java-6-openjdk-armhf \/usr\/lib\/jvm\/java-6-openjdk-i386 \/usr\/lib\/jvm\/default-java'\n- for jdir in '$JDK_DIRS'\n- '[' -r \/usr\/lib\/jvm\/java-7-oracle\/bin\/java -a -z '' ']'\n- for jdir in '$JDK_DIRS'\n- '[' -r \/usr\/lib\/jvm\/java-7-openjdk\/bin\/java -a -z '' ']'\n- for jdir in '$JDK_DIRS'\n- '[' -r \/usr\/lib\/jvm\/java-7-openjdk-amd64\/\/bin\/java -a -z '' ']'\n- for jdir in '$JDK_DIRS'\n- '[' -r \/usr\/lib\/jvm\/java-7-openjdk-armhf\/bin\/java -a -z '' ']'\n- for jdir in '$JDK_DIRS'\n- '[' -r \/usr\/lib\/jvm\/java-7-openjdk-i386\/\/bin\/java -a -z '' ']'\n- for jdir in '$JDK_DIRS'\n- '[' -r \/usr\/lib\/jvm\/java-6-sun\/bin\/java -a -z '' ']'\n- for jdir in '$JDK_DIRS'\n- '[' -r \/usr\/lib\/jvm\/java-6-openjdk\/bin\/java -a -z '' ']'\n- JAVA_HOME=\/usr\/lib\/jvm\/java-6-openjdk\n- for jdir in '$JDK_DIRS'\n- '[' -r \/usr\/lib\/jvm\/java-6-openjdk-amd64\/bin\/java -a -z \/usr\/lib\/jvm\/java-6-openjdk ']'\n- for jdir in '$JDK_DIRS'\n- '[' -r \/usr\/lib\/jvm\/java-6-openjdk-armhf\/bin\/java -a -z \/usr\/lib\/jvm\/java-6-openjdk ']'\n- for jdir in '$JDK_DIRS'\n- '[' -r \/usr\/lib\/jvm\/java-6-openjdk-i386\/bin\/java -a -z \/usr\/lib\/jvm\/java-6-openjdk ']'\n- for jdir in '$JDK_DIRS'\n- '[' -r \/usr\/lib\/jvm\/default-java\/bin\/java -a -z \/usr\/lib\/jvm\/java-6-openjdk ']'\n- export JAVA_HOME\n- ES_HOME=\/usr\/share\/elasticsearch\n- MAX_OPEN_FILES=65535\n- LOG_DIR=\/var\/log\/elasticsearch\n- DATA_DIR=\/var\/lib\/elasticsearch\n- WORK_DIR=\/tmp\/elasticsearch\n- CONF_DIR=\/etc\/elasticsearch\n- CONF_FILE=\/etc\/elasticsearch\/elasticsearch.yml\n- MAX_MAP_COUNT=65535\n- '[' -f \/etc\/default\/elasticsearch ']'\n- . \/etc\/default\/elasticsearch\n- PID_FILE=\/var\/run\/elasticsearch.pid\n- DAEMON=\/usr\/share\/elasticsearch\/bin\/elasticsearch\n- DAEMON_OPTS='-p \/var\/run\/elasticsearch.pid -Des.default.config=\/etc\/elasticsearch\/elasticsearch.yml -Des.default.path.home=\/usr\/share\/elasticsearch -Des.default.path.logs=\/var\/log\/elasticsearch -Des.default.path.data=\/var\/lib\/elasticsearch -Des.default.path.work=\/tmp\/elasticsearch -Des.default.path.conf=\/etc\/elasticsearch'\n- export ES_HEAP_SIZE\n- export ES_HEAP_NEWSIZE\n- export ES_DIRECT_SIZE\n- export ES_JAVA_OPTS\n- test -x \/usr\/share\/elasticsearch\/bin\/elasticsearch\n- case \"$1\" in\n- checkJava\n- '[' -x \/usr\/lib\/jvm\/java-6-openjdk\/bin\/java ']'\n- JAVA=\/usr\/lib\/jvm\/java-6-openjdk\/bin\/java\n- '[' '!' -x \/usr\/lib\/jvm\/java-6-openjdk\/bin\/java ']'\n- '[' -n '' -a -z '' ']'\n- log_daemon_msg 'Starting ElasticSearch Server'\n- '[' -z 'Starting ElasticSearch Server' ']'\n- log_use_fancy_output\n- TPUT=\/usr\/bin\/tput\n- EXPR=\/usr\/bin\/expr\n- '[' -t 1 ']'\n- '[' xxterm '!=' x ']'\n- '[' xxterm '!=' xdumb ']'\n- '[' -x \/usr\/bin\/tput ']'\n- '[' -x \/usr\/bin\/expr ']'\n- \/usr\/bin\/tput hpa 60\n- \/usr\/bin\/tput setaf 1\n- '[' -z ']'\n- FANCYTTY=1\n- case \"$FANCYTTY\" in\n- true\n- \/usr\/bin\/tput xenl\n  ++ \/usr\/bin\/tput cols\n- COLS=80\n- '[' 80 ']'\n- '[' 80 -gt 6 ']'\n  ++ \/usr\/bin\/expr 80 - 7\n- COL=73\n- log_use_plymouth\n- '[' n = y ']'\n- plymouth --ping\n- printf ' \\* Starting ElasticSearch Server       '\n  - Starting ElasticSearch Server       ++ \/usr\/bin\/expr 80 - 1\n- \/usr\/bin\/tput hpa 79\n                                                                             + printf ' '\n  ++ pidofproc -p \/var\/run\/elasticsearch.pid elasticsearch\n  ++ local pidfile line i pids= status specified pid\n  ++ pidfile=\n  ++ specified=\n  ++ OPTIND=1\n  ++ getopts p: opt\n  ++ case \"$opt\" in\n  ++ pidfile=\/var\/run\/elasticsearch.pid\n  ++ specified=1\n  ++ getopts p: opt\n  ++ shift 2\n  ++ base=elasticsearch\n  ++ '[' '!' 1 ']'\n  ++ '[' -n \/var\/run\/elasticsearch.pid -a -r \/var\/run\/elasticsearch.pid ']'\n  ++ read pid\n  ++ '[' -n 16987 ']'\n  +++ kill -0 16987\n  ++ ps 16987\n  ++ return 1\n- pid=\n- '[' -n '' ']'\n- mkdir -p \/var\/log\/elasticsearch \/var\/lib\/elasticsearch \/tmp\/elasticsearch\n- chown elasticsearch:elasticsearch \/var\/log\/elasticsearch \/var\/lib\/elasticsearch \/tmp\/elasticsearch\n- touch \/var\/run\/elasticsearch.pid\n- chown elasticsearch:elasticsearch \/var\/run\/elasticsearch.pid\n- '[' -n 65535 ']'\n- ulimit -n 65535\n- '[' -n '' ']'\n- '[' -n 65535 ']'\n- sysctl -q -w vm.max_map_count=65535\n  error: permission denied on key 'vm.max_map_count'\n- start-stop-daemon --start -b --user elasticsearch -c elasticsearch --pidfile \/var\/run\/elasticsearch.pid --exec \/usr\/share\/elasticsearch\/bin\/elasticsearch -- -p \/var\/run\/elasticsearch.pid -Des.default.config=\/etc\/elasticsearch\/elasticsearch.yml -Des.default.path.home=\/usr\/share\/elasticsearch -Des.default.path.logs=\/var\/log\/elasticsearch -Des.default.path.data=\/var\/lib\/elasticsearch -Des.default.path.work=\/tmp\/elasticsearch -Des.default.path.conf=\/etc\/elasticsearch\n- log_end_msg 0\n- '[' -z 0 ']'\n- '[' 73 ']'\n- '[' -x \/usr\/bin\/tput ']'\n- log_use_plymouth\n- '[' n = y ']'\n- plymouth --ping\n- printf '\\r'\n- \/usr\/bin\/tput hpa 73\n                                                                       + '[' 0 -eq 0 ']'\n- echo '[ OK ]'\n  [ OK ]\n- return 0\n- exit 0\n"},{"date":"2014-02-24T17:21:49Z","author":"spinscale","text":"can you run `ps p $(cat \/var\/run\/elasticsearch.pid)` - this actually looks as if elasticsearch was started...\n"},{"date":"2014-02-25T13:22:36Z","author":"dmikhaylov","text":"Returns just header, i've tried `service elasticsearch status`  and it says `* elasticsearch is not running`.\n"},{"date":"2014-02-25T14:29:27Z","author":"spinscale","text":"Can you make sure elasticsearch does not run, and try this on the commandline (as root!):\n\n```\ntouch \/var\/run\/elasticsearch.pid\nchown elasticsearch:elasticsearch \/var\/run\/elasticsearch.pid\nstart-stop-daemon -v --start --user elasticsearch -c elasticsearch --pidfile \/var\/run\/elasticsearch.pid --exec \/usr\/share\/elasticsearch\/bin\/elasticsearch -- -p \/var\/run\/elasticsearch.pid -Des.default.config=\/etc\/elasticsearch\/elasticsearch.yml -Des.default.path.home=\/usr\/share\/elasticsearch -Des.default.path.logs=\/var\/log\/elasticsearch -Des.default.path.data=\/var\/lib\/elasticsearch -Des.default.path.work=\/tmp\/elasticsearch -Des.default.path.conf=\/etc\/elasticsearch\n```\n\nand paste the output here?\n"},{"date":"2014-02-25T17:24:38Z","author":"dmikhaylov","text":"{0.90.8}: Initialization Failed ...\n- ElasticSearchIllegalStateException[Failed to obtain node lock, is the following location writable?: [\/var\/data\/elasticsearch\/hostname]]\n  IOException[failed to obtain lock on \/var\/data\/elasticsearch\/hostname\/nodes\/49]\n      IOException[Cannot create directory: \/var\/data\/elasticsearch\/hostname\/nodes\/49]\n  Turns out I just forgot to create `\/var\/data` directory. Now everything works. Thank you, @spinscale.\n"},{"date":"2014-02-26T07:32:07Z","author":"spinscale","text":"@empirik One last question: Did this message also occur in the log file at `\/var\/log\/elasticsearch` or was it just printed to stdout? Would be awesome if you could check it!\n"},{"date":"2014-02-26T18:16:14Z","author":"dmikhaylov","text":"@spinscale I don't see this message in logs.\n"},{"date":"2014-03-16T23:53:59Z","author":"derEremit","text":"Just for the record. Getting same error on startup in openvz, but elasticsearch runs without problems for now. A bit irritating as normaly a service start error means the service won't run. As opposed to a warning.\nI expect without correct vm.max_map_count I should expect \"only\" performace problems?\n"},{"date":"2014-04-25T19:51:32Z","author":"spinscale","text":"I dont know openvz enough, but not setting this setting can also result in lucene exceptions and indexing problems, it is not only about performance here - I dont how openvz is handling this. Can you get us any insight here @derEremit?\n"},{"date":"2014-05-12T02:07:15Z","author":"13h3r","text":"Got the same on openvz\n"},{"date":"2014-05-14T08:53:35Z","author":"kritik","text":"I see the same message on big index but after allocating more memmory elasticsearch starts at least :) Using OpenVZ too and elasticsearch 1.1.1.\n"},{"date":"2014-06-03T13:00:04Z","author":"idanov","text":"Got the same on OpenVZ and elasticsearch 1.2.0.\n"},{"date":"2014-06-10T21:59:03Z","author":"r0mdau","text":"Got the same on OpenVZ and elasticsearch 1.2.0\nTemplate : [debian-7.0-x86_64.tar.gz](http:\/\/download.openvz.org\/template\/precreated\/debian-7.0-x86_64.tar.gz)\nHost Server : Proxmox 3.1-21\n"},{"date":"2014-06-11T14:58:31Z","author":"derEremit","text":"Basically openvz does not allow modifications of kernel parameters as these would affect the host and every other guest machine.\nI think that setting should be removed from init scripts and put in a README.\nThe init script could check for the correct setting and output a warning but not set these kernel parameters itself!\n"},{"date":"2014-06-26T13:23:59Z","author":"joke2k","text":":+1: \n"},{"date":"2014-07-02T17:51:44Z","author":"scamianbas","text":"Hi all,\nkinda newbie here.\nI'm using Proxmox, Ubuntu OpenVZ container, Elasticsearch 1.2.1.\nI had the same service starting issues and I fixed that by removing the \"-d\" option in DAEMON_OPTS (a directory path shoud have been provided here)\nAlso I changed the line \nif [ -n \"$MAX_MAP_COUNT\" ]\nwith\nif [ -n \"$MAX_MAP_COUNT\" ] && [ ! -f \/proc\/sys\/vm\/max_map_count ]\nto address the unwanted sysctl issue.\nHope it helps.\n"},{"date":"2014-08-10T13:58:01Z","author":"adeleglise","text":"Hi all,\n\nThanks to scamianbas for his tips, I tested this on proxmox 3.2-4, debian7-x64 template, works like a charm !\n\nThanks to the elasticsearch guys too for this great piece of software ;)\n"},{"date":"2014-08-12T10:28:36Z","author":"andykillen","text":"FYI:  fresh install of Unbuntu 12.4 with elastic search 1.3.1 over the top, following this Gist (easy way) for install https:\/\/gist.github.com\/wingdspur\/2026107\n\nI get the error\n- Starting Elasticsearch Server  \n                                                                                                                                                                  error: permission denied on key 'vm.max_map_count'\n\nwhen I do a ps check, I get\n\nps p $(cat \/var\/run\/elasticsearch.pid)\n  PID TTY      STAT   TIME COMMAND\n 8246 ?        Sl     0:08 \/usr\/lib\/jvm\/java-7-openjdk-i386\/\/bin\/java -Xms256m -Xmx1g -Xss256k -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX\n\nand finally when doing \"service elasticsearch status\"\n\nit says it is running. \n\nSo the error has a slightly different prefix (error: and not sysctl:) but as with other information above it is not stopping the server.   If needed I have the screen output with set -x in the elasticsearch config file. \n"},{"date":"2014-09-02T14:29:43Z","author":"grepwood","text":"I'm having the same issue on CentOS 6.5 with elasticsearch 1.3.2 in OpenVZ.\nps p $(cat \/var\/run\/elasticsearch\/elasticsearch.pid)\n  PID TTY      STAT   TIME COMMAND\n 1132 ?        Sl     0:05 \/usr\/bin\/java -Xms256m -Xmx1g -Xss256k -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Delastics\nJust like andykillen, the service is running after all.\n"},{"date":"2014-09-30T12:38:16Z","author":"deric","text":"Same issue with LXC and Ubuntu 14.04\n\n```\nsysctl: permission denied on key 'vm.max_map_count'\n```\n\nthough elasticsearch is running, is it a huge problem for production environment?\n"},{"date":"2014-10-29T16:09:29Z","author":"karmux","text":"I also get this error on VPS running Ubuntu 14.04 when I (re)start Elasticsearch but is running at least.\n\n```\nsysctl: permission denied on key 'vm.max_map_count'\n```\n"},{"date":"2014-11-13T15:05:03Z","author":"sts","text":"Can someone please merge this? This works! :+1: :shipit: \n\n`if [ -n \"$MAX_MAP_COUNT\" ]`\nwith\n`if [ -n \"$MAX_MAP_COUNT\" ] && [ ! -f \/proc\/sys\/vm\/max_map_count ]`\n"},{"date":"2014-11-13T15:10:39Z","author":"spinscale","text":"@sts, shouldnt the check be vice versa and only done if the file exists and thus without the negation? I am confused or maybe misunderstanding the intent\n"},{"date":"2014-11-17T15:57:46Z","author":"alexgarel","text":"@sts to avoid the warning you can also edit `\/etc\/default\/elasticsearch` to set `MAX_MAP_COUNT` with an empty value, no need to fix the startup script.\n"},{"date":"2014-11-18T10:01:58Z","author":"grepwood","text":"@alexgarel I think it'd be very nice of ES to construct `\/etc\/default\/elasticsearch` that has `MAX_MAP_COUNT` set to a null value when an offending environment like OpenVZ is detected as the host of ES, or otherwise, whatever it is meant to be set to.\n"}],"reopen_on":"2015-10-30T13:19:11Z","opened_by":"bline79","closed_on":"2016-11-06T11:53:47Z","description":"Hello,\n\nElasticsearch 0.90.9 and higher do not work with openvz (tested with debian wheezy i386) with the following error showing on startup:\n\nStarting ElasticSearch Server:sysctl: permission denied on key 'vm.max_map_count'  \n","id":"26721868","title":"sysctl: permission denied on key 'vm.max_map_count'  - OpenVZ Elasticsearch 0.90.9 compatibility issue","reopen_by":"clintongormley","opened_on":"2014-01-31T22:52:05Z","closed_by":"clintongormley"},{"number":"4679","comments":[{"date":"2014-01-10T07:02:58Z","author":"dadoonet","text":"Which version are you using?\n"},{"date":"2014-01-10T19:28:45Z","author":"govindm","text":"version: {\nnumber: \"0.90.2\",\nsnapshot_build: false,\nlucene_version: \"4.3.1\"\n},\n"},{"date":"2014-01-10T19:35:03Z","author":"s1monw","text":"that is a pretty old version given the fact that the has_child has been rewritten twice since then. I guess it's wise to upgrade to the latest version. \n"},{"date":"2014-01-10T19:42:31Z","author":"govindm","text":"which is the stable latest version that I can move to?\n"},{"date":"2014-01-10T21:26:54Z","author":"s1monw","text":"http:\/\/www.elasticsearch.org\/download\/ `0.90.10`\n"},{"date":"2014-01-10T22:45:59Z","author":"govindm","text":"Thanks a lot.\n"},{"date":"2014-01-12T06:42:35Z","author":"govindm","text":"I upgraded from 0.90.2 to 0.90.10 and I still see the NPE.\n\nRemoteTransportException[[i-72d6065c][inet[\/10.150.0.70:7102]][search\/phase\/query]]; nested: QueryPhaseExecutionException[[mmd][9]: query[filtered(filtered(movieId:[\\* TO 0])->+CustomQueryWrappingFilter(child_filter[flags\/movie](filtered%28countryCode:US%29->cache%28_type:flags%29)))->cache(_type:movie)],from[0],size[1]: Query Failed [Failed to execute main query]]; nested: RuntimeException[java.lang.NullPointerException]; nested: NullPointerException; }]\n"},{"date":"2014-01-12T06:57:43Z","author":"govindm","text":"Looks like similar issue(https:\/\/github.com\/elasticsearch\/elasticsearch\/issues\/3965) reported earlier in 0.90.5 and the issue is closed stating it will be addressed in the next release (90.6) but this issue is still showing in 0.90.10\n"},{"date":"2014-01-12T09:05:02Z","author":"dadoonet","text":"Could you post the full stacktrace please?\n"},{"date":"2014-01-14T12:05:36Z","author":"martijnvg","text":"@govindm Does it reproduce easily? Also can you share your query that you execute?\n"},{"date":"2014-01-15T00:47:55Z","author":"govindm","text":"This problem occurs often, but I couldn't find a pattern triggers it. Also the exception is logged only in debug mode and not in info level. I will try to bounce the ES server in debug mode and capture the stack trace.\n"},{"date":"2014-01-15T01:24:43Z","author":"govindm","text":"[2014-01-15 01:22:12,194][DEBUG][org.elasticsearch.action.search.type] [i-88c9a8a8] [51754] Failed to execute query phase\norg.elasticsearch.transport.RemoteTransportException: [i-b059f291][inet[\/10.166.46.221:7102]][search\/phase\/scan\/scroll]\nCaused by: org.elasticsearch.search.query.QueryPhaseExecutionException: [mmd][8]: query[filtered(filtered(+(countryCode:US) +(movieId:[70140365 TO 70140365] movieId:[60023248 TO 60023248]))->CustomQueryWr\nappingFilter(parent_filter[movie](filtered%28movieTypeCode:show movieTypeCode:standalone%29->cache%28_type:movie%29)))->cache(_type:flags)],from[0],size[10000]: Query Failed [Failed to execute main query]\n        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:121)\n        at org.elasticsearch.search.SearchService.executeScan(SearchService.java:215)\n        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchScanScrollTransportHandler.messageReceived(SearchServiceTransportAction.java:791)\n        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchScanScrollTransportHandler.messageReceived(SearchServiceTransportAction.java:780)\n        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:724)\nCaused by: java.lang.NullPointerException\n        at org.elasticsearch.index.search.child.ParentConstantScoreQuery$ChildrenWeight$ChildrenDocIdIterator.match(ParentConstantScoreQuery.java:176)\n        at org.apache.lucene.search.FilteredDocIdSetIterator.advance(FilteredDocIdSetIterator.java:71)\n        at org.elasticsearch.index.search.child.ConstantScorer.advance(ConstantScorer.java:70)\n        at org.apache.lucene.search.FilteredQuery$LeapFrogScorer.advanceToNextCommonDoc(FilteredQuery.java:268)\n        at org.apache.lucene.search.FilteredQuery$LeapFrogScorer.advance(FilteredQuery.java:292)\n        at org.apache.lucene.search.FilteredQuery$LeapFrogScorer.score(FilteredQuery.java:247)\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)\n        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:167)\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:309)\n        at org.elasticsearch.search.scan.ScanContext.execute(ScanContext.java:50)\n        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:108)\n        ... 7 more\n"},{"date":"2014-01-15T09:24:26Z","author":"martijnvg","text":"The stacktrace contains valuable information and I think this bug is related to #4703. I can reproduce the same NPE if I use the has_parent filter in the scan api. I have a fix for it and hopefully get it in soon.\n"},{"date":"2014-01-15T22:22:59Z","author":"govindm","text":"Thanks a lot Martijn\n"},{"date":"2014-02-03T23:29:18Z","author":"govindm","text":"I am getting NPE in non scan search type as well.\n\n[2014-02-03 23:11:34,466][DEBUG][org.elasticsearch.action.search.type] [i-45d2a865] [mmd][3], node[LoO5A_v4TEaJHGgWUkjfDA], [R], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@4c9c6877]\norg.elasticsearch.transport.RemoteTransportException: [i-05ec6c24][inet[\/10.166.31.158:7102]][search\/phase\/query]\nCaused by: org.elasticsearch.search.query.QueryPhaseExecutionException: [mmd][3]: query[filtered(ConstantScore(child_filter[character_asset\/jfk_character](+%28%28spaceCode:creative-services%29 %28sharedSpaceCodes:creative-services-originals%29%29 -deleted:true)))->cache(_type:jfk_character)],from[0],size[100]: Query Failed [failed to execute context rewrite]\n    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:99)\n    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:243)\n    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:529)\n    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:518)\n    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:269)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:724)\nCaused by: java.lang.NullPointerException\n"},{"date":"2014-02-03T23:30:18Z","author":"govindm","text":"ES version \n\nversion: {\nnumber: \"0.90.3\",\nbuild_hash: \"5c38d6076448b899d758f29443329571e2522410\",\nbuild_timestamp: \"2013-08-06T13:18:31Z\",\nbuild_snapshot: false,\nlucene_version: \"4.4\"\n}\n"},{"date":"2014-02-10T19:54:01Z","author":"martijnvg","text":"There were various bugs fixed in the last months. I don't think this error occurs when you upgrade to ES `0.90.11`.\n"},{"date":"2014-04-18T07:29:16Z","author":"martijnvg","text":"Fixed in recent versions.\n"}],"reopen_on":"2014-01-12T06:58:35Z","opened_by":"govindm","closed_on":"2014-04-18T07:29:16Z","description":")],from[0],size[1]: Query Failed [failed to execute context rewrite]\n    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:99)\n    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:239)\n    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:529)\n    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:518)\n    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:265)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:722)\nCaused by: java.lang.NullPointerException\n^C\n\nAnd once a while i see the following\n\n> cache(_type:movie)],from[0],size[1]: Query Failed [failed to execute context rewrite]\n> at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:99)\n> at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:239)\n> at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:529)\n> at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:518)\n> at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:265)\n> at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n> at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n> at java.lang.Thread.run(Thread.java:722)\n> Caused by: java.lang.NullPointerException\n> at org.elasticsearch.index.cache.id.simple.SimpleIdCache.refresh(SimpleIdCache.java:210)\n> at org.elasticsearch.index.search.child.HasChildFilter.contextRewrite(HasChildFilter.java:118)\n> at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:96)\n> ... 7 more\n","id":"25374068","title":"NPE while doing haschild query, intermittently","reopen_by":"govindm","opened_on":"2014-01-10T04:01:31Z","closed_by":"martijnvg"},{"number":"4511","comments":[{"date":"2013-12-18T22:30:06Z","author":"s1monw","text":"Hey @seallison can you gimme more information what you mean by template mapping and maybe provide a small recreation gist for the problem you see?\n"},{"date":"2013-12-18T22:54:56Z","author":"spinscale","text":"Hey,\n\nthere were a couple of bugs fixed for 0.90.8 regarding file based mapping template loading - actually template loading by file was not working until 0.90.8 in a couple of previous 0.90 releases (wondering that this has worked for you so far). I am very happy to help and debug the issue with you, if you help with a bit more information.\n"},{"date":"2013-12-18T23:32:16Z","author":"seallison","text":"This is an example of what I'm talking about:  https:\/\/gist.github.com\/seallison\/8031640\n\nLet me know if you need any additional information. Thanks!\n"},{"date":"2013-12-18T23:38:12Z","author":"seallison","text":"@spinscale all of my mappings have been in templates that I construct like in my gist. I've deployed this using 0.90.0, 0.90.3, and 0.90.5 without issue.\n"},{"date":"2013-12-19T09:49:21Z","author":"spinscale","text":"I can see why it happens. A quick fix is to remove the most outer data structure named `template_testlocations` and directly start with `template` like this:\n\n```\n{\n        \"template\": \"locations*\",\n        \"mappings\": {\n            \"locations\": {\n                \"_source\": {\n                    \"compress\": true\n                },\n                \"_all\": {\n                    \"enabled\": false\n                },\n                \"properties\": {\n                    \"contactPersonId\" : {\n                        \"index_analyzer\": \"keyword\",\n                        \"type\" : \"string\"\n                    },\n                    \"state\": {\n                        \"index_analyzer\": \"keyword\",\n                        \"type\": \"string\"\n                    }\n                }\n            }\n        }\n    }\n```\n"},{"date":"2013-12-22T20:58:17Z","author":"s1monw","text":"We gonna revert the revert to make sure people that were on `0.90.8` and move to the next version will have a better user experience if we support both formats hence I reopened https:\/\/github.com\/elasticsearch\/elasticsearch\/pull\/4517\n"}],"reopen_on":"2013-12-22T20:58:17Z","opened_by":"seallison","closed_on":"2013-12-22T21:12:25Z","description":"I have templates that define specific mappings. In testing 0.90.8 today, I noticed that the template mappings are not being used when I load data in to an index that matches the template.  This has worked in all previous versions of ElasticSearch that I have used (0.19x -> 0.90.7).\n","id":"24518909","title":"template mappings are not loading in 0.90.8","reopen_by":"s1monw","opened_on":"2013-12-18T21:12:57Z","closed_by":"spinscale"},{"number":"4356","comments":[{"date":"2013-12-06T13:52:54Z","author":"s1monw","text":"can you please ask this question as it is on the mailing list. This is a bug tracker while this seems to be a question rather than a bug. \n\nthanks,\n\nsimon\n"},{"date":"2013-12-06T15:01:52Z","author":"clintongormley","text":"@s1monw Actually I've found two bugs in the `query_string` query from the above.  More clearly demonstrated here:\n\n```\ncurl -XPUT \"http:\/\/localhost:9200\/test\/product\/1\" -d'\n{\n   \"desc\": \"one two three\"\n}'\n\ncurl -XPUT \"http:\/\/localhost:9200\/test\/customer\/2\" -d'\n{\n   \"desc\": \"one two three\"\n}'\n```\n\nFirst bug: Setting the default field to `customer.field` does not limit the results to just docs of type `customer`.  No type filter is added, and both docs are returned:\n\n```\ncurl -XPOST \"http:\/\/localhost:9200\/test\/_search\" -d'\n{\n   \"query\": {\n      \"query_string\": {\n         \"default_field\": \"customer.desc\",\n         \"query\": \"\\\"one three\\\"~5\"\n      }\n   }\n}'\n```\n\nResults:\n\n```\n  \"hits\": [\n     {\n        \"_index\": \"test\",\n        \"_type\": \"customer\",\n        \"_id\": \"2\",\n        \"_score\": 0.2169777,\n        \"_source\": {\n           \"desc\": \"one two three\"\n        }\n     },\n     {\n        \"_index\": \"test\",\n        \"_type\": \"product\",\n        \"_id\": \"1\",\n        \"_score\": 0.2169777,\n        \"_source\": {\n           \"desc\": \"one two three\"\n        }\n     }\n  ]\n```\n\nWhen `customer.desc` is specified using `fields`, the type filter is correctly applied:\n\n```\ncurl -XPOST \"http:\/\/localhost:9200\/test\/_search\" -d'\n{\n   \"query\": {\n      \"query_string\": {\n         \"fields\": [\n            \"customer.desc\"\n         ],\n         \"query\": \"\\\"one two\\\"~5\"\n      }\n   }\n}'\n```\n\nThis returns:\n\n```\n  \"hits\": [\n     {\n        \"_index\": \"test\",\n        \"_type\": \"customer\",\n        \"_id\": \"2\",\n        \"_score\": 0.30685282,\n        \"_source\": {\n           \"desc\": \"one two three\"\n        }\n     }\n  ]\n```\n\nHowever, when the documents _requires_ `slop` on the phrase in order to match, nothing is returned:\n\n```\ncurl -XPOST \"http:\/\/localhost:9200\/test\/_search\" -d'\n{\n   \"query\": {\n      \"query_string\": {\n         \"fields\": [\n            \"customer.desc\"\n         ],\n         \"query\": \"\\\"one three\\\"~5\"\n      }\n   }\n}'\n```\n"},{"date":"2013-12-06T21:39:44Z","author":"s1monw","text":"Thanks @clintongormley for reopening. I must would have missed it! Sorry for jumping to conclusion so quickly.\nI added tests and fixed the problem in the attached the commit. I will open a PR soonish\n"}],"reopen_on":"2013-12-06T15:01:52Z","opened_by":"spaisey","closed_on":"2013-12-06T22:11:11Z","description":"When running a simple query_string search, specifying the search fields (prefixed with the type), the search returns results as expected.\nWhen running a proximity search query_string, using the same search fields, the search fails. If I remove the type prefix from the search field name, the search works.\n# EXAMPLE:\n## Setup:\n\ncurl -XPUT 'http:\/\/localhost:9201\/test\/product\/1' -d '{ \n    \"desc\": \"description of product one with something\" \n}'\ncurl -XPUT 'http:\/\/localhost:9200\/test\/product\/2' -d '{ \n    \"desc\": \"description of product two with something else\" \n}'\ncurl -XPUT 'http:\/\/localhost:9200\/test\/product\/3' -d '{ \n    \"desc\": \"description of product three with something else again\" \n}'\ncurl -XPUT 'http:\/\/localhost:9200\/test\/customer\/1' -d '{ \n    \"desc\": \"description of customer one with something\" \n}'\ncurl -XPUT 'http:\/\/localhost:9200\/test\/customer\/2' -d '{ \n    \"desc\": \"description of customer two with something else\" \n}'\ncurl -XPUT 'http:\/\/localhost:9200\/test\/customer\/3' -d '{ \n    \"desc\": \"description of customer three with something else again\" \n}'\n## Simple Search:\n\ncurl -XPOST 'http:\/\/localhost:9200\/test\/_search?pretty=true' -d '{\n  \"query\" : {\n      \"query_string\" : {\n          \"query\" : \"description\",\n          \"fields\" : [\"customer.desc\", \"product.desc\"]\n      }\n  }\n}'\n## Failing proximity search:\n\ncurl -XPOST 'http:\/\/localhost:9200\/test\/_search?pretty=true' -d '{\n  \"query\" : {\n      \"query_string\" : {\n          \"query\" : \"\\\"customer else\\\"~5\",\n          \"fields\" : [\"customer.desc\", \"product.desc\"]\n      }\n  }\n}'\n## Successful proximity search:\n\ncurl -XPOST 'http:\/\/localhost:9200\/test\/_search?pretty=true' -d '{\n  \"query\" : {\n      \"query_string\" : {\n          \"query\" : \"\\\"customer else\\\"~5\",\n          \"fields\" : [\"desc\"]\n      }\n  }\n}'\n\nWe want to be able to allow the user to search specific fields, hence prefixing the 'desc' field with either 'customer' or 'product' (our real ES instance has many 'title' and 'desc' fields, so we need to prefix these fields with the type). Is this an incorrect use of fields, or a defect with proximity search?\n\nJava: 1.7.0_45\nES: 90.5\nOS: Windows 7\n","id":"23848527","title":"Inconsistent search results when running proximity searches","reopen_by":"clintongormley","opened_on":"2013-12-06T10:22:07Z","closed_by":"s1monw"},{"number":"4093","comments":[{"date":"2013-11-05T17:11:17Z","author":"clintongormley","text":"Hiya\n\nYes, this is a known issue, see #4078 \n\nWe will be releasing version 0.90.7 soon, which will fix this.  In the meantime, you can add this to your config:\n\n```\nindex.warmer.enabled: false\n```\n"},{"date":"2013-11-05T17:19:54Z","author":"clintongormley","text":"(at least I assume this is the same thing)\n\nCould you try the setting above and let me know if it fixed the issue?\n\nthanks\n"},{"date":"2013-11-05T17:30:54Z","author":"brackxm","text":"No, that setting does not fix the issue.\n"},{"date":"2013-11-05T17:33:41Z","author":"clintongormley","text":"Could you give us some idea of what your tests do and how you configure elasticsearch?\n"},{"date":"2013-11-05T17:34:07Z","author":"brackxm","text":"stacktrace\n\n```\njava.lang.OutOfMemoryError: Direct buffer memory\n    at java.nio.Bits.reserveMemory(Bits.java:658)\n    at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:123)\n    at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:306)\n    at org.apache.lucene.store.bytebuffer.PlainByteBufferAllocator.allocate(PlainByteBufferAllocator.java:55)\n    at org.apache.lucene.store.bytebuffer.CachingByteBufferAllocator.allocate(CachingByteBufferAllocator.java:52)\n    at org.elasticsearch.cache.memory.ByteBufferCache.allocate(ByteBufferCache.java:101)\n    at org.apache.lucene.store.bytebuffer.ByteBufferIndexOutput.switchCurrentBuffer(ByteBufferIndexOutput.java:106)\n    at org.apache.lucene.store.bytebuffer.ByteBufferIndexOutput.writeBytes(ByteBufferIndexOutput.java:93)\n    at org.elasticsearch.common.lucene.store.BufferedChecksumIndexOutput.flushBuffer(BufferedChecksumIndexOutput.java:69)\n    at org.apache.lucene.store.BufferedIndexOutput.flushBuffer(BufferedIndexOutput.java:113)\n    at org.apache.lucene.store.BufferedIndexOutput.flush(BufferedIndexOutput.java:102)\n    at org.elasticsearch.common.lucene.store.BufferedChecksumIndexOutput.flush(BufferedChecksumIndexOutput.java:80)\n    at org.apache.lucene.store.BufferedIndexOutput.close(BufferedIndexOutput.java:126)\n    at org.elasticsearch.common.lucene.store.BufferedChecksumIndexOutput.close(BufferedChecksumIndexOutput.java:60)\n    at org.elasticsearch.index.store.Store$StoreIndexOutput.close(Store.java:587)\n    at org.apache.lucene.util.IOUtils.close(IOUtils.java:140)\n    at org.apache.lucene.codecs.lucene41.Lucene41PostingsWriter.close(Lucene41PostingsWriter.java:582)\n    at org.apache.lucene.util.IOUtils.closeWhileHandlingException(IOUtils.java:81)\n    at org.apache.lucene.codecs.BlockTreeTermsWriter.close(BlockTreeTermsWriter.java:1082)\n    at org.elasticsearch.index.codec.postingsformat.BloomFilterPostingsFormat$BloomFilteredFieldsConsumer.close(BloomFilterPostingsFormat.java:408)\n    at org.elasticsearch.index.codec.postingsformat.ElasticSearch090PostingsFormat$1.close(ElasticSearch090PostingsFormat.java:63)\n    at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat$FieldsConsumerAndSuffix.close(PerFieldPostingsFormat.java:86)\n    at org.apache.lucene.util.IOUtils.close(IOUtils.java:163)\n    at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat$FieldsWriter.close(PerFieldPostingsFormat.java:154)\n    at org.apache.lucene.util.IOUtils.close(IOUtils.java:140)\n    at org.apache.lucene.index.FreqProxTermsWriter.flush(FreqProxTermsWriter.java:102)\n    at org.apache.lucene.index.TermsHash.flush(TermsHash.java:116)\n    at org.apache.lucene.index.DocInverter.flush(DocInverter.java:53)\n    at org.apache.lucene.index.DocFieldProcessor.flush(DocFieldProcessor.java:81)\n    at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:466)\n    at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:499)\n    at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:609)\n    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:367)\n    at org.apache.lucene.index.StandardDirectoryReader.doOpenFromWriter(StandardDirectoryReader.java:277)\n    at org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:252)\n    at org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:242)\n    at org.apache.lucene.index.DirectoryReader.openIfChanged(DirectoryReader.java:170)\n    at org.apache.lucene.search.SearcherManager.refreshIfNeeded(SearcherManager.java:118)\n    at org.apache.lucene.search.SearcherManager.refreshIfNeeded(SearcherManager.java:58)\n    at org.apache.lucene.search.ReferenceManager.doMaybeRefresh(ReferenceManager.java:155)\n    at org.apache.lucene.search.ReferenceManager.maybeRefresh(ReferenceManager.java:204)\n    at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:786)\n    at org.elasticsearch.index.shard.service.InternalIndexShard.refresh(InternalIndexShard.java:448)\n    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:228)\n    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:556)\n    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:426)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:724)\n```\n"},{"date":"2013-11-05T17:36:28Z","author":"brackxm","text":"configuration\n\n```\n        final Node node = NodeBuilder.nodeBuilder()\n                .local(true)\n                .clusterName(clusterName)\n                .settings(ImmutableSettings.settingsBuilder()\n                        .put(\"index.store.type\", \"memory\")\n                        .put(\"index.number_of_shards\", \"1\")\n                        .put(\"index.number_of_replicas\", \"0\")\n                        .put(\"gateway.type\", \"none\")\n                        .put(\"http.enabled\", false)\n                        .put(\"index.warmer.enabled\", false)\n                        .put(\"path.data\", \"target\/es\")\n                        .put(\"path.logs\", \"target\/es\")\n                        .put(\"path.work\", \"target\/es\")\n                        .build())\n                .build();\n```\n"},{"date":"2013-11-05T17:39:49Z","author":"kimchy","text":"can you maybe write isolate it to a test case that shows that it passes in 0.90.5, while it fails in 0.90.6 (with the `index.warmer.enabled` set to `false`)? it would be a great help in chasing it down...\n"},{"date":"2013-11-05T18:14:26Z","author":"brackxm","text":"```\nimport org.elasticsearch.client.Client;\nimport org.elasticsearch.common.settings.ImmutableSettings;\nimport org.elasticsearch.node.Node;\nimport org.elasticsearch.node.NodeBuilder;\n\nimport java.util.UUID;\n\npublic class Issue {\n    public static void main(final String[] args) {\n        final Node node = NodeBuilder.nodeBuilder()\n                .local(true)\n                .clusterName(\"test\")\n                .settings(ImmutableSettings.settingsBuilder()\n                        .put(\"index.store.type\", \"memory\")\n                        .put(\"index.number_of_shards\", \"1\")\n                        .put(\"index.number_of_replicas\", \"0\")\n                        .put(\"gateway.type\", \"none\")\n                        .put(\"http.enabled\", false)\n                        .put(\"index.warmer.enabled\", false)\n                        .build())\n                .build();\n        node.start();\n        final Client client = node.client();\n        final byte[] source = \"{\\\"a\\\":\\\"a1\\\"}\".getBytes();\n        for (int i = 0; i < 1000; i++) {\n            final String id = UUID.randomUUID().toString();\n            client.prepareIndex(\"index1\", \"type1\", id)\n                    .setSource(source)\n                    .setRefresh(true)\n                    .execute()\n                    .actionGet();\n        }\n    }\n}\n```\n"},{"date":"2013-11-05T22:24:33Z","author":"kimchy","text":"@s1monw and myself chased it up, seems like a regression in Lucene, opened an issue: https:\/\/issues.apache.org\/jira\/browse\/LUCENE-5330 and already have a patch for it: https:\/\/issues.apache.org\/jira\/secure\/attachment\/12612263\/LUCENE-5330.\n"},{"date":"2013-11-05T22:58:44Z","author":"brackxm","text":"thanks for looking into this\n"}],"reopen_on":"2013-11-05T17:19:54Z","opened_by":"brackxm","closed_on":"2013-11-06T13:15:22Z","description":"0.90.6 needs a lot more direct memory compared to 0.90.5 when using an in memory store for tests.\n\nSetting needed for 0.90.6\n\n```\n-XX:MaxDirectMemorySize=4608m\n```\n\nSetting needed for 0.90.5\n\n```\n-XX:MaxDirectMemorySize=512m\n```\n","id":"22134432","title":"0.90.6 memory issue","reopen_by":"clintongormley","opened_on":"2013-11-05T16:59:06Z","closed_by":"s1monw"},{"number":"3955","comments":[{"date":"2013-10-24T06:46:27Z","author":"s1monw","text":"+1 to the fix! GOOD CATCH! If I recall correctly the test query utils check for this don't we pass those queries through it? @jpountz any idea?\n"},{"date":"2013-10-24T07:21:33Z","author":"jpountz","text":"@s1monw I had the same reaction, I'll try to understand why our tests didn't catch this.\n"},{"date":"2013-10-25T17:15:27Z","author":"jpountz","text":"Pushed. Thanks Josh!\n"}],"reopen_on":"2013-10-24T07:43:42Z","opened_by":"joshcanfield","closed_on":"2013-10-25T17:15:27Z","description":"The defect is in \nChildrenQuery.ParentScorer#nextDoc()\n\n``` java\n          @Override\n            public int nextDoc() throws IOException {\n                if (remaining == 0) {\n                    return NO_MORE_DOCS;\n                }\n...\n```\n\nThis fails to update `currentDocId`. This causes `ChildrenQuery.ParentScorer#docID()` to continue returning the last doc id forever.\n\nThe BooleanScorer calls\n\n``` java\npublic boolean score(Collector collector, int max, int firstDocID)\n```\n\npassing the `firstDocID` it pulled from the `docID()`. When the ParentScorer finishes with a low id document but continues to get processed and added to the BucketTable it opens the possibility of a document from the other boolean sub queries overlapping and causing get an infinite loop.\n\nThe fix is to set \n`currentDocId = NO_MORE_DOCS;`\nin both `int nextDoc()` and `int advance(int target)`\n","id":"21487686","title":"has_child can cause an infinite loop (100% CPU) when used in bool query","reopen_by":"jpountz","opened_on":"2013-10-23T21:57:09Z","closed_by":"jpountz"},{"number":"3551","comments":[{"date":"2013-08-21T15:49:38Z","author":"dadoonet","text":"Commit updated thanks to @karmi!\n"},{"date":"2013-08-27T14:35:42Z","author":"javanna","text":"It looks like the url user\/repo\/zipball\/master is now working. It wasn't working back when this issue was opened, therefore we changed the github url where we get the archive from. Unfortunately the structure of the archive is slightly different when downloaded from the new url, since it contains a subfolder that has a slightly different naming, which we relied on in order to strip that part of the path from every zip entry and put all the files directly under the _site folder. To be more specific, the archives contain now a `repo-master` top-level folder, while using the other address the top-level folder is `user-repo-hash`.\n\nWe would like to keep the new adopted url format (user\/repo\/archive\/master.zip) since it seems more future-proof, but we need to adapt the zip extraction process too in order for this to work properly.\n\nReopening this issue as I'm working it, a fix will follow.\n"}],"reopen_on":"2013-08-27T14:35:42Z","opened_by":"dadoonet","closed_on":"2013-08-27T15:04:04Z","description":"Sounds like github changes a bit download url for master zip file.\n\nFrom `https:\/\/github.com\/username\/reponame\/zipball\/master` to `https:\/\/github.com\/username\/reponame\/archive\/master.zip`.\n\nWe need to update plugin manager to reflect that change.\n","id":"18355891","title":"Plugin Manager can not download _site plugins from github","reopen_by":"javanna","opened_on":"2013-08-21T13:57:43Z","closed_by":"javanna"},{"number":"3521","comments":[{"date":"2013-08-16T12:41:15Z","author":"s1monw","text":"hey @lmenezes, I am afraid but this is the expected behavior. there is a lot going on in this boolean query construct that depends on a number of factors.  the only guarantee here is that it will be the same score for all docs. Yet the eventual score and the query norm depend on your similarity, with a similarity that doesn not modify the query norm I guess it'd be 1.0 across the board.\nOne thing you can do is to wram top level query in a constant score that should give you a score of 1.0\n"},{"date":"2013-08-16T12:45:59Z","author":"lmenezes","text":"@s1monw that's the thing... i don't get a constant score for all documents. i do understand(also, don't care. not interested in the score for this case) that running the same query multiple times might result in different scores. but, for the same execution the documents should all score the same, right? if so, then there is something wrong here.\n"},{"date":"2013-08-16T12:48:34Z","author":"s1monw","text":"I just wrote a tests for this and I actually get back scores for all docs taht are consistent. I might not understand your problem here. you run this query and two docs get different scores?\n"},{"date":"2013-08-16T13:04:58Z","author":"lmenezes","text":"i tried pretty hard myself to write an example that worked(got 2 documents with different scores) here and wasn't able to.\nanyway, here is the response from a single query with the behavior i'm trying do describe: https:\/\/gist.github.com\/lmenezes\/6249787\n\ni could not reproduce that into my staging environment, only on live. the difference between staging and live at the moment, is that staging is not getting updates and has no replicas(if this info might help).\n\nwould setting explain -> true, and getting all the results help? its a pretty big response...\n"},{"date":"2013-08-16T13:14:12Z","author":"s1monw","text":"@lmenezes any idea where the boost comes from that is shown in your response?\n"},{"date":"2013-08-16T13:18:32Z","author":"lmenezes","text":"you mean this: ConstantScore(_:_)^3 right? if so, no idea. actually i'm executing the query from a file, so i know its always the same.\n"},{"date":"2013-08-16T13:19:48Z","author":"clintongormley","text":"@lmenezes check your email\n"},{"date":"2013-08-16T14:53:53Z","author":"s1monw","text":"this is more sneaky than I though... we are modifying a cached version of match all docs in the query parser... a fix is attached\n"},{"date":"2013-08-16T15:21:51Z","author":"lmenezes","text":"looks good. we currently fixed using a default boost != 1.0 for the matchall queries and will remove when updating to next version.\n"},{"date":"2013-08-16T19:20:40Z","author":"s1monw","text":"FYI - this can be triggered from a user query via `query_string` ie. `(*:*)^3.0` will leave a boost behind on the shards it hits.\n"},{"date":"2013-08-17T13:03:55Z","author":"lmenezes","text":"great :)\n"}],"reopen_on":"2013-08-16T14:53:17Z","opened_by":"lmenezes","closed_on":"2013-08-16T19:18:52Z","description":"unfortunately i wasn't able to reproduce this in the usual way, but here is the case:\n\nI have a query such as \n\n```\n{ \"from\": 0, \"size\": 10, \"query\": { \"bool\": {  \"must\": [ {\"match_all\":{}},{ \"constant_score\": { \"filter\": { \"terms\": { \"id\": [...] } } } } ] } }, \"fields\": \"\", \"sort\": [ { \"_score\": {} }, { \"id\": { \"order\": \"desc\" } } ] }\n```\n\nto the terms filter, i pass a list of ids(anywhere between 1 and 200k unique ids).\n\nwhen executing this query multiple times i get different results. so, investigating a little i traced it to the score not being constant sometimes(not what i expected at all).\n\ni ran the same query a few times with explain set to true and getting only the last document, and here is what i got:\n\n```\n_explanation: {\nvalue: 1.264911\ndescription: sum of:\ndetails: [\n{\nvalue: 0.94868326\ndescription: ConstantScore(*:*)^3.0, product of:\ndetails: [\n{\nvalue: 3\ndescription: boost\n}\n{\nvalue: 0.31622776\ndescription: queryNorm\n}\n]\n}\n{\nvalue: 0.31622776\ndescription: ConstantScore...\n```\n\nand then \n\n```\n_explanation: {\nvalue: 1.4142135\ndescription: sum of:\ndetails: [\n{\nvalue: 0.70710677\ndescription: ConstantScore(*:*), product of:\ndetails: [\n{\nvalue: 1\ndescription: boost\n}\n{\nvalue: 0.70710677\ndescription: queryNorm\n}\n]\n}\n{\nvalue: 0.70710677\ndescription: ConstantScore...\n```\n\nso, here i would expect this query to ALWAYS have the same score, and also, that every document scores exactly the same. \nit could even seem ok if the score wasnt constant across requests, but not really that documents score differently.\n- i do know i don't need the \"match_all\" query, but that's the way i managed to reproduce it on our cluster. without that, the score for the documents would always be 1 and i could not reproduce this behaviour.\n\n*\\* hope thats clear enough... but let me know if you need more info, or even the complete output for the explain(its pretty big)\n","id":"18152219","title":"odd scoring behaviour \/ inconsistent scoring","reopen_by":"s1monw","opened_on":"2013-08-16T11:31:39Z","closed_by":"s1monw"},{"number":"3469","comments":[{"date":"2013-08-09T05:22:11Z","author":"dadoonet","text":"Does it work if you search on title.title?\n"},{"date":"2013-08-09T08:55:04Z","author":"dadoonet","text":"That's true, it fails.\n\nBut when using another _sub field_ in multi field, it works fine:\n\n``` sh\ncurl -XDELETE \"http:\/\/localhost:9200\/test?pretty\"\ncurl -XPOST \"http:\/\/localhost:9200\/test?pretty\" -d '{\n  \"settings\": {\n    \"index\": {\n      \"number_of_replicas\": 0,\n      \"analysis\":{\n        \"analyzer\":{\n          \"suggest\":{\n            \"type\": \"custom\",\n            \"tokenizer\": \"standard\",\n            \"filter\": [ \"standard\", \"lowercase\", \"suggest_shingle\" ]\n          }\n        },\n        \"filter\":{\n          \"suggest_shingle\":{\n            \"type\": \"shingle\",\n            \"min_shingle_size\": 2,\n            \"max_shingle_size\": 5,\n            \"output_unigrams\": true\n          }\n        }\n      }\n    }\n  }\n}'\ncurl -XPOST \"http:\/\/localhost:9200\/test\/test\/_mapping?pretty\" -d '{\n  \"test\": {\n \"properties\" : {\n    \"title\": {\n      \"path\": \"just_name\",\n      \"type\": \"multi_field\",\n      \"fields\": \n        {\n           \"title\": {\n            \"type\": \"string\",\n            \"index\": \"analyzed\",\n            \"similarity\": \"BM25\",\n            \"analyzer\": \"suggest\"\n          },\n           \"title_suggest\": {\n            \"type\": \"string\",\n            \"index\": \"analyzed\",\n            \"similarity\": \"BM25\",\n            \"analyzer\": \"suggest\"\n          }\n        }\n      }\n    }\n  }\n}'\n\n\ncurl -XPOST \"http:\/\/localhost:9200\/test\/test?pretty\" -d '{\n  \"title\": \"Just testing the suggestions api\"\n}'\n\ncurl -XPOST \"http:\/\/localhost:9200\/test\/test?pretty&refresh\" -d '{\n  \"title\": \"An other title\"\n}'\n\ncurl -XGET \"http:\/\/localhost:9200\/test\/test\/_search?pretty\" -d '{\n  \"suggest\": {\n    \"text\": \"tetsting sugestion\",\n    \"phrase\":{\n      \"phrase\":{\n        \"field\": \"title_suggest\",\n        \"max_errors\": 5\n      }\n    }\n  }\n}'\n```\n\nI'm going to look at it but I don't think you can use suggest with multifield and only one field declared under (the default one).\n"},{"date":"2013-08-09T09:28:43Z","author":"dadoonet","text":"I found the _issue_ which is not exactly an issue.\n\nIf you add a `sleep` in your script, it works fine: \n\n``` sh\ncurl -XDELETE \"http:\/\/localhost:9200\/test?pretty\"\ncurl -XPOST \"http:\/\/localhost:9200\/test?pretty\" -d '{\n  \"settings\": {\n    \"index\": {\n      \"number_of_replicas\": 0,\n      \"analysis\":{\n        \"analyzer\":{\n          \"suggest\":{\n            \"type\": \"custom\",\n            \"tokenizer\": \"standard\",\n            \"filter\": [ \"standard\", \"lowercase\", \"suggest_shingle\" ]\n          }\n        },\n        \"filter\":{\n          \"suggest_shingle\":{\n            \"type\": \"shingle\",\n            \"min_shingle_size\": 2,\n            \"max_shingle_size\": 5,\n            \"output_unigrams\": true\n          }\n        }\n      }\n    }\n  }\n}'\n\ncurl -XPOST \"http:\/\/localhost:9200\/test\/test\/_mapping?pretty\" -d '{\n  \"test\": {\n \"properties\" : {\n    \"title\": {\n      \"path\": \"just_name\",\n      \"type\": \"multi_field\",\n      \"fields\": \n        {\n           \"title\": {\n            \"type\": \"string\",\n            \"index\": \"analyzed\",\n            \"similarity\": \"BM25\",\n            \"analyzer\": \"suggest\"\n          }\n        }\n      }\n    }\n  }\n}'\n\n\ncurl -XPOST \"http:\/\/localhost:9200\/test\/test?pretty\" -d '{\n  \"title\": \"Just testing the suggestions api\"\n}'\n\ncurl -XPOST \"http:\/\/localhost:9200\/test\/test?pretty&refresh\" -d '{\n  \"title\": \"An other title\"\n}'\n\nsleep 5\n\ncurl -XGET \"http:\/\/localhost:9200\/test\/test\/_search?pretty\" -d '{\n  \"suggest\": {\n    \"text\": \"tetsting sugestion\",\n    \"phrase\":{\n      \"phrase\":{\n        \"field\": \"title\",\n        \"max_errors\": 5\n      }\n    }\n  }\n}'\n```\n"},{"date":"2013-08-09T09:59:54Z","author":"dadoonet","text":"Sorry. I was wrong.\n\nYou have two issues here:\n- In your script, you need to add a refresh after the last index command\n- You have 5 shards but only 2 documents. Suggestion on other empty shards does not work which cause the error you are seeing.\n\nThe following script is working fine:\n\n``` sh\ncurl -XDELETE \"http:\/\/localhost:9200\/test?pretty\"\ncurl -XPOST \"http:\/\/localhost:9200\/test?pretty\" -d '{\n  \"settings\": {\n    \"index\": {\n      \"number_of_shards\": 1,\n      \"number_of_replicas\": 0,\n      \"analysis\":{\n        \"analyzer\":{\n          \"suggest\":{\n            \"type\": \"custom\",\n            \"tokenizer\": \"standard\",\n            \"filter\": [ \"standard\", \"lowercase\", \"suggest_shingle\" ]\n          }\n        },\n        \"filter\":{\n          \"suggest_shingle\":{\n            \"type\": \"shingle\",\n            \"min_shingle_size\": 2,\n            \"max_shingle_size\": 5,\n            \"output_unigrams\": true\n          }\n        }\n      }\n    }\n  }\n}'\n\ncurl -XPOST \"http:\/\/localhost:9200\/test\/test\/_mapping?pretty\" -d '{\n  \"test\": {\n \"properties\" : {\n    \"title\": {\n      \"path\": \"just_name\",\n      \"type\": \"multi_field\",\n      \"fields\": \n        {\n           \"title\": {\n            \"type\": \"string\",\n            \"index\": \"analyzed\",\n            \"similarity\": \"BM25\",\n            \"analyzer\": \"suggest\"\n          }\n        }\n      }\n    }\n  }\n}'\n\n\ncurl -XPOST \"http:\/\/localhost:9200\/test\/test?pretty\" -d '{\n  \"title\": \"Just testing the suggestions api\"\n}'\n\ncurl -XPOST \"http:\/\/localhost:9200\/test\/test?pretty&refresh\" -d '{\n  \"title\": \"An other title\"\n}'\n\ncurl -XGET \"http:\/\/localhost:9200\/test\/test\/_search?pretty\" -d '{\n  \"suggest\": {\n    \"text\": \"tetsting sugestion\",\n    \"phrase\":{\n      \"phrase\":{\n        \"field\": \"title\",\n        \"max_errors\": 5\n      }\n    }\n  }\n}'\n```\n\nI keep this issue opened to see if we can not _fail_ in case of empty shards.\n"},{"date":"2013-08-09T14:29:28Z","author":"mathieu007","text":"Ah , yeah your right, i understand now why i received this error, it make sense...\nThanks for pointing me the errors in my script, will remember that for next time.\n\nMath\n"},{"date":"2013-08-09T15:09:38Z","author":"nik9000","text":"I've been noticing that suggestions fail on empty shards problem for the past few days.  If no one gets to it the in next few days I'll have a crack at fixing the errors on empty shards.\n"},{"date":"2013-08-09T15:19:01Z","author":"dadoonet","text":"Closing this one as there is now issue #3473 and PR #3475 relative to it.\n"},{"date":"2013-08-16T11:17:12Z","author":"s1monw","text":"reopening since this is really a different issue than having an entirely empty shard. here we can have no value in a field rather than having no docs. \n"}],"reopen_on":"2013-08-16T11:17:14Z","opened_by":"mathieu007","closed_on":"2013-08-16T18:25:54Z","description":"The following error occur while trying to do a simple phrase query on a multi-field using the suggest api:\n\n<pre>\"failures\" : [ {\n      \"status\" : 400,\n      \"reason\" : \"ElasticSearchIllegalArgumentException[generator field [title] doesn't exist]\"\n    }, {\n      \"status\" : 400,\n      \"reason\" : \"RemoteTransportException[[Bella Donna][inet[\/127.0.0.1:9301]][search\/phase\/query]]; nested: ElasticSearchIllegalArgumentException[generator field [title] doesn't exist]; \"\n    } ]\n<\/pre>\n\n\nAnd here is the code:\n\n<pre>curl -XDELETE \"http:\/\/localhost:9200\/test?pretty\"\ncurl -XPOST \"http:\/\/localhost:9200\/test?pretty\" -d '{\n  \"settings\": {\n    \"index\": {\n      \"number_of_replicas\": 0,\n      \"analysis\":{\n        \"analyzer\":{\n          \"suggest\":{\n            \"type\": \"custom\",\n            \"tokenizer\": \"standard\",\n            \"filter\": [ \"standard\", \"lowercase\", \"suggest_shingle\" ]\n          }\n        },\n        \"filter\":{\n          \"suggest_shingle\":{\n            \"type\": \"shingle\",\n            \"min_shingle_size\": 2,\n            \"max_shingle_size\": 5,\n            \"output_unigrams\": true\n          }\n        }\n      }\n    }\n  }\n}'\ncurl -XPOST \"http:\/\/localhost:9200\/test\/test\/_mapping?pretty\" -d '{\n  \"test\": {\n \"properties\" : {\n    \"title\": {\n      \"path\": \"just_name\",\n      \"type\": \"multi_field\",\n      \"fields\": \n        {\n          \"title\": {\n            \"type\": \"string\",\n            \"index\": \"analyzed\",\n            \"similarity\": \"BM25\",\n            \"analyzer\": \"suggest\"\n          }\n        }\n      }\n      \n    }\n  }\n}'\n\n\ncurl -XPOST \"http:\/\/localhost:9200\/test\/test?pretty\" -d '{\n  \"title\": \"Just testing the suggestions api\"\n}'\n\ncurl -XPOST \"http:\/\/localhost:9200\/test\/test?pretty\" -d '{\n  \"title\": \"An other title\"\n}'\n\ncurl -XGET \"http:\/\/localhost:9200\/test\/test\/_search?pretty\" -d '{\n  \"query\": {\n    \"match_all\": {}\n  },\n  \"suggest\": {\n    \"text\": \"tetsting sugestion\",\n    \"phrase\":{\n      \"phrase\":{\n        \"field\": \"title\",\n        \"max_errors\": 5\n      }\n    }\n  }\n}'<\/pre>\n","id":"17845963","title":"Multi-field and suggest api error","reopen_by":"s1monw","opened_on":"2013-08-09T03:52:51Z","closed_by":"s1monw"},{"number":"3242","comments":[{"date":"2013-09-07T19:25:30Z","author":"alexeiemam","text":"Is this fix included in the 0.90.3 release?\nI am currently experiencing the same error when both a geo_shape and a geo_point field are present in the same document.\n\n**Edit: using 0.90.3**\n"},{"date":"2013-09-07T19:42:33Z","author":"s1monw","text":"this has never been backported... I will backport!\n"},{"date":"2013-09-07T20:16:56Z","author":"s1monw","text":"pushed to `0.90` branch. This will be part of `0.90.4`\n"},{"date":"2013-09-08T00:04:22Z","author":"alexeiemam","text":"When is the, approximate, anticipated release of 0.90.4?\n"},{"date":"2013-09-08T05:52:21Z","author":"s1monw","text":"> When is the, approximate, anticipated release of 0.90.4?\n\nwe plan on a release early next week  \n"}],"reopen_on":"2013-09-07T19:42:33Z","opened_by":"chilling","closed_on":"2013-09-07T20:16:56Z","description":"The `geo_shape` filter seems to be unable to handle multiple `geo_shape` fields in a single document if this document is used as indexed filter.\n\nAssume a mapping with multiple `geo_shape` fields:\n\n```\n{\n    \"type1\" : {\n        \"properties\" : {\n            \"location1\" : {\n                 \"type\" : \"geo_shape\"\n            },\n            \"location2\" : {\n                \"type\" : \"geo_shape\"\n            }\n        }\n    }\n}\n```\n\nand a document\n\n```\n{\n    \"location1\" : {\n        \"type\":\"polygon\",\n        \"coordinates\":[[[-10,-10],[10,-10],[10,10],[-10,10],[-10,-10]]]\n    },\n    \"location2\" : {\n        \"type\":\"polygon\",\n        \"coordinates\":[[[-20,-20],[20,-20],[20,20],[-20,20],[-20,-20]]]\n    }\n}\n```\n\nIf a `geo_shape` filter is applied to the `location2` field\n\n```\n{\n    \"geo_shape\": {\n        \"location2\": {\n            \"indexed_shape\": { \n                \"id\": \"1\",\n                \"type\": \"type1\",\n                \"index\": \"test\",\n                \"shape_field_name\": \"location2\"\n            }\n        }\n    }\n}\n```\n\nparsing fails with\n\n```\nElasticSearchIllegalStateException[Shape with name [1] found but missing location2 field];\n```\n","id":"16024086","title":"Geoshape filter can't handle multiple shapes","reopen_by":"s1monw","opened_on":"2013-06-26T10:10:52Z","closed_by":"s1monw"},{"number":"3078","comments":[{"date":"2013-05-23T17:39:40Z","author":"sarmiena","text":"Thanks for the ticket, @clintongormley . Sadly this bug is causing many people to yell at me :( They update a record and it's removed from their UI.\n"},{"date":"2013-05-23T18:52:12Z","author":"s1monw","text":"what version does this reproduce on? does this still happen on master?\n"},{"date":"2013-05-23T18:59:04Z","author":"s1monw","text":"@martijnvg this is fixed it seems. this caused by #2991 and fixed in master and 0.90\n"},{"date":"2013-05-23T19:45:41Z","author":"sarmiena","text":"@s1monw this issue is definitely happening on 0.90.0 release. I also just pulled down the repo and ran it against  v 1.0.0 beta1. The issue still exists there as well.\n\nPlease verify and reopen. \n\n@clintongormley can you confirm?\n"},{"date":"2013-05-23T19:47:31Z","author":"clintongormley","text":"@sarmiena For me it is broken in 0.90.0, but fixed in master and in the 0.90.1 branch.\n\nUnless you have a different test to show otherwise?\n"},{"date":"2013-05-23T19:54:54Z","author":"sarmiena","text":"@clintongormley I'm building from master using:\n\n```\nmvn clean package -DskipTests\n```\n\nHowever, this is building elasticsearch-1.0.0.Beta1-SNAPSHOT.\n\nI'm not sure how to build 0.90.1 since there is no tag or branch in the repo that I can see.\n\nLet me know if you want me to show you (live) how to reproduce it using 1.0.0.Beta1\n"},{"date":"2013-05-23T20:02:24Z","author":"sarmiena","text":"@clintongormley ok I just ran the same scenario on 0.90.1 branch and it's definitely still happening. Not sure why yours isn't showing the same issue.\n"},{"date":"2013-05-23T20:13:14Z","author":"sarmiena","text":"@clintongormley Sorry to keep bothering :) However I have good news and bad news:\n\nGood news: Your test case does work in 0.90.1\nBad news: An alternative test case produces same problem\n\nYou used bulk upload, while I simply added 1 record at a time (100 times).\n\nhttps:\/\/gist.github.com\/sarmiena\/d945848fd683f39d212c\n\nI used Ruby to iterate 100 POST requests in that gist, but you can use whatever you'd like.\n\nThe issue doesn't appear to be resolved. Can we reopen the ticket?\n"},{"date":"2013-05-23T20:18:32Z","author":"clintongormley","text":"@s1monw ?\n"},{"date":"2013-05-23T21:21:33Z","author":"s1monw","text":"i added a testcase that mirrors your ruby test in java and it doesn't fail. I can't reproduce your problem I am sorry. Are you sure you build 0.90.1?\n"},{"date":"2013-05-23T21:24:52Z","author":"sarmiena","text":"@s1monw I'm sure I can reproduce this in 0.90.1. Perhaps the test isn't producing the same problems since the JSON api is being used and the test is using the interfaces directly?\n\nI can to a teamviewer if you'd like. Otherwise you can just pop open irb and copy\/paste the ruby code in there. \n\ngchat me sarmiena@gmail.com if you want to get ahold of me. otherwise i'm on IRC in #elasticsearch as sarmiena_ (notice the underscore)\n"},{"date":"2013-05-23T22:41:44Z","author":"sarmiena","text":"Ok looks like 0.90.1 does fix this issue. The formatting was a little off and I missed the record:\n\nPlease close.\n\n```\n# ruby code (irb)\n(1).upto(100) do |i|\n    `curl -XPUT 'http:\/\/localhost:9200\/twitter\/tweet\/#{i}' -d '{ \"user\" : \"#{i}\"}'`\nend\n\n# command line\n$ curl -X POST \"http:\/\/localhost:9200\/twitter\/tweet\/1\" -d '\n> {\"user\":\"1\"}\n> '\n\n$ curl -X GET 'http:\/\/localhost:9200\/twitter\/tweet\/_search?pretty' -d '\n{\n  \"sort\": [\n    {\n      \"user\": \"asc\"\n    }\n  ],\n  \"size\": 10,\n  \"from\": 0\n}\n'\n{\n  \"took\" : 8,\n  \"timed_out\" : false,\n  \"_shards\" : {\n    \"total\" : 5,\n    \"successful\" : 5,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : 100,\n    \"max_score\" : null,\n    \"hits\" : [ {\n      \"_index\" : \"twitter\",\n      \"_type\" : \"tweet\",\n      \"_id\" : \"1\",\n      \"_score\" : null, \"_source\" :\n{\"user\":\"1\"}\n,\n      \"sort\" : [ \"1\" ]\n    }, {\n      \"_index\" : \"twitter\",\n      \"_type\" : \"tweet\",\n      \"_id\" : \"10\",\n      \"_score\" : null, \"_source\" : { \"user\" : \"10\"},\n      \"sort\" : [ \"10\" ]\n    }, {\n      \"_index\" : \"twitter\",\n      \"_type\" : \"tweet\",\n      \"_id\" : \"100\",\n      \"_score\" : null, \"_source\" : { \"user\" : \"100\"},\n      \"sort\" : [ \"100\" ]\n    }, {\n      \"_index\" : \"twitter\",\n      \"_type\" : \"tweet\",\n      \"_id\" : \"11\",\n      \"_score\" : null, \"_source\" : { \"user\" : \"11\"},\n      \"sort\" : [ \"11\" ]\n    }, {\n      \"_index\" : \"twitter\",\n      \"_type\" : \"tweet\",\n      \"_id\" : \"12\",\n      \"_score\" : null, \"_source\" : { \"user\" : \"12\"},\n      \"sort\" : [ \"12\" ]\n    }, {\n      \"_index\" : \"twitter\",\n      \"_type\" : \"tweet\",\n      \"_id\" : \"13\",\n      \"_score\" : null, \"_source\" : { \"user\" : \"13\"},\n      \"sort\" : [ \"13\" ]\n    }, {\n      \"_index\" : \"twitter\",\n      \"_type\" : \"tweet\",\n      \"_id\" : \"14\",\n      \"_score\" : null, \"_source\" : { \"user\" : \"14\"},\n      \"sort\" : [ \"14\" ]\n    }, {\n      \"_index\" : \"twitter\",\n      \"_type\" : \"tweet\",\n      \"_id\" : \"15\",\n      \"_score\" : null, \"_source\" : { \"user\" : \"15\"},\n      \"sort\" : [ \"15\" ]\n    }, {\n      \"_index\" : \"twitter\",\n      \"_type\" : \"tweet\",\n      \"_id\" : \"16\",\n      \"_score\" : null, \"_source\" : { \"user\" : \"16\"},\n      \"sort\" : [ \"16\" ]\n    }, {\n      \"_index\" : \"twitter\",\n      \"_type\" : \"tweet\",\n      \"_id\" : \"17\",\n      \"_score\" : null, \"_source\" : { \"user\" : \"17\"},\n      \"sort\" : [ \"17\" ]\n    } ]\n  }\n}\n```\n"},{"date":"2013-05-24T06:38:09Z","author":"s1monw","text":"thanks for bringing clarification! good to work with you last night!\n"}],"reopen_on":"2013-05-23T20:18:33Z","opened_by":"clintongormley","closed_on":"2013-05-24T06:38:09Z","description":"After reindexing a doc, it is not being returned in the correct sort order (when sorting on a string field)\n\nFirst, index docs 1..100 with a string field `user`:\n\n```\ncurl -XPOST 'http:\/\/127.0.0.1:9200\/test\/test\/_bulk?pretty=1'  -d '\n{\"index\" : {\"_id\" : \"1\"}}\n{\"user\" : \"1\"}\n{\"index\" : {\"_id\" : \"2\"}}\n{\"user\" : \"2\"}\n{\"index\" : {\"_id\" : \"3\"}}\n{\"user\" : \"3\"}\n{\"index\" : {\"_id\" : \"4\"}}\n{\"user\" : \"4\"}\n{\"index\" : {\"_id\" : \"5\"}}\n{\"user\" : \"5\"}\n{\"index\" : {\"_id\" : \"6\"}}\n{\"user\" : \"6\"}\n{\"index\" : {\"_id\" : \"7\"}}\n{\"user\" : \"7\"}\n{\"index\" : {\"_id\" : \"8\"}}\n{\"user\" : \"8\"}\n{\"index\" : {\"_id\" : \"9\"}}\n{\"user\" : \"9\"}\n{\"index\" : {\"_id\" : \"10\"}}\n{\"user\" : \"10\"}\n{\"index\" : {\"_id\" : \"11\"}}\n{\"user\" : \"11\"}\n{\"index\" : {\"_id\" : \"12\"}}\n{\"user\" : \"12\"}\n{\"index\" : {\"_id\" : \"13\"}}\n{\"user\" : \"13\"}\n{\"index\" : {\"_id\" : \"14\"}}\n{\"user\" : \"14\"}\n{\"index\" : {\"_id\" : \"15\"}}\n{\"user\" : \"15\"}\n{\"index\" : {\"_id\" : \"16\"}}\n{\"user\" : \"16\"}\n{\"index\" : {\"_id\" : \"17\"}}\n{\"user\" : \"17\"}\n{\"index\" : {\"_id\" : \"18\"}}\n{\"user\" : \"18\"}\n{\"index\" : {\"_id\" : \"19\"}}\n{\"user\" : \"19\"}\n{\"index\" : {\"_id\" : \"20\"}}\n{\"user\" : \"20\"}\n{\"index\" : {\"_id\" : \"21\"}}\n{\"user\" : \"21\"}\n{\"index\" : {\"_id\" : \"22\"}}\n{\"user\" : \"22\"}\n{\"index\" : {\"_id\" : \"23\"}}\n{\"user\" : \"23\"}\n{\"index\" : {\"_id\" : \"24\"}}\n{\"user\" : \"24\"}\n{\"index\" : {\"_id\" : \"25\"}}\n{\"user\" : \"25\"}\n{\"index\" : {\"_id\" : \"26\"}}\n{\"user\" : \"26\"}\n{\"index\" : {\"_id\" : \"27\"}}\n{\"user\" : \"27\"}\n{\"index\" : {\"_id\" : \"28\"}}\n{\"user\" : \"28\"}\n{\"index\" : {\"_id\" : \"29\"}}\n{\"user\" : \"29\"}\n{\"index\" : {\"_id\" : \"30\"}}\n{\"user\" : \"30\"}\n{\"index\" : {\"_id\" : \"31\"}}\n{\"user\" : \"31\"}\n{\"index\" : {\"_id\" : \"32\"}}\n{\"user\" : \"32\"}\n{\"index\" : {\"_id\" : \"33\"}}\n{\"user\" : \"33\"}\n{\"index\" : {\"_id\" : \"34\"}}\n{\"user\" : \"34\"}\n{\"index\" : {\"_id\" : \"35\"}}\n{\"user\" : \"35\"}\n{\"index\" : {\"_id\" : \"36\"}}\n{\"user\" : \"36\"}\n{\"index\" : {\"_id\" : \"37\"}}\n{\"user\" : \"37\"}\n{\"index\" : {\"_id\" : \"38\"}}\n{\"user\" : \"38\"}\n{\"index\" : {\"_id\" : \"39\"}}\n{\"user\" : \"39\"}\n{\"index\" : {\"_id\" : \"40\"}}\n{\"user\" : \"40\"}\n{\"index\" : {\"_id\" : \"41\"}}\n{\"user\" : \"41\"}\n{\"index\" : {\"_id\" : \"42\"}}\n{\"user\" : \"42\"}\n{\"index\" : {\"_id\" : \"43\"}}\n{\"user\" : \"43\"}\n{\"index\" : {\"_id\" : \"44\"}}\n{\"user\" : \"44\"}\n{\"index\" : {\"_id\" : \"45\"}}\n{\"user\" : \"45\"}\n{\"index\" : {\"_id\" : \"46\"}}\n{\"user\" : \"46\"}\n{\"index\" : {\"_id\" : \"47\"}}\n{\"user\" : \"47\"}\n{\"index\" : {\"_id\" : \"48\"}}\n{\"user\" : \"48\"}\n{\"index\" : {\"_id\" : \"49\"}}\n{\"user\" : \"49\"}\n{\"index\" : {\"_id\" : \"50\"}}\n{\"user\" : \"50\"}\n{\"index\" : {\"_id\" : \"51\"}}\n{\"user\" : \"51\"}\n{\"index\" : {\"_id\" : \"52\"}}\n{\"user\" : \"52\"}\n{\"index\" : {\"_id\" : \"53\"}}\n{\"user\" : \"53\"}\n{\"index\" : {\"_id\" : \"54\"}}\n{\"user\" : \"54\"}\n{\"index\" : {\"_id\" : \"55\"}}\n{\"user\" : \"55\"}\n{\"index\" : {\"_id\" : \"56\"}}\n{\"user\" : \"56\"}\n{\"index\" : {\"_id\" : \"57\"}}\n{\"user\" : \"57\"}\n{\"index\" : {\"_id\" : \"58\"}}\n{\"user\" : \"58\"}\n{\"index\" : {\"_id\" : \"59\"}}\n{\"user\" : \"59\"}\n{\"index\" : {\"_id\" : \"60\"}}\n{\"user\" : \"60\"}\n{\"index\" : {\"_id\" : \"61\"}}\n{\"user\" : \"61\"}\n{\"index\" : {\"_id\" : \"62\"}}\n{\"user\" : \"62\"}\n{\"index\" : {\"_id\" : \"63\"}}\n{\"user\" : \"63\"}\n{\"index\" : {\"_id\" : \"64\"}}\n{\"user\" : \"64\"}\n{\"index\" : {\"_id\" : \"65\"}}\n{\"user\" : \"65\"}\n{\"index\" : {\"_id\" : \"66\"}}\n{\"user\" : \"66\"}\n{\"index\" : {\"_id\" : \"67\"}}\n{\"user\" : \"67\"}\n{\"index\" : {\"_id\" : \"68\"}}\n{\"user\" : \"68\"}\n{\"index\" : {\"_id\" : \"69\"}}\n{\"user\" : \"69\"}\n{\"index\" : {\"_id\" : \"70\"}}\n{\"user\" : \"70\"}\n{\"index\" : {\"_id\" : \"71\"}}\n{\"user\" : \"71\"}\n{\"index\" : {\"_id\" : \"72\"}}\n{\"user\" : \"72\"}\n{\"index\" : {\"_id\" : \"73\"}}\n{\"user\" : \"73\"}\n{\"index\" : {\"_id\" : \"74\"}}\n{\"user\" : \"74\"}\n{\"index\" : {\"_id\" : \"75\"}}\n{\"user\" : \"75\"}\n{\"index\" : {\"_id\" : \"76\"}}\n{\"user\" : \"76\"}\n{\"index\" : {\"_id\" : \"77\"}}\n{\"user\" : \"77\"}\n{\"index\" : {\"_id\" : \"78\"}}\n{\"user\" : \"78\"}\n{\"index\" : {\"_id\" : \"79\"}}\n{\"user\" : \"79\"}\n{\"index\" : {\"_id\" : \"80\"}}\n{\"user\" : \"80\"}\n{\"index\" : {\"_id\" : \"81\"}}\n{\"user\" : \"81\"}\n{\"index\" : {\"_id\" : \"82\"}}\n{\"user\" : \"82\"}\n{\"index\" : {\"_id\" : \"83\"}}\n{\"user\" : \"83\"}\n{\"index\" : {\"_id\" : \"84\"}}\n{\"user\" : \"84\"}\n{\"index\" : {\"_id\" : \"85\"}}\n{\"user\" : \"85\"}\n{\"index\" : {\"_id\" : \"86\"}}\n{\"user\" : \"86\"}\n{\"index\" : {\"_id\" : \"87\"}}\n{\"user\" : \"87\"}\n{\"index\" : {\"_id\" : \"88\"}}\n{\"user\" : \"88\"}\n{\"index\" : {\"_id\" : \"89\"}}\n{\"user\" : \"89\"}\n{\"index\" : {\"_id\" : \"90\"}}\n{\"user\" : \"90\"}\n{\"index\" : {\"_id\" : \"91\"}}\n{\"user\" : \"91\"}\n{\"index\" : {\"_id\" : \"92\"}}\n{\"user\" : \"92\"}\n{\"index\" : {\"_id\" : \"93\"}}\n{\"user\" : \"93\"}\n{\"index\" : {\"_id\" : \"94\"}}\n{\"user\" : \"94\"}\n{\"index\" : {\"_id\" : \"95\"}}\n{\"user\" : \"95\"}\n{\"index\" : {\"_id\" : \"96\"}}\n{\"user\" : \"96\"}\n{\"index\" : {\"_id\" : \"97\"}}\n{\"user\" : \"97\"}\n{\"index\" : {\"_id\" : \"98\"}}\n{\"user\" : \"98\"}\n{\"index\" : {\"_id\" : \"99\"}}\n{\"user\" : \"99\"}\n{\"index\" : {\"_id\" : \"100\"}}\n{\"user\" : \"100\"}\n'\n```\n\nSearch, sorting on `user`:\n\n```\ncurl -XGET 'http:\/\/127.0.0.1:9200\/test\/_search?pretty=1'  -d '\n{\n   \"sort\" : {\n      \"user\" : \"asc\"\n   },\n   \"fields\" : [],\n   \"size\" : 10\n}\n'\n```\n\nResults show that `user:1` is in first position:\n\n```\n# {\n#    \"hits\" : {\n#       \"hits\" : [\n#          {\n#             \"sort\" : [\n#                \"1\"\n#             ],\n#             \"_score\" : null,\n#             \"_index\" : \"test\",\n#             \"_id\" : \"1\",\n#             \"_type\" : \"test\"\n#          },\n#          {\n#             \"sort\" : [\n#                \"10\"\n#             ],\n#             \"_score\" : null,\n#             \"_index\" : \"test\",\n#             \"_id\" : \"10\",\n#             \"_type\" : \"test\"\n#          },\n# ....\n```\n\nNow reindex the first doc, with the same values:\n\n```\ncurl -XPUT 'http:\/\/127.0.0.1:9200\/test\/test\/1?pretty=1'  -d '\n{\n   \"user\" : \"1\"\n}\n'\n```\n\nAnd search again:\n\n```\ncurl -XGET 'http:\/\/127.0.0.1:9200\/test\/_search?pretty=1'  -d '\n{\n   \"sort\" : {\n      \"user\" : \"asc\"\n   },\n   \"fields\" : [],\n   \"size\" : 10\n}\n'\n```\n\nDoc with `user:1` no longer appears in the correct position, in fact it doesn't appear anywhere in the first 10 results:\n\n```\n# {\n#    \"hits\" : {\n#       \"hits\" : [\n#          {\n#             \"sort\" : [\n#                \"10\"\n#             ],\n#             \"_score\" : null,\n#             \"_index\" : \"test\",\n#             \"_id\" : \"10\",\n#             \"_type\" : \"test\"\n#          },\n#          {\n#             \"sort\" : [\n#                \"100\"\n#             ],\n#             \"_score\" : null,\n#             \"_index\" : \"test\",\n#             \"_id\" : \"100\",\n#             \"_type\" : \"test\"\n#          },\n#          {\n#             \"sort\" : [\n#                \"11\"\n#             ],\n#             \"_score\" : null,\n#             \"_index\" : \"test\",\n#             \"_id\" : \"11\",\n#             \"_type\" : \"test\"\n#          },\n#          {\n#             \"sort\" : [\n#                \"12\"\n#             ],\n#             \"_score\" : null,\n#             \"_index\" : \"test\",\n#             \"_id\" : \"12\",\n#             \"_type\" : \"test\"\n#          },\n#          {\n#             \"sort\" : [\n#                \"13\"\n#             ],\n#             \"_score\" : null,\n#             \"_index\" : \"test\",\n#             \"_id\" : \"13\",\n#             \"_type\" : \"test\"\n#          },\n#          {\n#             \"sort\" : [\n#                \"14\"\n#             ],\n#             \"_score\" : null,\n#             \"_index\" : \"test\",\n#             \"_id\" : \"14\",\n#             \"_type\" : \"test\"\n#          },\n#          {\n#             \"sort\" : [\n#                \"15\"\n#             ],\n#             \"_score\" : null,\n#             \"_index\" : \"test\",\n#             \"_id\" : \"15\",\n#             \"_type\" : \"test\"\n#          },\n#          {\n#             \"sort\" : [\n#                \"16\"\n#             ],\n#             \"_score\" : null,\n#             \"_index\" : \"test\",\n#             \"_id\" : \"16\",\n#             \"_type\" : \"test\"\n#          },\n#          {\n#             \"sort\" : [\n#                \"17\"\n#             ],\n#             \"_score\" : null,\n#             \"_index\" : \"test\",\n#             \"_id\" : \"17\",\n#             \"_type\" : \"test\"\n#          },\n#          {\n#             \"sort\" : [\n#                \"18\"\n#             ],\n#             \"_score\" : null,\n#             \"_index\" : \"test\",\n#             \"_id\" : \"18\",\n#             \"_type\" : \"test\"\n#          }\n#       ],\n#       \"max_score\" : null,\n#       \"total\" : 100\n#    },\n#    \"timed_out\" : false,\n#    \"_shards\" : {\n#       \"failed\" : 0,\n#       \"successful\" : 5,\n#       \"total\" : 5\n#    },\n#    \"took\" : 3\n# }\n```\n\nHowever, if you return all 100 docs, then it appears in the first position again (correctly):\n\n```\ncurl -XGET 'http:\/\/127.0.0.1:9200\/test\/_search?pretty=1'  -d '\n{\n   \"sort\" : {\n      \"user\" : \"asc\"\n   },\n   \"fields\" : [],\n   \"size\" : 100\n}\n'\n\n# {\n#    \"hits\" : {\n#       \"hits\" : [\n#          {\n#             \"sort\" : [\n#                \"1\"\n#             ],\n#             \"_score\" : null,\n#             \"_index\" : \"test\",\n#             \"_id\" : \"1\",\n#             \"_type\" : \"test\"\n#          },\n```\n\nwhich leads me to think that it is the shard level sorting which is incorrect.\n","id":"14665837","title":"String sorting incorrect after reindex","reopen_by":"clintongormley","opened_on":"2013-05-23T09:43:59Z","closed_by":"s1monw"},{"number":"3037","comments":[{"date":"2013-05-15T10:08:45Z","author":"clintongormley","text":"The parameter is `prefix_len` not `prefix_leng`:\n"},{"date":"2013-05-15T10:11:32Z","author":"clintongormley","text":"```\n    curl -XPOST '127.0.0.1:9200\/test\/_suggest?pretty' -d'{\n      \"did_you_mean\": {\n        \"text\": \"ice\",\n        \"term\": {\n          \"field\": \"name\",\n          \"max_edits\":2,\n          \"suggest_mode\": \"always\",\n          \"min_word_len\": 0,\n          \"prefix_len\": 0\n        }\n      }\n    }'\n    {\n      \"_shards\" : {\n        \"total\" : 1,\n        \"successful\" : 1,\n        \"failed\" : 0\n      },\n      \"did_you_mean\" : [ {\n        \"text\" : \"ice\",\n        \"offset\" : 0,\n        \"length\" : 3,\n        \"options\" : [ {\n          \"text\" : \"iced\",\n          \"score\" : 0.6666666,\n          \"freq\" : 1\n        } ]\n      } ]\n    }\n```\n"},{"date":"2013-05-15T11:36:11Z","author":"jtreher","text":"@clintongormley While I did have a typo in the term suggest, the phrase suggest example is working and demonstrates the issue. Could you reopen?\n\nI will clarify that the gist was demonstrating that the term suggest is providing the term \"iced\" but I believe the candidate generator in the phrase suggest is not provided the term \"iced\" for the phrase suggest to consider because of the word length. \n"},{"date":"2013-05-15T12:02:07Z","author":"clintongormley","text":"@jtreher sorry - got that completely wrong. I'll reopen\n\nI'm seeing the same thing you're seeing.\n\n@s1monw ?\n"},{"date":"2013-05-15T12:32:08Z","author":"s1monw","text":"this seems to be a bug in the min_doc_freq smoothing. The good thing is that this only happens if your query term has a freq = 1 and the replacement has a freq = 1 as well. So in practice this might not be an issue. I will have a fix soon, in the meanwhile this should help:\n\n```\ncurl -XPOST 'localhost:9200\/test\/_suggest?pretty=true' -d '{                                                                                                                              \n  \"text\": \"ice tea\",\n  \"did_you_mean\": {\n    \"phrase\": {\n      \"field\": \"name_shingled\",\n      \"gram_size\": 3,\n      \"direct_generator\": [\n        {\n          \"field\": \"name\",\n          \"max_edits\": 2,\n          \"suggest_mode\": \"always\",\n          \"min_word_len\": 0,\n          \"prefix_len\": 0,\n          \"min_doc_freq\": 1.0\n        }\n      ]\n    }\n  }\n}'\n```\n"},{"date":"2013-05-15T17:39:44Z","author":"jtreher","text":"@s1monw Is there any chance that max_term_freq is not being obeyed as well with \"always?\" While this patch fixed this test issue, I actually have a situation where ice appears thousands of times and iced several hundred. I see \"iced\" appear from the term suggest, but it's like the phrase suggest never gets it.\n"},{"date":"2013-05-15T21:01:17Z","author":"s1monw","text":"`max_term_freq` is the maximum threshold (default: 0.01f) of documents a query term can appear in order to provide suggestions. Which means that if you won't even get a candidate for `ice` if the freq exceeds `max_term_freq`. Maybe I am not understanding you question right?\n"},{"date":"2013-05-16T00:28:25Z","author":"jtreher","text":"@s1monw  I have to set max_term_freq in the term suggest to 0.999 (99.9%) to have term show up. However, when I do this in phrase suggest, it's as though the candidate is not generated. \n"},{"date":"2013-06-07T17:08:44Z","author":"jtreher","text":"@s1monw This did fix the issue I had. It seems to respect max_term_freq now with 0.9.1\n"},{"date":"2013-06-07T19:07:55Z","author":"s1monw","text":"@jtreher I think I did!  \n"}],"reopen_on":"2013-05-15T12:02:07Z","opened_by":"jtreher","closed_on":"2013-05-15T13:21:33Z","description":"I ran into an issue where the phrase suggester does not seem to be generating terms for words of length less than the default of four even with the min_word_len set to 0,1,2, or 3. When I run a term suggest, the term comes back as expected.\n\nHere is a gist reproducing the issue:\nhttps:\/\/gist.github.com\/jtreher\/5577747\n","id":"14321284","title":"Phrase suggest direct generator possibly not obeying min_word_len 0.90 ","reopen_by":"clintongormley","opened_on":"2013-05-14T17:24:41Z","closed_by":"s1monw"},{"number":"2944","comments":[{"date":"2013-04-30T12:36:57Z","author":"spinscale","text":"Hey,\n\nthis looks really strange.. I managed to reproduce it with a smaller example and will try to take a look at it\n\n```\ncurl -XDELETE 'http:\/\/localhost:9200\/geo_test'\ncurl -XPUT 'http:\/\/localhost:9200\/geo_test'\ncurl -XPUT 'http:\/\/localhost:9200\/geo_test\/item\/_mapping' -d '{\n    \"item\": {\n        \"_source\": { \"excludes\": [\"body\"] },\n        \"properties\" : {\n            \"area\": {\"type\": \"geo_shape\"}\n        }\n    }\n}'\n\ncurl -XPUT 'http:\/\/localhost:9200\/geo_test\/item\/1?refresh=true' -d '{\n    \"area\": {\n        \"type\" : \"envelope\",\n        \"coordinates\" : [[-45.0, 45.0], [45.0, -45.0]]\n    }\n}'\n\ncurl -XPOST 'http:\/\/localhost:9200\/geo_test\/item\/_search?pretty' -d '{\n    \"query\": {\"match_all\": {}}\n}'\n\ncurl http:\/\/localhost:9200\/geo_test\/item\/1\n```\n\nAs soon as the source exclude is omitted in the mapping, everything is working again. The GET on the id also works as expected.\n"},{"date":"2013-06-03T13:28:05Z","author":"fxh","text":"Hi, I was just trying this in ES 0.90.1 and I still get the same error as reported above. Could you please reopen this issues and doublecheck again? Thanks a lot.\n"},{"date":"2013-06-03T15:52:09Z","author":"spinscale","text":"@fxh hey, will reopen it and recheck as soon as possible\n"},{"date":"2013-06-05T10:04:21Z","author":"spinscale","text":"@fxh we did not include the fix in the 0.90 branch, even though I thought so. I have just pushed it into the 0.90 release branch so it will be included in the next release. Sorry for the inconvenience and thanks for bringing it up again!\n"}],"reopen_on":"2013-06-03T15:52:09Z","opened_by":"fxh","closed_on":"2013-06-05T10:04:21Z","description":"ES Version: 0.90.0.RC2\n\nSteps to reproduce:\n\nok case, without source exclusion:\n\n``` bash\ncurl -XDELETE 'http:\/\/localhost:9200\/geo_test'\ncurl -XPUT 'http:\/\/localhost:9200\/geo_test'\ncurl -XPUT 'http:\/\/localhost:9200\/geo_test\/item\/_mapping' -d '{\n    \"item\" : {\n        \"properties\" : {\n            \"location\" : {\n                \"type\" : \"object\",\n                \"properties\": {\n                     \"point\": {\"type\": \"geo_point\"},\n                     \"area\": {\"type\": \"geo_shape\"}\n                }\n            }\n        }\n    }\n}'\ncurl -XPUT 'http:\/\/localhost:9200\/geo_test\/item\/1' -d '{\n    \"location\": {\"point\": [45.0, 45.0]}\n}'\ncurl -XPUT 'http:\/\/localhost:9200\/geo_test\/item\/2' -d '{\n    \"location\": {\n        \"area\": {\n            \"type\" : \"envelope\",\n            \"coordinates\" : [[44.0, 46.0], [45.0, 45.0]]\n        }\n    }\n}'\ncurl -XPOST 'http:\/\/localhost:9200\/geo_test\/item\/_search?pretty' -d '{\n    \"query\": {\"match_all\": {}}\n}'\n```\n\nreturns the coordinates for geo_point and geo_shape items:\n\n``` JSON\n{\n    \"took\": 1,\n    \"timed_out\": false,\n    \"_shards\": {\n        \"total\": 1,\n        \"successful\": 1,\n        \"failed\": 0\n    },\n    \"hits\": {\n        \"total\": 2,\n        \"max_score\": 1.0,\n        \"hits\": [{\n            \"_index\": \"geo_test\",\n            \"_type\": \"item\",\n            \"_id\": \"1\",\n            \"_score\": 1.0,\n            \"_source\": {\n                \"location\": {\n                    \"point\": [45.0, 45.0]\n                }\n            }\n        }, {\n            \"_index\": \"geo_test\",\n            \"_type\": \"item\",\n            \"_id\": \"2\",\n            \"_score\": 1.0,\n            \"_source\": {\n                \"location\": {\n                    \"area\": {\n                        \"type\": \"envelope\",\n                        \"coordinates\": [\n                            [45.0, 45.0],\n                            [44.0, 46.0]\n                        ]\n                    }\n                }\n            }\n        }]\n    }\n}\n```\n\nhowever the same scenario but with a source exclusion mapping of an arbitrary field does not return the geo_shape coordinates any longer:\n\n``` bash\ncurl -XDELETE 'http:\/\/localhost:9200\/geo_test'\ncurl -XPUT 'http:\/\/localhost:9200\/geo_test'\ncurl -XPUT 'http:\/\/localhost:9200\/geo_test\/item\/_mapping' -d '{\n    \"item\": {\n        \"_source\": {\n            \"excludes\": [\"body\"]\n        },\n        \"properties\" : {\n            \"location\" : {\n                \"type\" : \"object\",\n                \"properties\": {\n                     \"point\": {\"type\": \"geo_point\"},\n                     \"area\": {\"type\": \"geo_shape\"}\n                }\n            }\n        }\n    }\n}'\ncurl -XPUT 'http:\/\/localhost:9200\/geo_test\/item\/1' -d '{\n    \"location\": {\"point\": [45.0, 45.0]}\n}'\ncurl -XPUT 'http:\/\/localhost:9200\/geo_test\/item\/2' -d '{\n    \"location\": {\n        \"area\": {\n            \"type\" : \"envelope\",\n            \"coordinates\" : [[44.0, 46.0], [45.0, 45.0]]\n        }\n    }\n}'\ncurl -XPOST 'http:\/\/localhost:9200\/geo_test\/item\/_search?pretty' -d '{\n    \"query\": {\"match_all\": {}}\n}'\n```\n\nreturns only the geo_point coordinates but not the geo_shape coordinates:\n\n``` JSON\n{\n    \"took\": 1,\n    \"timed_out\": false,\n    \"_shards\": {\n        \"total\": 1,\n        \"successful\": 1,\n        \"failed\": 0\n    },\n    \"hits\": {\n        \"total\": 2,\n        \"max_score\": 1.0,\n        \"hits\": [{\n            \"_index\": \"geo_test\",\n            \"_type\": \"item\",\n            \"_id\": \"1\",\n            \"_score\": 1.0,\n            \"_source\": {\n                \"location\": {\n                    \"point\": [45.0, 45.0]\n                }\n            }\n        }, {\n            \"_index\": \"geo_test\",\n            \"_type\": \"item\",\n            \"_id\": \"2\",\n            \"_score\": 1.0,\n            \"_source\": {\n                \"location\": {\n                    \"area\": {\n                        \"type\": \"envelope\",\n                        \"coordinates\": []\n                    }\n                }\n            }\n        }]\n    }\n}\n```\n","id":"13710970","title":"source exclusion mapping prevents geo shape coordinates to be returned in query result source field","reopen_by":"spinscale","opened_on":"2013-04-26T22:24:36Z","closed_by":"spinscale"},{"number":"2914","comments":[{"date":"2013-04-17T19:55:21Z","author":"s1monw","text":"I think you either need the source or the field needs to be stored or you need to store term vectors for the field. But I agree we should document that!\n\nthanks for raising this... what is your mapping for those fields?\n"},{"date":"2013-04-17T20:37:46Z","author":"rlvoyer","text":"``` json\n{\n    \"document\": {\n        \"_source\" : {\n            \"enabled\" : false\n        },\n        \"term_vector\": \"yes\",\n        \"dynamic\": false,\n        \"properties\": {\n            \"_id\": {\n                \"type\": \"long\", \n                \"index\": \"not_analyzed\"\n            },\n            \"cs\": {\n                \"type\": \"string\", \n                \"analyzer\": \"keyword\",\n                \"store\": \"no\"\n            }, \n            \"ks\": {\n                \"type\": \"string\", \n                \"analyzer\": \"keyword\", \n                \"store\": \"no\"\n            },\n            \"tpcs\": {\n                \"type\": \"string\",\n                \"analyzer\": \"keyword\", \n                \"store\": \"no\"\n            }\n        }\n    }\n}\n```\n"},{"date":"2013-04-18T08:40:42Z","author":"s1monw","text":"ah I see you should put `term_vector` next to `store` for each filed you want to store term vectors. Can you try that?\n\nlike this:\n\n```\n{\n  \"type\" : \"string\",\n  \"store\" : \"no\",\n  \"term_vector\" : \"yes\"\n}\n```\n\nsimon\n"},{"date":"2013-04-18T08:44:25Z","author":"s1monw","text":"I pushed a fix to the documentation: https:\/\/github.com\/elasticsearch\/elasticsearch.github.com\/commit\/25614ced9513e24dc3ad99b976b00e8c384ff9f2\n"},{"date":"2013-04-18T16:21:57Z","author":"rlvoyer","text":"Thanks -- I'll make that fix. What is the effect (if any) of enabling term_vector storage at the top-level as I have done here?\n"},{"date":"2013-04-19T20:13:00Z","author":"s1monw","text":"hmm it seems that this only works if it's stored or you enabled source. we should be able to support this if TV are stored for the fields as well... reopening\n"},{"date":"2013-05-02T21:17:08Z","author":"rlvoyer","text":"Hey @s1monw -- have you had an opportunity to look into this issue?\n"},{"date":"2013-05-02T21:20:34Z","author":"kimchy","text":"I am not a fan of supporting it for tern vector and no store, cause then we need to get that info(TV) from the document on the specific shard and then send it to all the shards to do the MLT based on it. Just store the source and MLT based on that. You can also, btw, always use the MLT query as part of a search request and provide the text there externally.\n"},{"date":"2013-05-02T21:32:08Z","author":"rlvoyer","text":"@kimchy can you explain how storing the source alleviates the problem of distributing the term vector to all the shards for the MLT computation?\n"},{"date":"2013-05-02T21:33:47Z","author":"kimchy","text":"cause with the source text to do MLT by, you don't need the term vectors.\n"},{"date":"2013-05-02T21:34:10Z","author":"s1monw","text":"I agree this seems odd... isn't the TV just a different representation of a field?\n"},{"date":"2013-05-02T21:46:00Z","author":"rlvoyer","text":"@kimchy @s1monw so why store the term vectors at all? (I was only storing them because of the following doc: http:\/\/www.elasticsearch.org\/guide\/reference\/api\/more-like-this\/) If MLT doesn't need them when it has the source text, does it then recompute term vectors given the source text? \n"},{"date":"2013-05-02T21:46:59Z","author":"s1monw","text":"I agree this should also work on TV though. yet at this point it doesn't so you might want to get rid of TV if you don't need them.\n"},{"date":"2013-06-12T17:01:22Z","author":"rlvoyer","text":"@kimchy @s1monw I'd like to try to write a plugin similar to more-like-this that does exactly what I want. Can you suggest any plugins that access term vectors that I might use as references? Any tips \/ documentation are much appreciated.\n"},{"date":"2013-06-12T17:47:24Z","author":"s1monw","text":"hey, we just added TermVector support lately. this issue is on our list to make use of the feature. Can you wait for it?\n"},{"date":"2013-06-12T18:51:21Z","author":"rlvoyer","text":"@s1monw Unfortunately, my company has a rapidly narrowing window for determining whether elasticsearch is right for the problem we're trying to solve. Given that the current built-in functionality doesn't seem to handle our use-case, a plugin seems like our only option in the short-term.\n"},{"date":"2013-11-19T20:12:28Z","author":"Signum","text":"Excuse me but I'm currently trying to use the MLT feature. I read http:\/\/www.elasticsearch.org\/guide\/en\/elasticsearch\/reference\/current\/search-more-like-this.html#search-more-like-this and either my english is completely bad of I have not the remotest idea what it is supposed to mean:\n\n\"Note: In order to use the mlt feature a mlt_field needs to be either be stored, store term_vector or source needs to be enabled.\"\n\nWhat is \"stored\"? Which \"source\"? I've been searching the internet for two hours now and can't any example of how to use MLT successfully. And to be honest this issue report doesn't help me either. Could anyone shed some light on it and fix the documentation please?\n"},{"date":"2013-11-19T20:50:21Z","author":"s1monw","text":"In Elasticsearch you can either store the entire document (the json you send to ES when you index) aka. the `source` or you can mark a field as `stored : true` then we only store the value of that particular field. By default the `source` is stored (or `enabled`) but you can also `disable` it via the mapping. The term_vectors don't work yet with `MLT` hence this issue. \n\nhope that helps\n"},{"date":"2013-11-19T21:01:48Z","author":"Signum","text":"@s1monw Thanks for the reply. So to rephrase: any field I'm using as \"mlt_fields=...\" needs to\n- either part of the actual source\/document\n- or be explicitly marked as \"stored:true\"\n\nOkay. In my case the documents contain two fields. Example:\n\n<pre>\n{\n_index: \"debshots\",\n_type: \"jdbc\",\n_id: \"396\",\n_version: 35,\nexists: true,\n_source: {\n    description: \"Alarm Clock for GTK Environments\",\n    name: \"alarm-clock\"\n    }\n}\n<\/pre>\n\nBut when I'm GETting http:\/\/localhost:9200\/debshots\/jdbc\/396\/_mlt Elasticsearch returns zero results:\n\n<pre>\n{\ntook: 3,\ntimed_out: false,\n_shards: {\n    total: 1,\n    successful: 1,\n    failed: 0\n    },\nhits: {\n    total: 0,\n    max_score: null,\n    hits: [ ]\n    }\n}\n<\/pre>\n\nThere are many other documents with a description like \"Alarm curl plugin for uWSGI\" so I had assumed that at least the \"Alarm\" is a term that makes it \"more-like-that\"-style.\n\nI'd welcome a hint what is going wrong here. Thanks.\n\nAnd I would also welcome a rewrite of that quoted phrase in the documentation because it's wrong english and hard to understand. (I still didn't.)\n"},{"date":"2013-11-19T21:03:02Z","author":"s1monw","text":"Can you take this please to the mailing list this is only for development issues. \n\nthanks\n"},{"date":"2013-11-19T21:05:38Z","author":"Signum","text":"@s1monw  Will do. Please still consider rewriting this sentence in the documentation to make it understandable.\n"},{"date":"2015-07-06T14:58:22Z","author":"alexksikes","text":"This issue is now outdated, closing.\n"}],"reopen_on":"2013-04-19T20:13:00Z","opened_by":"rlvoyer","closed_on":"2015-07-06T14:58:22Z","description":"I'm trying to use the more_like_this handler in almost the exact same way it's used in the documentation here:\n\nhttp:\/\/www.elasticsearch.org\/guide\/reference\/api\/more-like-this\/\n\ncurl -XGET \"http:\/\/localhost:9200\/foo\/document\/1008534\/_mlt?mlt_fields=cs,ks,tpcs&min_doc_freq=2\"\n\n{\"error\":\"ElasticSearchException[No fields found to fetch the 'likeText' from]\",\"status\":500}\n\nI'm guessing this bug stems from the fact that source is disabled, but I'm not really sure. If it is the case that source is required for MLT, you should document that fact.\n","id":"13313189","title":"MLT bug when source disabled?","reopen_by":"s1monw","opened_on":"2013-04-17T19:01:54Z","closed_by":"alexksikes"},{"number":"2848","comments":[{"date":"2013-04-03T10:28:39Z","author":"clintongormley","text":"Wrong version of JS plugin - closing\n"},{"date":"2013-04-03T10:28:40Z","author":"s1monw","text":"doh... there are some catch(Exception) rather than catch(Throwable) I will fix\n"},{"date":"2013-04-03T10:29:10Z","author":"s1monw","text":"nah this should return and not just die...\n"}],"reopen_on":"2013-04-03T10:29:10Z","opened_by":"clintongormley","closed_on":"2013-04-03T10:39:38Z","description":"When using the JS plugin in 0.20.6, the following request triggers an uncaught exception, which never returns:\n\n```\ncurl -XGET 'http:\/\/127.0.0.1:9200\/_all\/_search?pretty=1'  -d '\n{\n   \"script_fields\" : {\n      \"num\" : {\n         \"script\" : \"1\/doc[\\u0027num\\u0027].value\",\n         \"lang\" : \"js\"\n      }\n   }\n}\n'\n\nException in thread \"elasticsearch[Stunner][search][T#4]\" java.lang.AbstractMethodError\n    at org.elasticsearch.search.fetch.script.ScriptFieldsFetchSubPhase.hitExecute(ScriptFieldsFetchSubPhase.java:70)\n    at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:250)\n    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:438)\n    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:345)\n    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchQueryThenFetchAction.java:149)\n    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.run(TransportSearchQueryThenFetchAction.java:136)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:722)\n```\n","id":"12745892","title":"Uncaught exception in javascript","reopen_by":"s1monw","opened_on":"2013-04-03T10:15:02Z","closed_by":"s1monw"},{"number":"2730","comments":[{"date":"2013-03-05T19:47:06Z","author":"kimchy","text":"We don't validate it since its used during actual shard creation. When will happen is that an index will be created, but when the shard is actually allocated, it will fail to be allocated (so the index is in red state).\n"},{"date":"2013-11-19T15:17:23Z","author":"spinscale","text":"reverted, as we do not want to create an exception to the rule for parsing time based settings. need to come up with something more consistent here\n"},{"date":"2014-11-29T14:12:15Z","author":"clintongormley","text":"Possibly this can be handled by the proper settings framework planned in #6732?\n"},{"date":"2015-09-19T17:36:36Z","author":"clintongormley","text":"Closing as a duplicate of #2997\n"}],"reopen_on":"2013-11-19T15:17:24Z","opened_by":"ynux","closed_on":"2015-09-19T17:36:36Z","description":"We created an index and tried to \"curl -PUT\" a document, failed after a timeout with:\n\n```\n{\n    \"error\": \"NoShardAvailableActionException[[test][0] No shard available for [[test][test_AS24Elasticsearch][1]: routing [null]]]\",\n    \"status\": 500\n}\n```\n\nThe reason was this line in the elasticsearch.yml:\n\n```\nindex.search.slowlog.threshold.query.warn: s\n```\n\nWhen reproducing this i found that the elasticsearch.yml has to be broken at startup of the elasticsearch service. \n\nIt produced about 1'000'000 lines in the logfile.\n\nI post this hoping for a more precise error message, an earlier parsing of the elasticsearch.yml or that people running into the same problem will find this post and identify their problem faster.\n\nThe wrong stanza in elasticsearch.yml:\n\n```\nindex.search.slowlog.level: TRACE\nindex.search.slowlog.threshold.query.warn: s\nindex.search.slowlog.threshold.query.info: s\nindex.search.slowlog.threshold.query.debug: s\nindex.search.slowlog.threshold.query.trace: ms\n```\n\n(This kind of mistake is, as you will guess, the result of a beginner meddling with Puppet)\n\nIn the logfile - only the first 82 lines:\n\n```\n[2013-03-05 09:35:10,805][WARN ][indices.cluster          ] [dexxxv001] [sunytest][0] failed to create shard\norg.elasticsearch.index.shard.IndexShardCreationException: [sunytest][0] failed to create shard\n        at org.elasticsearch.index.service.InternalIndexService.createShard(InternalIndexService.java:323)\n        at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:561)\n        at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:526)\n        at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:171)\n        at org.elasticsearch.cluster.service.InternalClusterService$2.run(InternalClusterService.java:315)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:722)\nCaused by: org.elasticsearch.ElasticSearchParseException: Failed to parse [s]\n        at org.elasticsearch.common.unit.TimeValue.parseTimeValue(TimeValue.java:253)\n        at org.elasticsearch.common.settings.ImmutableSettings.getAsTime(ImmutableSettings.java:191)\n        at org.elasticsearch.index.search.slowlog.ShardSlowLogSearchService.<init>(ShardSlowLogSearchService.java:132)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:525)\n        at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)\n        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)\n        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)\n        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)\n        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:819)\n        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)\n        at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:56)\n        at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)\n        at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)\n        at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)\n        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)\n        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)\n        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)\n        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:819)\n        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)\n        at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:56)\n        at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)\n        at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)\n        at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)\n        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)\n        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)\n        at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:52)\n        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)\n        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:819)\n        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)\n        at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:56)\n        at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)\n        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:200)\n        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)\n        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:812)\n        at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)\n        at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)\n        at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)\n        at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:129)\n        at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:66)\n        at org.elasticsearch.index.service.InternalIndexService.createShard(InternalIndexService.java:321)\n        ... 7 more\nCaused by: java.lang.NumberFormatException: empty String\n        at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1011)\n        at java.lang.Double.parseDouble(Double.java:540)\n```\n","id":"11658370","title":"NoShardAvailableActionException caused by missing value in elasticsearch.yml","reopen_by":"spinscale","opened_on":"2013-03-05T08:56:00Z","closed_by":"clintongormley"},{"number":"2693","comments":[],"reopen_on":"2013-02-27T17:46:48Z","opened_by":"konradkonrad","closed_on":"2013-02-27T17:58:15Z","description":"The sample application below will be stuck after \"it runs...\". ThreadDump\/Debugging shows, it is never returning from XContentSettingsloader#serializeValue(...) [118:125].\n\nThis is especially annoying, as there is no Exception thrown.\n\njdk: jdk1.7.0_15\nelasticsearch: 0.20.5\n### Sample application\n\nsrc\/main\/java\/Runner.java:\n\n```\n    import org.elasticsearch.client.Client;\n    import org.elasticsearch.node.Node;\n    import static org.elasticsearch.node.NodeBuilder.nodeBuilder;\n    public class Runner {    \n        public static void main(String[] args) {\n            System.out.println(\"it runs...\");\n            \/\/http:\/\/www.elasticsearch.org\/guide\/reference\/java-api\/client.html\n            Node node = nodeBuilder().node();\n            System.out.println(\"got node\");\n            Client client = node.client();\n            System.out.println(\"got client\");\n            node.close();\n        }\n    }\n```\n\nsrc\/main\/resources\/elasticsearch.yml:\n\n```\n    cluster.name=malformed_yaml__XContentSettingsLoader_will_hang\n```\n","id":"11412047","title":"malformed elasticsearch.yml causes unresponsive hang","reopen_by":"konradkonrad","opened_on":"2013-02-26T14:56:46Z","closed_by":"kimchy"},{"number":"1582","comments":[{"date":"2012-01-02T21:07:07Z","author":"kimchy","text":"Thanks for spotting that!. I will push a fix to `0.18` branch, but also to the `elasticsearch-cloud-aws` repo. I will release a version of the `elasticsearch-cloud-aws` in an hour or so, so you don't have to wait for the next 0.18 version to come out (you can just install it from there, which will be the place to install plugins from with `0.19`).\n"}],"reopen_on":"2012-01-02T16:18:17Z","opened_by":"alambert","closed_on":"2012-01-02T21:08:03Z","description":"Thank you for your help in issue #1564. I am using ElasticSearch v0.18.5 with the S3 shared storage gateway and have been experiencing index data loss: after a restart, some indexes will appear with correct mappings but zero documents.\n\nIt looks like this happens because deleting an index named \"x\" causes ES to remove all keys with prefix \n\"clustername\/indicies\/x\" which includes data for any index whose name begins with \"x\". It should only remove keys with prefix \"clustername\/indicies\/x\/\".\n\nTo reproduce:\n\n<pre>\n$ curl -XPUT 'http:\/\/localhost:9200\/x\/'\n{\"ok\":true,\"acknowledged\":true}\n$ curl -XPUT 'http:\/\/localhost:9200\/x2\/'\n{\"ok\":true,\"acknowledged\":true}\n$ curl -XPOST 'http:\/\/localhost:9200\/x\/_gateway\/snapshot'\n{\"ok\":true,\"_shards\":{\"total\":5,\"successful\":4,\"failed\":0}}\n$ curl -XPOST 'http:\/\/localhost:9200\/x2\/_gateway\/snapshot'\n{\"ok\":true,\"_shards\":{\"total\":5,\"successful\":5,\"failed\":0}}\n$ curl -XDELETE 'http:\/\/localhost:9200\/x\/'\n{\"ok\":true,\"acknowledged\":true}\n<\/pre>\n\n\nAfter restarting and restoring from the gateway, x2 should have zero documents. After the DELETE request is sent, ES deletes the commit data for index x2:\n\n<pre>\n[2012-01-02 14:53:00,664][INFO ][com.amazonaws.request    ] Sending Request: GET http:\/\/xxxxxxxx-app-dev-es.s3.amazonaws.com \/ Parameters: (prefix: es-amalgam-20111025\/indices\/x, ) Headers: (Content-Type: application\/x-www-form-urlencoded; charset=utf-8, ) \n...\n[2012-01-02 14:53:00,954][INFO ][com.amazonaws.request    ] Sending Request: DELETE http:\/\/xxxxxxxx-app-dev-es.s3.amazonaws.com \/es-amalgam-20111025%2Findices%2Fx2%2F4%2Fcommit-0 Headers: (Content-Type: application\/x-www-form-urlencoded; charset=utf-8, ) \n<\/pre>\n\n\nThe full logs for the DELETE request are below:\n\n<pre>\n[2012-01-02 14:53:00,405][DEBUG][cluster.service          ] [aws-e1b-8.xxxxxxxx.net] processing [delete-index [x]]: execute\n[2012-01-02 14:53:00,405][INFO ][cluster.metadata         ] [aws-e1b-8.xxxxxxxx.net] [x] deleting index\n[2012-01-02 14:53:00,409][DEBUG][cluster.service          ] [aws-e1b-8.xxxxxxxx.net] cluster state updated, version [220], source [delete-index [x]]\n[2012-01-02 14:53:00,409][DEBUG][river.cluster            ] [aws-e1b-8.xxxxxxxx.net] processing [reroute_rivers_node_changed]: execute\n[2012-01-02 14:53:00,409][DEBUG][river.cluster            ] [aws-e1b-8.xxxxxxxx.net] processing [reroute_rivers_node_changed]: no change in cluster_state\n[2012-01-02 14:53:00,410][DEBUG][indices.cluster          ] [aws-e1b-8.xxxxxxxx.net] [x] deleting index\n[2012-01-02 14:53:00,410][DEBUG][indices                  ] [aws-e1b-8.xxxxxxxx.net] deleting Index [x]\n[2012-01-02 14:53:00,410][DEBUG][index.service            ] [aws-e1b-8.xxxxxxxx.net] [x] deleting shard_id [0]\n[2012-01-02 14:53:00,410][DEBUG][index.service            ] [aws-e1b-8.xxxxxxxx.net] [x] deleting shard_id [1]\n[2012-01-02 14:53:00,411][DEBUG][index.service            ] [aws-e1b-8.xxxxxxxx.net] [x] deleting shard_id [2]\n[2012-01-02 14:53:00,411][DEBUG][index.shard.service      ] [aws-e1b-8.xxxxxxxx.net] [x][0] state: [STARTED]->[CLOSED], reason [deleting index]\n[2012-01-02 14:53:00,411][DEBUG][index.shard.service      ] [aws-e1b-8.xxxxxxxx.net] [x][1] state: [STARTED]->[CLOSED], reason [deleting index]\n[2012-01-02 14:53:00,411][DEBUG][index.shard.service      ] [aws-e1b-8.xxxxxxxx.net] [x][2] state: [STARTED]->[CLOSED], reason [deleting index]\n[2012-01-02 14:53:00,411][INFO ][com.amazonaws.request    ] Sending Request: GET http:\/\/xxxxxxxx-app-dev-es.s3.amazonaws.com \/ Parameters: (prefix: es-amalgam-20111025\/indices\/x\/1, ) Headers: (Content-Type: application\/x-www-form-urlencoded; charset=utf-8, ) \n[2012-01-02 14:53:00,412][DEBUG][index.service            ] [aws-e1b-8.xxxxxxxx.net] [x] deleting shard_id [3]\n[2012-01-02 14:53:00,412][DEBUG][index.service            ] [aws-e1b-8.xxxxxxxx.net] [x] deleting shard_id [4]\n[2012-01-02 14:53:00,412][DEBUG][index.shard.service      ] [aws-e1b-8.xxxxxxxx.net] [x][3] state: [STARTED]->[CLOSED], reason [deleting index]\n[2012-01-02 14:53:00,413][INFO ][com.amazonaws.request    ] Sending Request: GET http:\/\/xxxxxxxx-app-dev-es.s3.amazonaws.com \/ Parameters: (prefix: es-amalgam-20111025\/indices\/x\/0, ) Headers: (Content-Type: application\/x-www-form-urlencoded; charset=utf-8, ) \n[2012-01-02 14:53:00,413][DEBUG][index.shard.service      ] [aws-e1b-8.xxxxxxxx.net] [x][4] state: [STARTED]->[CLOSED], reason [deleting index]\n[2012-01-02 14:53:00,414][INFO ][com.amazonaws.request    ] Sending Request: GET http:\/\/xxxxxxxx-app-dev-es.s3.amazonaws.com \/ Parameters: (prefix: es-amalgam-20111025\/indices\/x\/2, ) Headers: (Content-Type: application\/x-www-form-urlencoded; charset=utf-8, ) \n[2012-01-02 14:53:00,415][INFO ][com.amazonaws.request    ] Sending Request: GET http:\/\/xxxxxxxx-app-dev-es.s3.amazonaws.com \/ Parameters: (prefix: es-amalgam-20111025\/indices\/x\/3, ) Headers: (Content-Type: application\/x-www-form-urlencoded; charset=utf-8, ) \n[2012-01-02 14:53:00,415][INFO ][com.amazonaws.request    ] Sending Request: GET http:\/\/xxxxxxxx-app-dev-es.s3.amazonaws.com \/ Parameters: (prefix: es-amalgam-20111025\/indices\/x\/4, ) Headers: (Content-Type: application\/x-www-form-urlencoded; charset=utf-8, ) \n[2012-01-02 14:53:00,432][INFO ][com.amazonaws.request    ] Received successful response: 200, AWS Request ID: 1CF9DDBE56ACAE52\n[2012-01-02 14:53:00,432][DEBUG][indices.memory           ] [aws-e1b-8.xxxxxxxx.net] recalculating shard indexing buffer (reason=removed_shard[x][4]), total is [1gb] with [5] active shards, each shard set to [225.1mb]\n[2012-01-02 14:53:00,437][INFO ][com.amazonaws.request    ] Received successful response: 200, AWS Request ID: 5439E48D1C6C66E3\n[2012-01-02 14:53:00,437][INFO ][com.amazonaws.request    ] Sending Request: DELETE http:\/\/xxxxxxxx-app-dev-es.s3.amazonaws.com \/es-amalgam-20111025%2Findices%2Fx%2F0%2F__0 Headers: (Content-Type: application\/x-www-form-urlencoded; charset=utf-8, ) \n[2012-01-02 14:53:00,456][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: D5F6AEE4C87331BA\n[2012-01-02 14:53:00,456][INFO ][com.amazonaws.request    ] Sending Request: DELETE http:\/\/xxxxxxxx-app-dev-es.s3.amazonaws.com \/es-amalgam-20111025%2Findices%2Fx%2F0%2Fcommit-0 Headers: (Content-Type: application\/x-www-form-urlencoded; charset=utf-8, ) \n[2012-01-02 14:53:00,465][INFO ][com.amazonaws.request    ] Received successful response: 200, AWS Request ID: 4C3580B5A6536906\n[2012-01-02 14:53:00,465][INFO ][com.amazonaws.request    ] Sending Request: DELETE http:\/\/xxxxxxxx-app-dev-es.s3.amazonaws.com \/es-amalgam-20111025%2Findices%2Fx%2F3%2F__0 Headers: (Content-Type: application\/x-www-form-urlencoded; charset=utf-8, ) \n[2012-01-02 14:53:00,480][INFO ][com.amazonaws.request    ] Received successful response: 200, AWS Request ID: E4C5A1CD3FCC0A51\n[2012-01-02 14:53:00,480][INFO ][com.amazonaws.request    ] Sending Request: DELETE http:\/\/xxxxxxxx-app-dev-es.s3.amazonaws.com \/es-amalgam-20111025%2Findices%2Fx%2F2%2F__0 Headers: (Content-Type: application\/x-www-form-urlencoded; charset=utf-8, ) \n[2012-01-02 14:53:00,483][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: BFE115230486752E\n[2012-01-02 14:53:00,483][DEBUG][indices.memory           ] [aws-e1b-8.xxxxxxxx.net] recalculating shard indexing buffer (reason=removed_shard[x][0]), total is [1gb] with [5] active shards, each shard set to [225.1mb]\n[2012-01-02 14:53:00,494][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: 9E9CE9A22BFF2209\n[2012-01-02 14:53:00,494][INFO ][com.amazonaws.request    ] Sending Request: DELETE http:\/\/xxxxxxxx-app-dev-es.s3.amazonaws.com \/es-amalgam-20111025%2Findices%2Fx%2F3%2Fcommit-0 Headers: (Content-Type: application\/x-www-form-urlencoded; charset=utf-8, ) \n[2012-01-02 14:53:00,500][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: ACAC04C51612FE27\n[2012-01-02 14:53:00,500][INFO ][com.amazonaws.request    ] Sending Request: DELETE http:\/\/xxxxxxxx-app-dev-es.s3.amazonaws.com \/es-amalgam-20111025%2Findices%2Fx%2F2%2Fcommit-0 Headers: (Content-Type: application\/x-www-form-urlencoded; charset=utf-8, ) \n[2012-01-02 14:53:00,517][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: EFBA0DA2BAB7D65E\n[2012-01-02 14:53:00,517][DEBUG][indices.memory           ] [aws-e1b-8.xxxxxxxx.net] recalculating shard indexing buffer (reason=removed_shard[x][2]), total is [1gb] with [5] active shards, each shard set to [225.1mb]\n[2012-01-02 14:53:00,518][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: DA077754B6C41906\n[2012-01-02 14:53:00,518][DEBUG][indices.memory           ] [aws-e1b-8.xxxxxxxx.net] recalculating shard indexing buffer (reason=removed_shard[x][3]), total is [1gb] with [5] active shards, each shard set to [225.1mb]\n[2012-01-02 14:53:00,626][INFO ][com.amazonaws.request    ] Received successful response: 200, AWS Request ID: C97B12628A245B67\n[2012-01-02 14:53:00,626][INFO ][com.amazonaws.request    ] Sending Request: DELETE http:\/\/xxxxxxxx-app-dev-es.s3.amazonaws.com \/es-amalgam-20111025%2Findices%2Fx%2F1%2F__0 Headers: (Content-Type: application\/x-www-form-urlencoded; charset=utf-8, ) \n[2012-01-02 14:53:00,643][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: C444D05B9C2526EC\n[2012-01-02 14:53:00,643][INFO ][com.amazonaws.request    ] Sending Request: DELETE http:\/\/xxxxxxxx-app-dev-es.s3.amazonaws.com \/es-amalgam-20111025%2Findices%2Fx%2F1%2Fcommit-0 Headers: (Content-Type: application\/x-www-form-urlencoded; charset=utf-8, ) \n[2012-01-02 14:53:00,662][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: A2EC73D53CEF11B5\n[2012-01-02 14:53:00,663][DEBUG][indices.memory           ] [aws-e1b-8.xxxxxxxx.net] recalculating shard indexing buffer (reason=removed_shard[x][1]), total is [1gb] with [5] active shards, each shard set to [225.1mb]\n[2012-01-02 14:53:00,664][INFO ][com.amazonaws.request    ] Sending Request: GET http:\/\/xxxxxxxx-app-dev-es.s3.amazonaws.com \/ Parameters: (prefix: es-amalgam-20111025\/indices\/x, ) Headers: (Content-Type: application\/x-www-form-urlencoded; charset=utf-8, ) \n[2012-01-02 14:53:00,701][INFO ][com.amazonaws.request    ] Received successful response: 200, AWS Request ID: 126AAAFACB6B9E52\n[2012-01-02 14:53:00,701][INFO ][com.amazonaws.request    ] Sending Request: DELETE http:\/\/xxxxxxxx-app-dev-es.s3.amazonaws.com \/es-amalgam-20111025%2Findices%2Fx2%2F0%2F__0 Headers: (Content-Type: application\/x-www-form-urlencoded; charset=utf-8, ) \n[2012-01-02 14:53:00,719][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: AE11CA882DC3B5F2\n[2012-01-02 14:53:00,720][INFO ][com.amazonaws.request    ] Sending Request: DELETE http:\/\/xxxxxxxx-app-dev-es.s3.amazonaws.com \/es-amalgam-20111025%2Findices%2Fx2%2F0%2Fcommit-0 Headers: (Content-Type: application\/x-www-form-urlencoded; charset=utf-8, ) \n[2012-01-02 14:53:00,747][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: 611EB0181D55BE54\n[2012-01-02 14:53:00,747][INFO ][com.amazonaws.request    ] Sending Request: DELETE http:\/\/xxxxxxxx-app-dev-es.s3.amazonaws.com \/es-amalgam-20111025%2Findices%2Fx2%2F1%2F__0 Headers: (Content-Type: application\/x-www-form-urlencoded; charset=utf-8, ) \n[2012-01-02 14:53:00,769][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: 756ACDD05674B387\n[2012-01-02 14:53:00,769][INFO ][com.amazonaws.request    ] Sending Request: DELETE http:\/\/xxxxxxxx-app-dev-es.s3.amazonaws.com \/es-amalgam-20111025%2Findices%2Fx2%2F1%2Fcommit-0 Headers: (Content-Type: application\/x-www-form-urlencoded; charset=utf-8, ) \n[2012-01-02 14:53:00,789][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: F840A4C6AC34606D\n[2012-01-02 14:53:00,789][INFO ][com.amazonaws.request    ] Sending Request: DELETE http:\/\/xxxxxxxx-app-dev-es.s3.amazonaws.com \/es-amalgam-20111025%2Findices%2Fx2%2F2%2F__0 Headers: (Content-Type: application\/x-www-form-urlencoded; charset=utf-8, ) \n[2012-01-02 14:53:00,811][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: D7CDEDD29EBC7D61\n[2012-01-02 14:53:00,812][INFO ][com.amazonaws.request    ] Sending Request: DELETE http:\/\/xxxxxxxx-app-dev-es.s3.amazonaws.com \/es-amalgam-20111025%2Findices%2Fx2%2F2%2Fcommit-0 Headers: (Content-Type: application\/x-www-form-urlencoded; charset=utf-8, ) \n[2012-01-02 14:53:00,833][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: 9617334FDC66FB23\n[2012-01-02 14:53:00,833][INFO ][com.amazonaws.request    ] Sending Request: DELETE http:\/\/xxxxxxxx-app-dev-es.s3.amazonaws.com \/es-amalgam-20111025%2Findices%2Fx2%2F3%2F__0 Headers: (Content-Type: application\/x-www-form-urlencoded; charset=utf-8, ) \n[2012-01-02 14:53:00,904][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: E7B2AC2C2DFE2ED6\n[2012-01-02 14:53:00,904][INFO ][com.amazonaws.request    ] Sending Request: DELETE http:\/\/xxxxxxxx-app-dev-es.s3.amazonaws.com \/es-amalgam-20111025%2Findices%2Fx2%2F3%2Fcommit-0 Headers: (Content-Type: application\/x-www-form-urlencoded; charset=utf-8, ) \n[2012-01-02 14:53:00,926][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: A47D9D007502212A\n[2012-01-02 14:53:00,926][INFO ][com.amazonaws.request    ] Sending Request: DELETE http:\/\/xxxxxxxx-app-dev-es.s3.amazonaws.com \/es-amalgam-20111025%2Findices%2Fx2%2F4%2F__0 Headers: (Content-Type: application\/x-www-form-urlencoded; charset=utf-8, ) \n[2012-01-02 14:53:00,954][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: 38EC545C47C99D65\n[2012-01-02 14:53:00,954][INFO ][com.amazonaws.request    ] Sending Request: DELETE http:\/\/xxxxxxxx-app-dev-es.s3.amazonaws.com \/es-amalgam-20111025%2Findices%2Fx2%2F4%2Fcommit-0 Headers: (Content-Type: application\/x-www-form-urlencoded; charset=utf-8, ) \n[2012-01-02 14:53:00,974][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: A709CE7D578AAF0B\n[2012-01-02 14:53:00,981][DEBUG][gateway.s3               ] [aws-e1b-8.xxxxxxxx.net] writing to gateway org.elasticsearch.gateway.shared.SharedStorageGateway$2@3d761403 ...\n[2012-01-02 14:53:00,984][DEBUG][cluster.service          ] [aws-e1b-8.xxxxxxxx.net] processing [delete-index [x]]: done applying updated cluster_state\n[2012-01-02 14:53:00,984][INFO ][com.amazonaws.request    ] Sending Request: PUT http:\/\/xxxxxxxx-app-dev-es.s3.amazonaws.com \/es-amalgam-20111025%2Fmetadata%2Fmetadata-2455 Headers: (Content-Length: 77420, Content-Type: application\/octet-stream, ) \n[2012-01-02 14:53:01,057][INFO ][com.amazonaws.request    ] Received successful response: 200, AWS Request ID: BAD343B536EDE0F7\n[2012-01-02 14:53:01,057][INFO ][com.amazonaws.request    ] Sending Request: GET http:\/\/xxxxxxxx-app-dev-es.s3.amazonaws.com \/ Parameters: (prefix: es-amalgam-20111025\/metadata\/, ) Headers: (Content-Type: application\/x-www-form-urlencoded; charset=utf-8, ) \n[2012-01-02 14:53:01,105][INFO ][com.amazonaws.request    ] Received successful response: 200, AWS Request ID: 34F9E73651ADB6CD\n[2012-01-02 14:53:01,105][INFO ][com.amazonaws.request    ] Sending Request: DELETE http:\/\/xxxxxxxx-app-dev-es.s3.amazonaws.com \/es-amalgam-20111025%2Fmetadata%2Fmetadata-2454 Headers: (Content-Type: application\/x-www-form-urlencoded; charset=utf-8, ) \n[2012-01-02 14:53:01,131][INFO ][com.amazonaws.request    ] Received successful response: 204, AWS Request ID: 75732598971E7A5F\n[2012-01-02 14:53:01,131][DEBUG][gateway.s3               ] [aws-e1b-8.xxxxxxxx.net] wrote to gateway org.elasticsearch.gateway.shared.SharedStorageGateway$2@3d761403, took 150ms\n<\/pre>\n\n\nI can provide additional logs if needed. Thank you!\n","id":"2701835","title":"S3 blob storage gateway: deleting an index named x destroys data for any index with name beginning with x","reopen_by":"alambert","opened_on":"2012-01-02T15:08:08Z","closed_by":"kimchy"},{"number":"1156","comments":[{"date":"2011-07-26T03:13:26Z","author":"kimchy","text":"It does support versioning, read the page you link to again. And again, questions in the mailing list please.\n"},{"date":"2011-07-26T03:17:07Z","author":"ghost","text":"I tried deleting versions that were not there and succeeded. I'll try again.\n\nAnd sorry for posting questions in here. Thought this one was a missing feature or a bug.\n"},{"date":"2011-07-26T03:27:13Z","author":"kimchy","text":"Ahh, I checked the code, and I see the problem, will push a fix.\n"}],"reopen_on":"2011-07-26T03:27:11Z","opened_by":"ghost","closed_on":"2011-07-26T04:35:43Z","description":"The `delete` action does not uses the `_version` provided properly, effectively ignoring it.\n","id":"1285321","title":"Bulk API: _version on delete actions is not honored","reopen_by":"kimchy","opened_on":"2011-07-25T22:53:51Z","closed_by":"kimchy"},{"number":"1145","comments":[{"date":"2011-07-22T19:48:13Z","author":"Alex-Ikanow","text":"Updated: edited original issue, edited title and added fact it didn't work for js either (also added then removed what I thought was a workaround but wasn't in fact)\n"},{"date":"2011-07-30T12:19:10Z","author":"kimchy","text":"Heya, can you gist a recreation? Something like indexing some sample data using curl, and then issuing the offending scripts? This sounds like a bug, will be simpler to fix it then. \n"},{"date":"2011-08-01T14:01:43Z","author":"Alex-Ikanow","text":"The GIST is here:\n\nhttps:\/\/gist.github.com\/1118155\n\nIn the simple reproduction above, \"mvel\" and \"js\" behaved consistently, ie \"values\" always failed even for a single record with multiple values. I didn't investigate any further (ie why mvel did work in my original, more complex, reproduction) since it's almost certainly the same issue anyway\n\nAlso, with regard to [1], do you intend for \"values\" to return an array of length 1 for single-value entries (ie multiValued==false tells you if you can use \"value\", but you can always use \"values\")?\n\nAs noted above, it would be great if it did work this way (at least if the field ever contained multiple values, though that's presumably harder than making it always behave that way)\n"},{"date":"2011-08-01T14:04:31Z","author":"Alex-Ikanow","text":"(sorry closed by mistake - not a big fan of the big \"comment & close\" button github gives you!)\n"},{"date":"2011-08-09T15:49:37Z","author":"kimchy","text":"This is a bug in how the cached array is constructed to support this type. Will push a fix shortly.\n"}],"reopen_on":"2011-08-01T14:03:47Z","opened_by":"Alex-Ikanow","closed_on":"2011-08-09T15:50:15Z","description":"(See also https:\/\/github.com\/elasticsearch\/elasticsearch\/issues\/1144)\n\nI now have a dataset with a multi-valued field (\"locs\" from the previous issue, the type happens to be a geo-point; I don't know if that's significant but I suspect not) that sometimes has a single value, eg\n\n\"fields\": {\n   \"locs\": \"40.3278286374,-74.511843005\"\n}\n\nand sometimes many values:\n\n\"fields\": {\n   \"locs\": [\n      \"40.3278286374,-74.511843005\",\n      \"40.3278286374,-74.511843005\"\n   ]\n}\n\nTwo related issues (the second of which looks like a definite bug)\n\nBoth of these are issues with mvel (tried js and got a separate problem, described in [3])\n\n1] Ideally I'd like to be able to use \"doc['locs'].values\" in both cases, eg with \"doc['locs'].values.length==1\" in the first case and \"==2\" in the second.\n\nIn practice, if I try to access any properties of \"values\" in the first case (eg \"n = doc['locs'].values.length\"), I get an error like:\n\n\"reason\": \"RuntimeException[cannot invoke getter: getValues [declr.class: org.elasticsearch.index.mapper.xcontent.geo.GeoPointDocFieldData; act.class: org.elasticsearch.index.mapper.xcontent.geo.GeoPointDocFieldData](see trace)]; nested: nested: ArrayIndexOutOfBoundsException[0]; \"\n\n2] Since the above happened, I wanted to test on \".multiValued\" and have 2 different clauses to handle the two cases. Unfortunately, \"doc['locs'].multiValued\" returns true in both cases.\n\n(As an aside: If there's any mvel workaround which would allow me to keep the code in mvel with 0.16.2 that would be awesome, I use a bunch of trig functions and wotnot later on in the script, and I'm not particularly excited to rewrite it in JS or mess around with native scripts)\n\n3] Perhaps I'm going mad, but with \"lang\":\"js\", then \"doc['locs'].values\" always failed for me (with the same ArrayIndexOutOfBoundsException[0]), ie even with 'locs' having 2 or 3 values.\n\n(Conversely, doc['locs'].lats and doc['locs'].lons worked in JS)\n","id":"1264822","title":"Scripts: arrays: \".multiValued\" returns true even when \".values\" fails","reopen_by":"Alex-Ikanow","opened_on":"2011-07-21T18:02:07Z","closed_by":"kimchy"},{"number":"873","comments":[{"date":"2011-04-21T08:38:19Z","author":"lukas-vlcek","text":"The issues seems to be only when the field is not stored, see https:\/\/gist.github.com\/933978 it works as expected.\nSo getting the field content from `_source` is probably not working correctly.\n"},{"date":"2011-04-21T08:41:36Z","author":"karmi","text":"I don't understand: I think it should work as expected in the original gist, no?\n"},{"date":"2011-04-21T09:06:15Z","author":"lukas-vlcek","text":"Based on the docs it should.\n\nHere is the situation: you need to get the content of the Lucene field to have it highlighted. And there are two options, either you store the content (which is what I did in my gits example) and then you can retrieve the content from the Lucene index or you provide the content yourself to the highlighter which is what should happen if the filed is not stored, it should be taken from the `_source` field but this does not seem to work correctly.\n\nSo there is probably a bug in highlighting but only for the later situation, ie when the field is expected to be retrieved from the `_source` field.\n"},{"date":"2011-04-21T09:09:09Z","author":"lukas-vlcek","text":"btw: here is relevant ticket https:\/\/github.com\/elasticsearch\/elasticsearch\/issues\/561\n"},{"date":"2011-04-21T09:53:06Z","author":"lukas-vlcek","text":"karmi, it works, here is the final solution :-) https:\/\/gist.github.com\/933978\nThe problem is that if the field is not stored then you still have to set term_vector (and thus use mapping for this). The documentation needs to be updated to make this crystal clear.\n"},{"date":"2011-04-21T09:56:20Z","author":"karmi","text":"Thanks!, but... shouldn't it just work out of the box? Should the issue be really closed in this state?\n"},{"date":"2011-04-21T11:12:28Z","author":"lukas-vlcek","text":"You are right, there is a bug.\n"}],"reopen_on":"2011-04-21T09:56:25Z","opened_by":"karmi","closed_on":"2011-04-21T12:55:54Z","description":"The [Highlight documentation](http:\/\/www.elasticsearch.org\/guide\/reference\/api\/search\/highlighting.html) says:\n\n> If the number_of_fragments value is set to 0 then no fragments are produced, instead the whole content of the field is returned, and of course it is highlighted. \n\nHowever, this does not work as stated. When running this gist: https:\/\/gist.github.com\/931339 against current master, no highlights are returned.\n","id":"797878","title":"No highlight results when `number_of_fragments` set to 0","reopen_by":"karmi","opened_on":"2011-04-21T08:11:07Z","closed_by":"lukas-vlcek"}]